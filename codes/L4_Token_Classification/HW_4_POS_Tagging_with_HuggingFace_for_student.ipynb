{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeansAthiwat/NLP_NoScope/blob/main/codes/L4_Token_Classification/HW_4_POS_Tagging_with_HuggingFace_for_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxOTS6n1ikVL"
      },
      "source": [
        "# HW 4 - POS Tagging with Hugging Face\n",
        "\n",
        "In this exercise, you will create a part-of-speech (POS) tagging system for Thai text using NECTEC’s ORCHID corpus. Instead of building your own deep learning architecture from scratch, you will leverage a pretrained tokenizer and a pretrained token classification model from Hugging Face.\n",
        "\n",
        "We have provided some starter code for data cleaning and preprocessing in this notebook, but feel free to modify those parts to suit your needs. You are welcome to use additional libraries (e.g., scikit-learn) as long as you incorporate the pretrained Hugging Face model. Specifically, you will need to:\n",
        "\n",
        "1. Load a pretrained tokenizer and token classification model.\n",
        "2. Fine-tune it on the ORCHID corpus for POS tagging.\n",
        "3. Evaluate and report the performance of your model on the test data.\n",
        "\n",
        "### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1Uqldlj81G"
      },
      "source": [
        "## 1. Setup and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyb4FhsEEeH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82008f74-e97f-49e0-cfae-b7d76282e153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install transformers and thai2transformers\n",
        "!pip install wandb\n",
        "!pip install -q transformers==4.30.1 datasets evaluate thaixtransformers\n",
        "!pip install -q emoji pythainlp sefr_cut tinydb seqeval sentencepiece pydantic jsonlines\n",
        "!pip install peft==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTgw8WW3BxWZ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. Register [Wandb account](https://wandb.ai/login?signup=true) (and confirm your email)\n",
        "\n",
        "2. `wandb login` and copy paste the API key when prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FjMIqhTBfOX"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qSh7MXZB74z"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-BR7danGv6W"
      },
      "source": [
        "We encourage you to login to your `Hugging Face` account so you can upload and share your model with the community. When prompted, enter your token to login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7h8NENllZK2"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqkDkseilv19"
      },
      "source": [
        "Download the dataset from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRksERXFEngl"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "orchid = load_dataset(\"Thichow/orchid_corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_AWd4d5lCYd"
      },
      "outputs": [],
      "source": [
        "orchid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNmIqSo0FkAx"
      },
      "outputs": [],
      "source": [
        "orchid['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUuz3dLGlI_S"
      },
      "outputs": [],
      "source": [
        "orchid['train'][0][\"sentence\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "bh7fX19zI85W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47834fdf-8c2d-4a1a-bd3f-8637b825e6dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'การประชุมทางวิชาการ ครั้งที่ 1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "''.join(orchid['train'][0]['label_tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "38jM9YcSFmjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e48337-fc72-4066-8029-c84f5c63615c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total type of pos_tags : 47\n",
            "['ADVI', 'ADVN', 'ADVP', 'ADVS', 'CFQC', 'CLTV', 'CMTR', 'CMTR@PUNC', 'CNIT', 'CVBL', 'DCNM', 'DDAC', 'DDAN', 'DDAQ', 'DDBQ', 'DIAC', 'DIAQ', 'DIBQ', 'DONM', 'EAFF', 'EITT', 'FIXN', 'FIXV', 'JCMP', 'JCRG', 'JSBR', 'NCMN', 'NCNM', 'NEG', 'NLBL', 'NONM', 'NPRP', 'NTTL', 'PDMN', 'PNTR', 'PPRS', 'PREL', 'PUNC', 'RPRE', 'VACT', 'VATT', 'VSTA', 'XVAE', 'XVAM', 'XVBB', 'XVBM', 'XVMM']\n"
          ]
        }
      ],
      "source": [
        "label_list = orchid[\"train\"].features[f\"pos_tags\"].feature.names\n",
        "print('total type of pos_tags :', len(label_list))\n",
        "print(label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Uf_NDWg7F6z_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "#transformers\n",
        "from transformers import (\n",
        "    CamembertTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "#thaixtransformers\n",
        "from thaixtransformers import Tokenizer\n",
        "from thaixtransformers.preprocess import process_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1suScqmntBW"
      },
      "source": [
        "Next, we load a pretrained tokenizer from Hugging Face. In this work, we utilize WangchanBERTa, a Thai-specific pretrained model, as the tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt3ASYUVm54n"
      },
      "source": [
        "# Choose Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFBKLqbIm23-"
      },
      "source": [
        "In this notebook, you can choose from 5 versions of WangchanBERTa, XLMR and mBERT to perform downstream tasks on Thai datasets. The datasets are:\n",
        "\n",
        "* `wangchanberta-base-att-spm-uncased` (recommended) - Largest WangchanBERTa trained on 78.5GB of Assorted Thai Texts with subword tokenizer SentencePiece\n",
        "* `xlm-roberta-base` - Facebook's [XLMR](https://arxiv.org/abs/1911.02116) trained on 100 languages\n",
        "* `bert-base-multilingual-cased` - Google's [mBERT](https://arxiv.org/abs/1911.03310) trained on 104 languages\n",
        "* `wangchanberta-base-wiki-newmm` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's word-level tokenizer  `newmm`\n",
        "* `wangchanberta-base-wiki-syllable` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's syllabel-level tokenizer `syllable`\n",
        "* `wangchanberta-base-wiki-sefr` - WangchanBERTa trained on Thai Wikipedia Dump with word-level tokenizer  `SEFR`\n",
        "* `wangchanberta-base-wiki-spm` - WangchanBERTa trained on Thai Wikipedia Dump with subword-level tokenizer SentencePiece\n",
        "\n",
        "In the first part, we require you to select the wangchanberta-base-att-spm-uncased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HbZo_TZDn17"
      },
      "source": [
        "<b> Learn more about using wangchanberta at [wangchanberta_getting_started_ai_reseach](https://colab.research.google.com/github/PyThaiNLP/thaixtransformers/blob/main/notebooks/wangchanberta_getting_started_aireseach.ipynb?fbclid=IwY2xjawH61XZleHRuA2FlbQIxMAABHZUaAmHobzmCMHpX0EgdLdjDAEwSX0bjqpo5xPUSIx9b4O_dsIvvG8KVNA_aem_IyKkvzy-VPf9k2pYAFf6Nw#scrollTo=n5IaCot9b3cF) <b>\n",
        "\n",
        "\n",
        "\n",
        "*   You need to set the transformers version to transformers==4.30.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl2NposVIh9-"
      },
      "source": [
        "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "n5IaCot9b3cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ffa4be-f2d4-48e0-b014-d37a1a1b4a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
        "    'airesearch/wangchanberta-base-wiki-newmm',\n",
        "    'airesearch/wangchanberta-base-wiki-ssg',\n",
        "    'airesearch/wangchanberta-base-wiki-sefr',\n",
        "    'airesearch/wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(\n",
        "                f'{model_name}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzdbERHLwd0X"
      },
      "source": [
        "Let's try using a pretrained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "qwrwXsHFwl-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4441f1-0cb4-43cb-c75a-d3e047b2b8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text : ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร\n",
            "tokens : ['<s>', '', 'ศิลปะ', 'ไม่เป็น', 'เจ้านาย', 'ใคร', '<_>', 'และ', 'ไม่เป็น', 'ขี้ข้า', 'ใคร', '</s>']\n"
          ]
        }
      ],
      "source": [
        "text = 'ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร'\n",
        "print('text :', text)\n",
        "tokens = []\n",
        "for i in tokenizer([text], is_split_into_words=True)['input_ids']:\n",
        "  tokens.append(tokenizer.decode(i))\n",
        "print('tokens :', tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEQAVqO8pDhK"
      },
      "source": [
        "model : * `wangchanberta-base-att-spm-uncased`\n",
        "\n",
        "First, we print examples of label tokens from our dataset for inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Vw_GdRdlpAhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4df525d-584c-4504-9c08-451c4c6051c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id : 0\n",
            "label_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
            "pos_tags : [21, 39, 26, 26, 37, 4, 18]\n",
            "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n"
          ]
        }
      ],
      "source": [
        "example = orchid[\"train\"][0]\n",
        "for i in example :\n",
        "    print(i, ':', example[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTuiwEWkppdA"
      },
      "source": [
        "Then, we use the sentence 'การประชุมทางวิชาการ<space>ครั้งที่ 1' to be tokenized by the pretrained tokenizer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "BRCxMtHToN16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf04fce-9a64-441a-af4a-344749418b88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "text = 'การประชุมทางวิชาการ ครั้งที่ 1'\n",
        "tokenizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxi8WqZnGa5F"
      },
      "source": [
        "These are already mapped into discrete values. We can uncover the original token text from the tokens by."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "optGK_eco3K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ff9638-dbd7-45c0-8ce9-5425b548e755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "▁\n",
            "การประชุม\n",
            "ทางวิชาการ\n",
            "<_>\n",
            "▁\n",
            "ครั้งที่\n",
            "<_>\n",
            "▁\n",
            "1\n",
            "</s>\n"
          ]
        }
      ],
      "source": [
        "for i in tokenizer(text)['input_ids']:\n",
        "  print(tokenizer.convert_ids_to_tokens(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3l13UKnwK-d"
      },
      "source": [
        "Now let's look at another example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "UyfIR3BowU84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8eba6a1-095b-409d-b331-b7f4f9f06c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (Bilingual transfer dictionary)\n",
            "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', '<unk>', 'i', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
            "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'Bilingual transfer dictionary', ')']\n",
            "label pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n"
          ]
        }
      ],
      "source": [
        "example = orchid[\"train\"][1899]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])\n",
        "print('label pos :', example[\"pos_tags\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmV6M-vAwew5"
      },
      "source": [
        "Notice how `B` becomes an ``<unk>`` token. This is because this is an uncased model, meaning it only handles small English characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WniJR47ww7a0"
      },
      "source": [
        "# #TODO 0\n",
        "\n",
        "Convert the dataset to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "RQWm_iWBxFQ8"
      },
      "outputs": [],
      "source": [
        "# Create a lowercase dataset for uncased BERT\n",
        "def lower_case_sentences(examples):\n",
        "  lower_cased_examples = examples\n",
        "\n",
        "  # fill code here to lower case the \"sentence\" and \"label_tokens\"\n",
        "  lower_cased_examples[\"sentence\"] = examples[\"sentence\"].lower()\n",
        "  lower_cased_examples[\"label_tokens\"] = [word.lower() for word in examples[\"label_tokens\"]]\n",
        "  return lower_cased_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "ndBIqEpWuqBP"
      },
      "outputs": [],
      "source": [
        "orchidl = orchid.map(lower_case_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "z8xpcCqTrqbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022c70ed-83a5-48e0-b902-989581f2a9fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
              "        num_rows: 18500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n",
              "        num_rows: 4625\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "orchidl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ecpDHyTPv2py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303f5bd1-10f3-4dd7-dd1a-8db540839022"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 39, 38, 26, 26, 5, 37, 37, 26, 37]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "orchidl[\"train\"][1899]['pos_tags']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgV4ohz2xTY9"
      },
      "source": [
        "Now let's examine the labels again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "DoUDQzM7q265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2051990-69db-460a-e904-ef1f1f79d4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
            "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
            "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
            "label pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n"
          ]
        }
      ],
      "source": [
        "example = orchidl[\"train\"][1899]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])\n",
        "print('label pos :', example[\"pos_tags\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "aEHgBeX7fQFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfe58e4-e5d1-4188-cd60-8e2516b83429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n",
            "tokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\n",
            "label tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
            "label pos : [21, 39, 26, 26, 37, 4, 18]\n"
          ]
        }
      ],
      "source": [
        "example = orchidl[\"train\"][0]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])\n",
        "print('label pos :', example[\"pos_tags\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dVcLxYbrl4E"
      },
      "source": [
        "In the example above, tokens refer to those tokenized using the pretrained tokenizer, while label tokens refer to tokens tokenized from our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1inxbOYuBpB"
      },
      "source": [
        "**Do you see something?**\n",
        "\n",
        "Yes, the tokens from the two tokenizers do not match.\n",
        "\n",
        "- sentence : `การประชุมทางวิชาการ ครั้งที่ 1`\n",
        "\n",
        "---\n",
        "\n",
        "- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- label tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n",
        "- label pos : `[21, 39, 26, 26, 37, 4, 18]`\n",
        "\n",
        "You can see that in our label tokens, 'การ' has a POS tag of 21, and 'ประชุม' has a POS tag of 39. However, when we tokenize the sentence using WangchanBERTa, we get the token 'การประชุม'. What POS tag should we assign to this new token?\n",
        "\n",
        "**What should we do ?**\n",
        "\n",
        "Based on this example, we found that the tokens from the WangchanBERTa do not directly align with our label tokens. This means we cannot directly use the label POS tags. Therefore, we need to reassign POS tags to the tokens produced by WangchanBERTa tokenization. The method we will use is majority voting:\n",
        "- If a token from the WangchanBERTa matches a label token exactly, we will directly assign the POS tag from the label POS.\n",
        "- If the token generated overlaps or combines multiple label tokens, we assign the POS tag based on the number of characters in each token: If the token contains the most characters from any label token, we assign the POS tag from that label token.\n",
        "\n",
        "**Example :**\n",
        "\n",
        "    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
        "    # \"การ\" has a POS tag of 21,\n",
        "    # and \"ประชุม\" has a POS tag of 39.\n",
        "    # Therefore, the POS tag for \"การประชุม\" is 39,\n",
        "    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
        "\n",
        "    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n",
        "    # \"ทาง\" has a POS tag of 26,\n",
        "    # and \"วิชาการ\" has a POS tag of 2.\n",
        "    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n",
        "    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTkgye8K8sd8"
      },
      "source": [
        "# #TODO 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgU8Nudh2rUJ"
      },
      "source": [
        "`**Warning: Please be careful of <unk>, an unknown word token.**`\n",
        "\n",
        "`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`\n",
        "\n",
        "Assigning the label -100 to the special tokens `[<s>]` and `[</s>]` and `[_]`  so they’re ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): ignore_index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test = []\n",
        "# for i in tokenizer(['สวัสดีทุกท่านทุกคนในวันนี้เรามาตกปลาHหมึกยามค่ำข้างๆลิงสีดำ'], is_split_into_words=True)['input_ids']:\n",
        "#   test.append(tokenizer.decode(i))\n",
        "# print('tokens :', test)\n",
        "\n",
        "# test per character print of 'am' ำ\n",
        "for char in 'ำ':\n",
        "  print(char)\n",
        "\n",
        "print(len(tokenizer([\"สวัสดีทุกท่านทุกคนในวันนี้เรามาตกปลาหมึกยามค่ำข้างๆลิงสีดำ\"], is_split_into_words=True)['input_ids']))\n",
        "print(len(tokenizer.convert_ids_to_tokens(tokenizer([\"สวัสดีทุกท่านทุกคนในวันนี้เรามาตกปลาหมึกยามค่ำข้างๆลิงสีดำ\"], is_split_into_words=True)['input_ids'])))\n",
        "print(tokenizer.convert_ids_to_tokens(tokenizer([\"ำ\"], is_split_into_words=True)['input_ids']))\n",
        "print(tokenizer([\"ำ\"], is_split_into_words=True)['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDER1zsoFTd6",
        "outputId": "28db98b8-6bb5-4f3c-9ad9-2d6c1226678f"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ำ\n",
            "14\n",
            "14\n",
            "['<s>', '▁', 'ํา', '</s>']\n",
            "[5, 10, 4556, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "bdxxiUU69lDx"
      },
      "outputs": [],
      "source": [
        "def majority_vote_pos(examples):\n",
        "    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n",
        "    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
        "\n",
        "    label_tokens = examples[\"label_tokens\"]\n",
        "    label_pos_tags = examples[\"pos_tags\"]\n",
        "    # tokenized_inputs[\"chs\"] = []\n",
        "    # tokenized_inputs[\"pos\"]= []\n",
        "    def normalize_am(char):\n",
        "        return 'ํา' if char == 'ำ' else char\n",
        "\n",
        "    char_list = []\n",
        "    pos_list = []\n",
        "    for word_i, word in enumerate(label_tokens):\n",
        "        for c in word:\n",
        "            normed = normalize_am(c)\n",
        "            if normed == 'ํา':\n",
        "                char_list.append('ํ')\n",
        "                pos_list.append(label_pos_tags[word_i])\n",
        "                char_list.append('า')\n",
        "                pos_list.append(label_pos_tags[word_i])\n",
        "            else:\n",
        "                char_list.append(normed)\n",
        "                pos_list.append(label_pos_tags[word_i])\n",
        "\n",
        "    new_labels = []\n",
        "    char_ptr = 0  # pointer to char_list / pos_list\n",
        "\n",
        "    new_labels = []\n",
        "    char_ptr = 0  # pointer to char_list / pos_list\n",
        "    tok_index = 0\n",
        "    new_tokens = [' ' if x == '<_>' else x.replace('▁', '') for x in new_tokens]\n",
        "    while tok_index < len(new_tokens):\n",
        "        tok = new_tokens[tok_index]\n",
        "        if tok in ['<s>', '</s>', '▁']:\n",
        "            new_labels.append(-100)\n",
        "            tok_index += 1\n",
        "            continue\n",
        "\n",
        "        elif tok == '<unk>':\n",
        "            pos_count_inner = {}\n",
        "            # while char_ptr < len(char_list):\n",
        "            next_token = new_tokens[tok_index+1]\n",
        "            next_char = new_tokens[tok_index+1][0]\n",
        "            print(next_token)\n",
        "            print(next_char)\n",
        "            while not (  char_list[char_ptr] == next_token or char_list[char_ptr] == next_char):\n",
        "                vote_pos = pos_list[char_ptr]\n",
        "                pos_count_inner[vote_pos] = pos_count_inner.get(vote_pos, 0) + 1\n",
        "                char_ptr += 1\n",
        "\n",
        "            if len(pos_count_inner) == 0:\n",
        "                new_labels.append(-88)\n",
        "                assert False, \"error\"\n",
        "            else:\n",
        "                majority_pos = max(pos_count_inner, key=pos_count_inner.get)\n",
        "                new_labels.append(majority_pos)\n",
        "                tok_index += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        token_str = tok\n",
        "\n",
        "        pos_count = {}\n",
        "        for ch in token_str:\n",
        "            if char_ptr >= len(char_list):\n",
        "                char_ptr += 1\n",
        "                break\n",
        "            if ch == char_list[char_ptr]:\n",
        "                vote_pos = pos_list[char_ptr]\n",
        "                pos_count[vote_pos] = pos_count.get(vote_pos, 0) + 1\n",
        "                char_ptr += 1\n",
        "            else:\n",
        "                char_ptr += 1\n",
        "\n",
        "            # tokenized_inputs.setdefault(\"chs\", []).append(ch)\n",
        "            # tokenized_inputs.setdefault(\"pos\", []).append(vote_pos)\n",
        "\n",
        "        if len(pos_count) == 0:\n",
        "            new_labels.append(-100)\n",
        "            tok_index += 1\n",
        "        else:\n",
        "            majority_pos = max(pos_count, key=pos_count.get)\n",
        "            new_labels.append(majority_pos)\n",
        "            tok_index += 1\n",
        "\n",
        "\n",
        "\n",
        "    tokenized_inputs[\"tokens\"] = new_tokens\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    # tokenized_inputs[\"chars\"] = char_list\n",
        "    # tokenized_inputs[\"pos_tags\"] = pos_list\n",
        "    # tokenized_inputs[\"chars_len\"] = len(char_list)\n",
        "\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "doFKOhpbGf9N"
      },
      "outputs": [],
      "source": [
        "tokenized_orchid = orchidl.map(majority_vote_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "uvdDnWeOJYpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7f471a-0e70-448c-d4f7-5a3148cc7657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '', 'การประชุม', 'ทางวิชาการ', ' ', '', 'ครั้งที่', ' ', '', '1', '</s>']\n",
            "[-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]\n"
          ]
        }
      ],
      "source": [
        "tokenized_orchid\n",
        "print(tokenized_orchid['train'][0]['tokens'])\n",
        "print(tokenized_orchid['train'][0]['labels'])\n",
        "# i = 0\n",
        "# for sentence in tokenized_orchid['train']:\n",
        "#   print(sentence['tokens'])\n",
        "#   print(sentence['labels'])\n",
        "#   if i == 500:\n",
        "#     break\n",
        "#   i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "ojrRF85dJbwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040a537f-adcd-4aba-a052-286634433107"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n",
              " 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n",
              " 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1',\n",
              " 'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'tokens': ['<s>',\n",
              "  '',\n",
              "  'การประชุม',\n",
              "  'ทางวิชาการ',\n",
              "  ' ',\n",
              "  '',\n",
              "  'ครั้งที่',\n",
              "  ' ',\n",
              "  '',\n",
              "  '1',\n",
              "  '</s>'],\n",
              " 'labels': [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "tokenized_orchid['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "KMfzFnjSdCGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6861aaa-a51e-4b9f-8665-a48c52d9a173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id : 0\n",
            "label_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\n",
            "pos_tags : [21, 39, 26, 26, 37, 4, 18]\n",
            "sentence : การประชุมทางวิชาการ ครั้งที่ 1\n",
            "input_ids : [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6]\n",
            "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "tokens : ['<s>', '', 'การประชุม', 'ทางวิชาการ', ' ', '', 'ครั้งที่', ' ', '', '1', '</s>']\n",
            "labels : [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]\n"
          ]
        }
      ],
      "source": [
        "example = tokenized_orchid[\"train\"][0]\n",
        "for i in example :\n",
        "    print(i, \":\", example[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lhsQcdL6H3J"
      },
      "source": [
        "This is the result after we realigned the POS based on the majority vote.\n",
        "- label_tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n",
        "- pos_tags : `[21, 39, 26, 26, 37, 4, 18]`\n",
        "- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n",
        "- labels : `[-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]`\n",
        "\n",
        "`['<s>', '▁', '</s>'] : -100`\n",
        "\n",
        "**Check :**\n",
        "\n",
        "> \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n",
        "\n",
        "\n",
        "> \"การ\" has a POS tag of 21,\n",
        "\n",
        "> and \"ประชุม\" has a POS tag of 39.\n",
        "\n",
        "> Therefore, the POS tag for \"การประชุม\" is 39,\n",
        "\n",
        "> as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "iOE5CEgZdO9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deda16ab-ae71-440e-932e-e119d1e566d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id : 1899\n",
            "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
            "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
            "input_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\n",
            "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "tokens : ['<s>', 'โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bi', 'ling', 'ual', ' ', '', 'trans', 'fer', ' ', '', 'di', 'ction', 'ary', ')', '</s>']\n",
            "labels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n"
          ]
        }
      ],
      "source": [
        "# hard test case\n",
        "example = tokenized_orchid[\"train\"][1899]\n",
        "for i in example :\n",
        "    print(i, \":\", example[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_NkVQ6qsJe"
      },
      "source": [
        "Expected output\n",
        "\n",
        "\n",
        "```\n",
        "id : 1899\n",
        "label_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n",
        "pos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n",
        "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
        "input_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\n",
        "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "tokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\n",
        "labels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MY TEST"
      ],
      "metadata": {
        "id": "WAQh1T1PC1kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_orchid[\"train\"]['tokens'][0][0])\n",
        "for example in tokenized_orchid[\"train\"]:\n",
        "    for token in example['tokens']:\n",
        "      # print(token)\n",
        "      if not isinstance(token, str):\n",
        "          print(token)\n",
        "    for label in example['labels']:\n",
        "        # print(label)\n",
        "        if not isinstance(label, int):\n",
        "            print(label)\n",
        "        # else:\n",
        "        #     print(\"sex: \",label)\n",
        "    for label in example['labels']:\n",
        "        if not isinstance(label, int):\n",
        "            print(label)\n",
        "    if example[\"labels\"] is None:\n",
        "        print(\"Found None labels:\", example)\n",
        "    elif len(example[\"tokens\"]) != len(example[\"labels\"]):\n",
        "        print(f\"Mismatch: Tokens ({len(example['tokens'])}) vs Labels ({len(example['labels'])})\")\n",
        "    elif len(example[\"tokens\"]) != len(example[\"labels\"]):\n",
        "        print(f\"Mismatch: Tokens ({len(example['tokens'])}) vs Labels ({len(example['labels'])})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRJJtlZmC3Ls",
        "outputId": "9d61b5f4-36b2-4a43-868b-7ddb71bb7951"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pwADd1a85bn"
      },
      "source": [
        "# Train and Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnlIUJvYEy2"
      },
      "source": [
        "We will create a batch of examples using [DataCollatorWithPadding.](https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding)  \n",
        "\n",
        "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
        "\n",
        "DataCollatorWithPadding will help us pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length. This allows for efficient computation during each batch.\n",
        "\n",
        "*   DataCollatorForTokenClassification : `padding (bool, str or PaddingStrategy, optional, defaults to True)`\n",
        "*   `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single sequence is provided).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "CcAY4-E2J6e5"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg4v14KcElbY"
      },
      "source": [
        "For evaluating your model’s performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) framework (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric). Seqeval actually produces several scores: precision, recall, F1, and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "cZk3PjndK-Q8"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FllGDNO5RUIA"
      },
      "source": [
        "Huggingface requires us to write a ``compute_metrics()`` function. This will be invoked when huggingface evalutes a model.\n",
        "\n",
        "Note that we ignore to evaluate on -100 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "vDwNPItNLTM1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt13vTldvTw4"
      },
      "source": [
        "The total number of labels in our POS tag set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "JD84B79-Lxwf"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    0: 'ADVI',\n",
        "    1: 'ADVN',\n",
        "    2: 'ADVP',\n",
        "    3: 'ADVS',\n",
        "    4: 'CFQC',\n",
        "    5: 'CLTV',\n",
        "    6: 'CMTR',\n",
        "    7: 'CMTR@PUNC',\n",
        "    8: 'CNIT',\n",
        "    9: 'CVBL',\n",
        "    10: 'DCNM',\n",
        "    11: 'DDAC',\n",
        "    12: 'DDAN',\n",
        "    13: 'DDAQ',\n",
        "    14: 'DDBQ',\n",
        "    15: 'DIAC',\n",
        "    16: 'DIAQ',\n",
        "    17: 'DIBQ',\n",
        "    18: 'DONM',\n",
        "    19: 'EAFF',\n",
        "    20: 'EITT',\n",
        "    21: 'FIXN',\n",
        "    22: 'FIXV',\n",
        "    23: 'JCMP',\n",
        "    24: 'JCRG',\n",
        "    25: 'JSBR',\n",
        "    26: 'NCMN',\n",
        "    27: 'NCNM',\n",
        "    28: 'NEG',\n",
        "    29: 'NLBL',\n",
        "    30: 'NONM',\n",
        "    31: 'NPRP',\n",
        "    32: 'NTTL',\n",
        "    33: 'PDMN',\n",
        "    34: 'PNTR',\n",
        "    35: 'PPRS',\n",
        "    36: 'PREL',\n",
        "    37: 'PUNC',\n",
        "    38: 'RPRE',\n",
        "    39: 'VACT',\n",
        "    40: 'VATT',\n",
        "    41: 'VSTA',\n",
        "    42: 'XVAE',\n",
        "    43: 'XVAM',\n",
        "    44: 'XVBB',\n",
        "    45: 'XVBM',\n",
        "    46: 'XVMM',\n",
        "    # 47: 'O'\n",
        "}\n",
        "label2id = {}\n",
        "for k, v in id2label.items() :\n",
        "    label2id[v] = k\n",
        "\n",
        "# label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "mQtGN8QQQLME"
      },
      "outputs": [],
      "source": [
        "labels = [i for i in id2label.values()]\n",
        "# labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu7Z3QH_BJe4"
      },
      "source": [
        "## Load pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBYlcF-gDZcF"
      },
      "source": [
        "Select a pretrained model for fine-tuning to develop a POS Tagger model using the Orchid corpus dataset.\n",
        "\n",
        "\n",
        "\n",
        "*   model : `wangchanberta-base-att-spm-uncased`\n",
        "*   Don't forget to update the num_labels.\n",
        "\n",
        "You’re ready to start training your model now! Load pretrained model with AutoModelForTokenClassification along with the number of expected labels, and the label mappings:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OOu8s-mO_Fw"
      },
      "source": [
        "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OOsnubHyDMmA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "076449b1413741078d1d889257a69b40",
            "3eeed4c1102b47edb80f1dfe2e8ca274",
            "d24f8b32db67466d81265b561054ec35",
            "004c41b34a8a445ebf2c21c861e72687",
            "44a22468c154484385762bff6a6e3666",
            "124c5ff4121e4b3791cc18131e24b7df",
            "d1cc7845013d41828a9e7b9a586fc311",
            "856fb9da14be435c8a73fc7b21b229ca",
            "cff9d9de32774bc38579f3b4a87821d7",
            "9e4c787623b04681b312369b3b935c04",
            "d4586a53f3874d3f8edb1afa8b7d5cea"
          ]
        },
        "outputId": "083024fb-eac8-464c-b3ff-ee2ad58fd200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/423M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "076449b1413741078d1d889257a69b40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'wangchanberta-base-att-spm-uncased',\n",
        "    'wangchanberta-base-wiki-newmm',\n",
        "    'wangchanberta-base-wiki-ssg',\n",
        "    'wangchanberta-base-wiki-sefr',\n",
        "    'wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "#create model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    f\"airesearch/{model_name}\",\n",
        "    revision='main',\n",
        "    num_labels=47, id2label=id2label, label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H2OExQrCAfX"
      },
      "source": [
        "### #TODO 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBZKrz8nFXyT"
      },
      "source": [
        "* Configure your training hyperparameters using `**TrainingArguments**`. The only required parameter is is `output_dir`, which determines the directory where your model will be saved. To upload the model to the Hugging Face Hub, set push_to_hub=True (note: you must be logged into Hugging Face for this). During training, the Trainer will compute seqeval metrics at the end of each epoch and store the training checkpoint.\n",
        "* Provide the `**Trainer**` with the training arguments, as well as the model, dataset, tokenizer, data collator, and compute_metrics function.\n",
        "* Use `**train()**` to fine-tune the model.\n",
        "\n",
        "\n",
        "Read [huggingface's tutorial](https://huggingface.co/docs/transformers/en/tasks/token_classification) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gMUkNHNrCwsl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "0c78b15b-a6da-49f7-efd2-e54792fc5dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/content/pos_tagger_model_att_spm_uncased is already a clone of https://huggingface.co/JeansAthiwat/pos_tagger_model_att_spm_uncased. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/pos_tagger_model_att_spm_uncased is already a clone of https://huggingface.co/JeansAthiwat/pos_tagger_model_att_spm_uncased. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6939' max='6939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6939/6939 15:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.315300</td>\n",
              "      <td>0.295221</td>\n",
              "      <td>0.862339</td>\n",
              "      <td>0.875845</td>\n",
              "      <td>0.869040</td>\n",
              "      <td>0.916193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.237700</td>\n",
              "      <td>0.271533</td>\n",
              "      <td>0.870161</td>\n",
              "      <td>0.881117</td>\n",
              "      <td>0.875605</td>\n",
              "      <td>0.920963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.203100</td>\n",
              "      <td>0.272307</td>\n",
              "      <td>0.870657</td>\n",
              "      <td>0.880847</td>\n",
              "      <td>0.875722</td>\n",
              "      <td>0.920873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2254: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6939, training_loss=0.2804172888048136, metrics={'train_runtime': 946.1523, 'train_samples_per_second': 58.659, 'train_steps_per_second': 7.334, 'total_flos': 1264810975026288.0, 'train_loss': 0.2804172888048136, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"pos_tagger_model_att_spm_uncased\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_orchid[\"train\"],\n",
        "    eval_dataset=tokenized_orchid[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiXUG6aakihd"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPVknoQk4wL"
      },
      "source": [
        "With your model fine-tuned, you can now perform inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mMyq6I9CkZ1R"
      },
      "outputs": [],
      "source": [
        "text = \"การประชุมทางวิชาการ ครั้งที่ 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5qxD3EPLFt"
      },
      "source": [
        "`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "sKgM-EaGfxA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46587b7f-1e56-4760-dd21-eff742a9f38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load pretrained tokenizer from Hugging Face\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(model_name)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "PcRf-Q9nf4-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae0d55a-4727-484c-8600-d5a8b3fa5401"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   5,   10,  882, 8222,    8,   10, 1014,    8,   10,   59,    6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "6ADY5OuqkkHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e1b70c-4246-48ea-e1e5-fd06b2b89867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "## Load your fine-tuned model from Hugging Face\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"JeansAthiwat/pos_tagger_model_att_spm_uncased\") ## your model path from Hugging Face\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "AACsd7VZgT1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d12fb9-86c5-406a-939f-39bbffeead84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DONM',\n",
              " 'PUNC',\n",
              " 'VACT',\n",
              " 'NCMN',\n",
              " 'PUNC',\n",
              " 'PUNC',\n",
              " 'CFQC',\n",
              " 'DONM',\n",
              " 'DONM',\n",
              " 'DONM',\n",
              " 'PUNC']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "predicted_token_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kJKGbf4Rk0c9"
      },
      "outputs": [],
      "source": [
        "# id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6OikBqcKTcUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8b7188-1a96-4830-b2b9-e146c849ee26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens : ['<s>', '▁', 'จะว่าไป', 'แล้ว', 'เชิง', 'เทียน', 'ของ', 'ผมก็', 'สวยดี', 'เหมือนกัน', '</s>']\n",
            "predict pos : ['PUNC', 'PUNC', 'ADVS', 'XVAE', 'NCMN', 'NCMN', 'RPRE', 'PPRS', 'VATT', 'ADVN', 'PUNC']\n"
          ]
        }
      ],
      "source": [
        "# Inference\n",
        "# ignore special tokens\n",
        "\n",
        "text = 'จะว่าไปแล้วเชิงเทียนของผมก็สวยดีเหมือนกัน'\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "tokenized_input = tokenizer([text], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :', tokens)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "print('predict pos :', predicted_token_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-TtwYTCXs2O"
      },
      "source": [
        "**Evaluate model :**\n",
        "\n",
        "The output from the model is a softmax over classes. We choose the maximum class as the answer for evaluation. Again, we will ignore the -100 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "Hm_adLrcXyOe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def evaluation_report(y_true, y_pred, get_only_acc=False):\n",
        "    # retrieve all tags in y_true\n",
        "    tag_set = set()\n",
        "    for sent in y_true:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    for sent in y_pred:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    tag_list = sorted(list(tag_set))\n",
        "\n",
        "    # count correct points\n",
        "    tag_info = dict()\n",
        "    for tag in tag_list:\n",
        "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
        "\n",
        "    all_correct = 0\n",
        "    all_count = sum([len(sent) for sent in y_true])\n",
        "    speacial_tag = 0\n",
        "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
        "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
        "            # pass special token\n",
        "            if tag_true == -100 :\n",
        "                speacial_tag += 1\n",
        "                pass\n",
        "            if tag_true == tag_pred:\n",
        "                tag_info[tag_true]['correct_tagged'] += 1\n",
        "                all_correct += 1\n",
        "            tag_info[tag_true]['y_true'] += 1\n",
        "            tag_info[tag_pred]['y_pred'] += 1\n",
        "    print('speacial_tag :',speacial_tag) # delete number of special token from all_count\n",
        "    accuracy = (all_correct / (all_count-speacial_tag))\n",
        "\n",
        "    # get only accuracy for testing\n",
        "    if get_only_acc:\n",
        "      return accuracy\n",
        "\n",
        "    accuracy *= 100\n",
        "\n",
        "\n",
        "    # summarize and make evaluation result\n",
        "    eval_list = list()\n",
        "    for tag in tag_list:\n",
        "        eval_result = dict()\n",
        "        eval_result['tag'] = tag\n",
        "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
        "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
        "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
        "        eval_result['precision'] = precision\n",
        "        eval_result['recall'] = recall\n",
        "        eval_result['f1_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
        "\n",
        "        eval_list.append(eval_result)\n",
        "\n",
        "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f1_score': ''})\n",
        "\n",
        "    df = pd.DataFrame.from_dict(eval_list)\n",
        "    df = df[['tag', 'precision', 'recall', 'f1_score', 'correct_count']]\n",
        "\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "x7Lryj2aYCdn"
      },
      "outputs": [],
      "source": [
        "# prepare test set\n",
        "test_data = tokenized_orchid[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "FGXWNs9RY2Zv"
      },
      "outputs": [],
      "source": [
        "# labels for test set\n",
        "y_test = []\n",
        "for inputs in test_data:\n",
        "  y_test.append(inputs['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "U6__09qnX1DW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b81895b0-7502-4049-e2c0-0c4c1edc3133"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-286bfe76ca15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Append padded predictions to y_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/camembert/modeling_camembert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1288\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/camembert/modeling_camembert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         )\n\u001b[0;32m--> 904\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/camembert/modeling_camembert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 )\n\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    542\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/camembert/modeling_camembert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/camembert/modeling_camembert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/camembert/modeling_camembert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "y_pred = []\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "for inputs in test_data:\n",
        "    text = inputs['sentence']\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        pred = model(**inputs).logits\n",
        "        predictions = torch.argmax(pred, dim=2)\n",
        "        # Append padded predictions to y_pred\n",
        "        y_pred.append(predictions.tolist()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX0BhOe7g3eh"
      },
      "outputs": [],
      "source": [
        "# check our prediction with label\n",
        "# -100 is special tokens : [<s>, </s>, _]\n",
        "print(y_pred[0])\n",
        "print(y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na0L6NUdgLaE"
      },
      "outputs": [],
      "source": [
        "evaluation_report(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0BZzVgjm4l_"
      },
      "source": [
        "# Other Pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssZdIst48AwH"
      },
      "source": [
        "In this section, we will experiment by fine-tuning other pretrained models, such as airesearch/wangchanberta-base-wiki-newmm, to see how about their performance.\n",
        "\n",
        "Since each model uses a different word-tokenization method.\n",
        "for example, **airesearch/wangchanberta-base-wiki-newmm uses newmm**,\n",
        "while **airesearch/wangchanberta-base-att-spm-uncased uses SentencePiece**.\n",
        "please try fine-tuning and compare the performance of these models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUCWuhrl91mj"
      },
      "source": [
        "### #TODO 3\n",
        "\n",
        "CURRENT PIPELINE IS FOR WIKI SENTENCEPIECEMATCHING (WIKI SPM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "9etT-A_anBfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c448323-ab56-4e30-8a68-c975a6a9439b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'ThaiRobertaTokenizer'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'ThaiRobertaTokenizer'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
        "    'airesearch/wangchanberta-base-wiki-newmm',\n",
        "    'airesearch/wangchanberta-base-wiki-ssg',\n",
        "    'airesearch/wangchanberta-base-wiki-sefr',\n",
        "    'airesearch/wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-wiki-spm\" #@param [\"airesearch/wangchanberta-base-att-spm-uncased\", \"airesearch/wangchanberta-base-wiki-newmm\", \"airesearch/wangchanberta-base-wiki-syllable\", \"airesearch/wangchanberta-base-wiki-sefr\", \"airesearch/wangchanberta-base-wiki-spm\"]\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(\n",
        "                f'{model_name}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "LFXdV8V5nGk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2d7249-a71a-42e3-f95d-d8e04146f7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\n",
            "tokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '(', 'bi', 'ling', 'ual', '<_>', 't', 'ransfer', '<_>', 'di', 'ction', 'ary', ')', '</s>']\n",
            "label tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n"
          ]
        }
      ],
      "source": [
        "example = orchidl[\"train\"][1899]\n",
        "print('sentence :', example[\"sentence\"])\n",
        "tokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :',tokens)\n",
        "print('label tokens :', example[\"label_tokens\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7QMwcbK5Ibj"
      },
      "source": [
        "It's the same problem as above.\n",
        "\n",
        "`**Warning: Can we use same function as above ?**`\n",
        "\n",
        "`**Warning: Please beware of <unk>, an unknown word token.**`\n",
        "\n",
        "`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for char in \"คู่\":\n",
        "    print(char)\n",
        "\n",
        "ape = {}\n",
        "ape[' '] = \"negus\"\n",
        "print(ape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpw2-KLAMx77",
        "outputId": "63fc2fc1-3b62-4aaf-c7ef-05330e237e7a"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ค\n",
            "ู\n",
            "่\n",
            "{' ': 'negus'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "SI0fUulE5MH3"
      },
      "outputs": [],
      "source": [
        "def majority_vote_pos(examples):\n",
        "    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n",
        "    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
        "\n",
        "    label_tokens = examples[\"label_tokens\"]\n",
        "    label_pos_tags = examples[\"pos_tags\"]\n",
        "    # tokenized_inputs[\"chs\"] = []\n",
        "    # tokenized_inputs[\"pos\"]= []\n",
        "    def normalize_am(char):\n",
        "        return 'ํา' if char == 'ำ' else char\n",
        "\n",
        "    char_list = []\n",
        "    pos_list = []\n",
        "    for word_i, word in enumerate(label_tokens):\n",
        "        for c in word:\n",
        "            normed = normalize_am(c)\n",
        "            if normed == 'ํา':\n",
        "                char_list.append('ํ')\n",
        "                pos_list.append(label_pos_tags[word_i])\n",
        "                char_list.append('า')\n",
        "                pos_list.append(label_pos_tags[word_i])\n",
        "            else:\n",
        "                char_list.append(normed)\n",
        "                pos_list.append(label_pos_tags[word_i])\n",
        "\n",
        "    new_labels = []\n",
        "    char_ptr = 0  # pointer to char_list / pos_list\n",
        "\n",
        "    new_labels = []\n",
        "    char_ptr = 0  # pointer to char_list / pos_list\n",
        "    tok_index = 0\n",
        "    new_tokens = [' ' if x == '<_>' else x.replace('▁', '') for x in new_tokens]\n",
        "    while tok_index < len(new_tokens):\n",
        "        tok = new_tokens[tok_index]\n",
        "        if tok in ['<s>', '</s>', '▁']:\n",
        "            new_labels.append(-100)\n",
        "            tok_index += 1\n",
        "            continue\n",
        "\n",
        "        elif tok == '<unk>':\n",
        "            pos_count_inner = {}\n",
        "            # while char_ptr < len(char_list):\n",
        "            next_token = new_tokens[tok_index+1]\n",
        "            next_char = new_tokens[tok_index+1][0]\n",
        "            print(next_token)\n",
        "            print(next_char)\n",
        "            while not (  char_list[char_ptr] == next_token or char_list[char_ptr] == next_char):\n",
        "                vote_pos = pos_list[char_ptr]\n",
        "                pos_count_inner[vote_pos] = pos_count_inner.get(vote_pos, 0) + 1\n",
        "                char_ptr += 1\n",
        "\n",
        "            if len(pos_count_inner) == 0:\n",
        "                new_labels.append(-88)\n",
        "                assert False, \"error\"\n",
        "            else:\n",
        "                majority_pos = max(pos_count_inner, key=pos_count_inner.get)\n",
        "                new_labels.append(majority_pos)\n",
        "                tok_index += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        token_str = tok\n",
        "\n",
        "        pos_count = {}\n",
        "        for ch in token_str:\n",
        "            if char_ptr >= len(char_list):\n",
        "                char_ptr += 1\n",
        "                break\n",
        "            if ch == char_list[char_ptr]:\n",
        "                vote_pos = pos_list[char_ptr]\n",
        "                pos_count[vote_pos] = pos_count.get(vote_pos, 0) + 1\n",
        "                char_ptr += 1\n",
        "            else:\n",
        "                char_ptr += 1\n",
        "\n",
        "            # tokenized_inputs.setdefault(\"chs\", []).append(ch)\n",
        "            # tokenized_inputs.setdefault(\"pos\", []).append(vote_pos)\n",
        "\n",
        "        if len(pos_count) == 0:\n",
        "            new_labels.append(-100)\n",
        "            tok_index += 1\n",
        "        else:\n",
        "            majority_pos = max(pos_count, key=pos_count.get)\n",
        "            new_labels.append(majority_pos)\n",
        "            tok_index += 1\n",
        "\n",
        "\n",
        "\n",
        "    tokenized_inputs[\"tokens\"] = new_tokens\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    # tokenized_inputs[\"chars\"] = char_list\n",
        "    # tokenized_inputs[\"pos_tags\"] = pos_list\n",
        "    # tokenized_inputs[\"chars_len\"] = len(char_list)\n",
        "\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# orchidl[\"train\"][63]"
      ],
      "metadata": {
        "id": "bteAzp0wn6Iv"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "ashRh72szWiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5cd8e8-c348-4e37-cad2-a95e7c60ac98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids : [24005, 382, 1165, 11781, 3613, 381, 476, 1272, 1777, 1103, 26, 12815, 1274, 1783, 9, 1378, 9, 5010, 22, 923, 9, 1790, 9, 19041, 9, 2752, 226, 640, 150, 9, 16, 1208, 1031, 5440, 18, 9, 12, 4557, 947, 20, 1417, 778, 15, 477, 6]\n",
            "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "tokens : ['<s>', 'ก็ได้', 'ปรากฏ', 'ผลงานวิจัย', 'และพัฒนา', 'หลาย', 'โครงการ', 'ที่สามารถ', 'นําไปสู่', 'การผลิต', 'โดย', 'ภาคเอกชน', 'ได้อย่าง', 'ชัดเจน', ' ', 'อาทิ', ' ', 'ไมโคร', 'ค', 'อมพิวเตอร์', ' ', '32', ' ', 'บิท', ' ', 'วงจร', 'รวม', 'ขนาดใหญ่', 'มาก', ' ', '(', 'v', 'l', 'si', ')', ' ', 'และ', 'มอเตอร์', 'ขนาดเล็ก', 'ใน', 'ผลิตภัณฑ์', 'ไฟฟ้า', 'เป็น', 'สําคัญ', '</s>']\n",
            "labels : [-100, 43, 41, 26, 26, 17, 26, 43, 39, 39, 38, 26, 22, 40, 37, 38, 37, 26, 26, 26, 37, 10, 37, 6, 37, 26, 26, 26, 1, 37, 37, 26, 26, 26, 37, 37, 24, 26, 26, 38, 26, 26, 22, 40, -100]\n"
          ]
        }
      ],
      "source": [
        "# hard test cases\n",
        "example = orchidl[\"train\"][63]\n",
        "result_example = majority_vote_pos(example)\n",
        "for i in result_example :\n",
        "    print(i, \":\", result_example[i])\n",
        "\n",
        "# example = tokenized_orchid[\"train\"][1899]\n",
        "\n",
        "# for i in example :\n",
        "#     print(i, \":\", example[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "O5n4veYxo3rR"
      },
      "outputs": [],
      "source": [
        "tokenized_orchid = orchidl.map(majority_vote_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "7AL0Vqbv7cfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c084f9d-b596-4dfe-feaf-6c427e9e7557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
            "Some weights of the model checkpoint at airesearch/wangchanberta-base-wiki-spm were not used when initializing RobertaForTokenClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-wiki-spm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_names = [\n",
        "    'wangchanberta-base-att-spm-uncased',\n",
        "    'wangchanberta-base-wiki-newmm',\n",
        "    'wangchanberta-base-wiki-ssg',\n",
        "    'wangchanberta-base-wiki-sefr',\n",
        "    'wangchanberta-base-wiki-spm',\n",
        "]\n",
        "\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"wangchanberta-base-wiki-spm\" #@param [\"wangchanberta-base-att-spm-uncased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n",
        "\n",
        "#create model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    f\"airesearch/{model_name}\",\n",
        "    revision='main',\n",
        "    num_labels=47, id2label=id2label, label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "pWTh0bMfP70g"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, max_length=416)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkhYDS4q7oxK"
      },
      "source": [
        "### #TODO 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVdITM5E7tQ5"
      },
      "source": [
        "Fine-tuning other pretrained model with our orchid corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "hBHlUamr7syk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "795584a0-0d81-41a4-8901-aea9798837b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/content/pos_tagger_model_wiki_spm is already a clone of https://huggingface.co/JeansAthiwat/pos_tagger_model_wiki_spm. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/pos_tagger_model_wiki_spm is already a clone of https://huggingface.co/JeansAthiwat/pos_tagger_model_wiki_spm. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-18850453c0cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         )\n\u001b[0;32m-> 1645\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2759\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1414\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    846\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"pos_tagger_model_wiki_spm\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_orchid[\"train\"],\n",
        "    eval_dataset=tokenized_orchid[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIdxvgUGCVsm"
      },
      "outputs": [],
      "source": [
        "######## EVALUATE YOUR MODEL ########\n",
        "# Inference\n",
        "# ignore special tokens\n",
        "# Load pretrained tokenizer from Hugging Face\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(model_name)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "## Load your fine-tuned model from Hugging Face\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"JeansAthiwat/pos_tagger_model_att_spm_uncased\") ## your model path from Hugging Face\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "predicted_token_class\n",
        "\n",
        "text = 'จะว่าไปแล้วเชิงเทียนของผมก็สวยดีเหมือนกัน'\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "tokenized_input = tokenizer([text], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('tokens :', tokens)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "predictions = torch.argmax(logits, dim=2)\n",
        "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
        "print('predict pos :', predicted_token_class)\n",
        "\n",
        "\n",
        "y_pred = []\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "for inputs in test_data:\n",
        "    text = inputs['sentence']\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        pred = model(**inputs).logits\n",
        "        predictions = torch.argmax(pred, dim=2)\n",
        "        # Append padded predictions to y_pred\n",
        "        y_pred.append(predictions.tolist()[0])\n",
        "# check our prediction with label\n",
        "# -100 is special tokens : [<s>, </s>, _]\n",
        "print(y_pred[0])\n",
        "print(y_test[0])\n",
        "evaluation_report(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMpXErqPBv2I"
      },
      "source": [
        "### #TODO 5\n",
        "\n",
        "Compare the results between both models. Are they comparable? (Think about the ground truths of both models).\n",
        "\n",
        "Propose a way to fairly evaluate the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UigMzZVNCDuS"
      },
      "source": [
        "<b>Write your answer here :</b>\n",
        "<br>\n",
        "We cant compare them directly with just metrices on prediction vs tokenized label. as it is not fare in terms of the token amount ( newmm vs attn-spm)\n",
        "\n",
        "\n",
        "<b>What we can do</b>is to maybe assemble each tokenized value back in to the original ground truth form(Do alignment again but on pred_token to ground truth) and compare that token instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYN0X9HWVuUe"
      },
      "source": [
        "A note on preprocessing data.\n",
        "\n",
        "``process_transformers`` in ``thaixtransformers.preprocess`` also provides a preprocess code that deals with many issues such as casing, text cleaning, and white space replacement with <_>. You can also use this to preprocess your text. Note that space replacement is done automatically without preprocessing in thaixtransformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRV7tNW8fdT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "076449b1413741078d1d889257a69b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3eeed4c1102b47edb80f1dfe2e8ca274",
              "IPY_MODEL_d24f8b32db67466d81265b561054ec35",
              "IPY_MODEL_004c41b34a8a445ebf2c21c861e72687"
            ],
            "layout": "IPY_MODEL_44a22468c154484385762bff6a6e3666"
          }
        },
        "3eeed4c1102b47edb80f1dfe2e8ca274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124c5ff4121e4b3791cc18131e24b7df",
            "placeholder": "​",
            "style": "IPY_MODEL_d1cc7845013d41828a9e7b9a586fc311",
            "value": "model.safetensors: 100%"
          }
        },
        "d24f8b32db67466d81265b561054ec35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_856fb9da14be435c8a73fc7b21b229ca",
            "max": 423470574,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cff9d9de32774bc38579f3b4a87821d7",
            "value": 423470574
          }
        },
        "004c41b34a8a445ebf2c21c861e72687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e4c787623b04681b312369b3b935c04",
            "placeholder": "​",
            "style": "IPY_MODEL_d4586a53f3874d3f8edb1afa8b7d5cea",
            "value": " 423M/423M [00:02&lt;00:00, 240MB/s]"
          }
        },
        "44a22468c154484385762bff6a6e3666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "124c5ff4121e4b3791cc18131e24b7df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1cc7845013d41828a9e7b9a586fc311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "856fb9da14be435c8a73fc7b21b229ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff9d9de32774bc38579f3b4a87821d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e4c787623b04681b312369b3b935c04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4586a53f3874d3f8edb1afa8b7d5cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}