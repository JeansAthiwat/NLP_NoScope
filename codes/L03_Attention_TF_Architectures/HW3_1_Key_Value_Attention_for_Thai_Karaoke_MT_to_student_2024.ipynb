{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfUmXr1D1ZSR"
      },
      "source": [
        "# Key-Value Attention for Thai Karaoke Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
        "\n",
        "In this homework, you will create an MT model with attention mechnism that coverts names of Thai 2019 MP candidates from Thai script to Roman(Latin) script. E.g. นิยม-->niyom\n",
        "\n",
        "The use of Pytorch Lightning is optional but recommended. You can use Pytorch if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "18KMSkqZ-Pt-"
      },
      "outputs": [],
      "source": [
        "!pip install lightning wandb -q\n",
        "# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKCBCWKARZEx",
        "outputId": "6ad8a585-fd68-44ee-deac-37143137cbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ka2TN8IV1ZSU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "mpl.font_manager.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n",
        "mpl.rc('font', family='TH Sarabun New')\n",
        "import torch\n",
        "# import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-f_s6vX1ZSZ"
      },
      "source": [
        "## Load Dataset\n",
        "We have generated a toy dataset using names of Thai MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
        "\n",
        "```\n",
        "ไกรสีห์ kraisi\n",
        "พัชรี phatri\n",
        "ธีระ thira\n",
        "วุฒิกร wutthikon\n",
        "ไสว sawai\n",
        "สัมภาษณ์  samphat\n",
        "วศิน wasin\n",
        "ทินวัฒน์ thinwat\n",
        "ศักดินัย sakdinai\n",
        "สุรศักดิ์ surasak\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jte-Csrf-4kd",
        "outputId": "a1ba364b-64c2-4875-873c-90bc1808a4be"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L9zXp7KH1ZSa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('mp_name_th_en.csv') as csvfile:\n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    name_th = []\n",
        "    name_en = []\n",
        "    for row in readCSV:\n",
        "        temp_th = row[0]\n",
        "        temp_en = row[1]\n",
        "\n",
        "        name_th.append(temp_th)\n",
        "        name_en.append(temp_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZCsqrXxu1ZSe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ไกรสีห์ kraisi\n",
            "พัชรี phatri\n",
            "ธีระ thira\n",
            "วุฒิกร wutthikon\n",
            "ไสว sawai\n",
            "สัมภาษณ์  samphat\n",
            "วศิน wasin\n",
            "ทินวัฒน์ thinwat\n",
            "ศักดินัย sakdinai\n",
            "สุรศักดิ์ surasak\n"
          ]
        }
      ],
      "source": [
        "for th, en in zip(name_th[:10],name_en[:10]):\n",
        "    print(th,en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvW8xqT81ZSh"
      },
      "source": [
        "## TODO1: Preprocess dataset\n",
        "* You will need 2 vocabularies (1 for input and another for output)\n",
        "* DON'T FORGET TO INCLUDE special token for padding (for both input and output)\n",
        "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rv1Xd9A1ZSi",
        "outputId": "dece74ae-a492-41a7-f07d-2798157c7fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 10887 lines and 65 unique characters in your input data.\n"
          ]
        }
      ],
      "source": [
        "#Preprocessing\n",
        "input_chars = list(set(''.join(name_th)))\n",
        "output_chars = list(set(''.join(name_en)))\n",
        "data_size, vocab_size = len(name_th), len(input_chars)+1\n",
        "output_vocab_size = len(output_chars)+2#+2 for special end of sentence token/PADDING\n",
        "print('There are %d lines and %d unique characters in your input data.' % (data_size, vocab_size))\n",
        "maxlen = len( max(name_th, key=len)) #max input length\n",
        "maxlen_out = len( max(name_en, key=len)) #max output length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example:\n",
            "[4, 24, 19, 47, 56, 4, 34, 20, 54, 2, 60, 6]\n",
            "['ค', 'น', 'ด', 'ำ', 'โ', 'ค', 'ร', 'ต', 'เ', 'ก', '่', 'ง']\n",
            "\n",
            "Check X and Y of dataset\n",
            "torch.Size([10887, 20])\n",
            "torch.Size([10887, 20])\n",
            "\n",
            "Example:\n",
            "tensor([58,  2, 34, 39, 49, 40, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0]) \n",
            " ['ไ', 'ก', 'ร', 'ส', 'ี', 'ห', '์', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "tensor([12, 18,  3, 11, 19, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0]) \n",
            " ['k', 'r', 'a', 'i', 's', 'i', '<EOW>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ],
      "source": [
        "# Written by Jeans\n",
        "maxlen_out += 1 # add 1 for <EOW> token???\n",
        "\n",
        "#create  (EMBEDDING)  dictionaries to convert character to integer and vice versa\n",
        "input_chars = ['<PAD>'] + sorted(input_chars)\n",
        "output_chars = ['<PAD>','<EOW>'] + sorted(output_chars)\n",
        "# print(input_chars)\n",
        "input_char_to_idx = { char: idx for idx, char in enumerate(input_chars)}\n",
        "input_idx_to_char = { idx: char for idx, char in enumerate(input_chars)}\n",
        "input_encode = lambda x: [input_char_to_idx[char] for char in x]\n",
        "input_decode = lambda l: [input_idx_to_char[idx] for idx in l]\n",
        "\n",
        "output_char_to_idx = { char: idx for idx, char in enumerate(output_chars)}\n",
        "output_idx_to_char = { idx: char for idx, char in enumerate(output_chars)}\n",
        "output_encode = lambda x: [output_char_to_idx[char] for char in x]\n",
        "output_decode = lambda l: [output_idx_to_char[idx] for idx in l]\n",
        "\n",
        "# Example\n",
        "print(\"Example:\")\n",
        "print(input_encode(\"คนดำโครตเก่ง\"))\n",
        "print(input_decode(input_encode(\"คนดำโครตเก่ง\")))\n",
        "\n",
        "# print(\"Encoder Mapper:\")\n",
        "# print(input_char_to_idx)\n",
        "\n",
        "# print(\"Decoder Mapper:\")\n",
        "# print(output_char_to_idx)\n",
        "\n",
        "\n",
        "#PROCESS DATA FOR MODEL\n",
        "# tokenize and embed inputs\n",
        "# tokenize and embed outputs\n",
        "# pad inputs\n",
        "X = []\n",
        "for word in name_th:\n",
        "    word = [char for char in word]\n",
        "    X.append(torch.tensor(input_encode(word))) # convert to tensor shape (len(X), len(word))\n",
        "X = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0) # pad all sequences to same length with 0 : <PAD>\n",
        "\n",
        "Y = []\n",
        "for word in name_en:\n",
        "    word = [char for char in word] + ['<EOW>']\n",
        "    Y.append(torch.tensor(output_encode(word))) # convert to tensor shape (len(Y), len(word))\n",
        "Y = nn.utils.rnn.pad_sequence(Y, batch_first=True, padding_value=0) # pad all sequences to same length with 0 : <PAD>\n",
        "print(\"\\nCheck X and Y of dataset\")\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(\"\\nExample:\")\n",
        "print(X[0], \"\\n\", input_decode(list(X[0].detach().numpy().astype(int)))) # 0 is the first word in the dataset)\n",
        "print(Y[0], \"\\n\", output_decode(list(Y[0].detach().numpy().astype(int)))) # 0 is the first word in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo381I_t1ZSm",
        "outputId": "4467516a-90c8-477d-bc43-bb071b17a956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max input length: 20\n",
            "Max output length: 20\n"
          ]
        }
      ],
      "source": [
        "print(\"Max input length:\", maxlen)\n",
        "print(\"Max output length:\", maxlen_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W3aXyJBEC-j_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-yirzlseC9NS"
      },
      "outputs": [],
      "source": [
        "class NameDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    #FILL CODE HERE pass\n",
        "    self.encoded_X = X.long()\n",
        "    self.encoded_label = y.long()\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #FILL CODE HERE pass\n",
        "    return {\"x\" :self.encoded_X[idx], \"y\":self.encoded_label[idx]}\n",
        "\n",
        "  def __len__(self):\n",
        "    #FILL CODE HERE pass\n",
        "    return len(self.encoded_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qUPAB7LTDFOy"
      },
      "outputs": [],
      "source": [
        "class NameDataModule(L.LightningDataModule):\n",
        "\n",
        "  def __init__(self, train_data, y, batch_size, num_workers=0):\n",
        "      super().__init__()\n",
        "      self.train_data = train_data\n",
        "      self.y = y\n",
        "      self.batch_size = batch_size\n",
        "      self.num_workers = num_workers\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "    pass\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    '''turn encoded inputs into one-hot vectors'''\n",
        "    batched_one_hot_x = torch.stack([F.one_hot(dp[\"x\"], num_classes=vocab_size) for dp in batch])\n",
        "    batched_y = torch.stack([dp[\"y\"] for dp in batch])\n",
        "    return {\"x\": batched_one_hot_x, \"y\": batched_y}    \n",
        "\n",
        "  def train_dataloader(self):\n",
        "    train_dataset = NameDataset(self.train_data, self.y)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                                batch_size = self.batch_size,\n",
        "                                shuffle = True,\n",
        "                                collate_fn = self.collate_fn,\n",
        "                                num_workers = self.num_workers)\n",
        "    return train_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSG1FqK1ZSy"
      },
      "source": [
        "# Attention Mechanism\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAlOrhbismQp"
      },
      "source": [
        "## TODO 2: Code your own (key-value) attention mechnism\n",
        "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
        "* fill code for one_step_attention function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "avnlc6p9BZDv"
      },
      "outputs": [],
      "source": [
        "def one_step_attention(h, s_prev, attention_layer):\n",
        "    '''\n",
        "    h.shape = (batch_size, seq_len, hidden_size)\n",
        "    '''\n",
        "    #Split into Key-Value\n",
        "    key, value = torch.split(h, h.size(2) // 2, dim=2) # split hidden_size into 2 parts\n",
        "    #do concat with s_prev.\n",
        "    #hint: you will need to use s_prev.repeat(...) somehow so that it has the same dimension as the key\n",
        "    #hint2: s_prev.unsqueeze() could also be useful\n",
        "    s_prev = s_prev.unsqueeze(1).repeat((1, h.shape[1], 1))\n",
        "    concatenated = torch.cat((key, s_prev), dim=2) # (b, seq_len, hidden_size)\n",
        "\n",
        "    #Attention function###\n",
        "    # use layer(s) from your model to calculate attention_scores and then softmax\n",
        "    # calculate a context vector\n",
        "    f_attn = F.tanh(attention_layer(concatenated)) # (b, seq_len, 1)\n",
        "    attention_scores = F.softmax(f_attn, dim=1) # (b, seq_len, 1)\n",
        "    tmp = torch.mul(attention_scores, h) # (b, seq_len, hidden_size)\n",
        "    context = torch.sum(tmp,dim=1)\n",
        "    \n",
        "    return context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zWN02ZtuOIU"
      },
      "source": [
        "# Translation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0phyUQYg1ZS8"
      },
      "source": [
        "## TODO3: Create and train your encoder/decoder model here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji_rUPhK1ZS9"
      },
      "outputs": [],
      "source": [
        "class AttentionModel(L.LightningModule):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_h = #hidden dimensions for encoder\n",
        "        self.n_s = #hidden dimensions for decoder\n",
        "\n",
        "        #encoder can be any RNN of your choice\n",
        "\n",
        "        #decoder has to be (any) RNNCell since we will need to calculate attention for each timestep manually\n",
        "        self.decoder_lstm_cell = nn.LSTMCell(self.n_s, self.n_s)\n",
        "        self.output_layer = nn.Linear(self.n_s, len(output_vocab))\n",
        "        #attention\n",
        "\n",
        "\n",
        "    def forward(self, src, return_attention=False): #use return_attention only when you want to get the attention scores for visualizing\n",
        "        #pass the input to the encoder\n",
        "\n",
        "        #Initialize the LSTM states. We have to do this since we are using LSTMCell (https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
        "        #These states will get updated while we are decoding\n",
        "        decoder_s = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "        decoder_c = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "\n",
        "        #Iterate until max_output_length (Decoding)\n",
        "        prediction = torch.zeros((src.shape[0], \"max_output_length\", len(output_vocab))).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "        attention_scores = [] #to store the score for each step\n",
        "        for t in range(...):\n",
        "\n",
        "            #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
        "            context, attention_score = one_step_attention(...)\n",
        "\n",
        "            # Feed the context vector to the decoder.\n",
        "\n",
        "            # Pass the decoder hidden output to the output layer (softmax)\n",
        "\n",
        "            # Put the predicted output into the list for this timestep\n",
        "            prediction[:, t] = out\n",
        "\n",
        "        return (prediction, attention_scores if return_attention else None)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src = batch['x']\n",
        "        target = batch['y']\n",
        "        prediction,_ = self(src)\n",
        "        prediction = prediction.reshape(-1, len(output_vocab))\n",
        "        target = target.reshape(-1)\n",
        "        loss = self.criterion(prediction, target)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        src = batch['x']\n",
        "        with torch.no_grad():\n",
        "          prediction, attention_scores = self(src, return_attention=True)\n",
        "          prediction = F.softmax(prediction, dim=-1)\n",
        "          prediction = torch.argmax(prediction, dim=-1)\n",
        "          for pred in prediction:\n",
        "            print(\"\".join(output_vocab.lookup_tokens(pred.cpu().numpy())))\n",
        "        return prediction, attention_scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSM9dgDcCz1E"
      },
      "outputs": [],
      "source": [
        "model = AttentionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqrvmJalDLzF"
      },
      "outputs": [],
      "source": [
        "data_module = NameDataModule(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sFjzKX8SECo"
      },
      "outputs": [],
      "source": [
        "from lightning import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "wandb_logger = WandbLogger(project=\"hw3.1_attention\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGWSzS-X1ZTO"
      },
      "outputs": [],
      "source": [
        "trainer = L.Trainer(\n",
        "    max_epochs=100,\n",
        "    logger=wandb_logger\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZMi782c1ZTQ"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5BLw1Ir1ZTT"
      },
      "source": [
        "# Test Your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRLjZzBMtCdA"
      },
      "source": [
        "## TODO4: Test your model on 5 examples of your choice including your name!\n",
        "\n",
        "Example Output:\n",
        "```\n",
        "prayutthatha</s></s>aa</s></s>a</s>\n",
        "somchai</s></s></s></s>a</s></s>a</s></s></s></s></s>\n",
        "thanathon</s></s></s></s></s></s></s></s></s></s></s>\n",
        "newin</s>i</s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "suthep</s>he</s></s></s></s></s></s></s></s></s></s></s>\n",
        "prawit</s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "chatchachatti</s></s>i</s></s></s></s>\n",
        "```\n",
        "\n",
        "<font color='blue'>Paste your model predictions in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6stNACsUP9h-"
      },
      "outputs": [],
      "source": [
        "EXAMPLES = ['ประยุทธ','สมชาย','ธนาธร','เนวิน','สุเทพ','ประวิตร์','ชัชชาติ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbolC8XIhR3t"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsN71S9uQ9wo"
      },
      "outputs": [],
      "source": [
        "output = trainer.predict(model, predict_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o3893RL1ZT8"
      },
      "source": [
        "## TODO 5: Show your visualization of attention scores on one of your example\n",
        "\n",
        "<font color='blue'>Paste your visualization image in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHysSqYJ1ZUA"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdktVnMv1ZTh"
      },
      "outputs": [],
      "source": [
        "prediction, attention_scores = zip(*output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "BF6HD99lYlgQ",
        "outputId": "caaa0716-5b99-43e8-950f-dd0127d0fbb7"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(attn_viz, linewidth=0.5)\n",
        "ax.set_yticklabels(output_text,rotation=30)\n",
        "ax.set_xticklabels(xlabels,rotation=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1UkIsCztaMS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
