{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ8FRFIYMc5X"
      },
      "source": [
        "# HOMEWORK 6: TEXT CLASSIFICATION\n",
        "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
        "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
        "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n",
        "\n",
        "We will focus only on the Object Classification task for this homework.\n",
        "\n",
        "In this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n",
        "\n",
        "You will need to build 3 different models.\n",
        "\n",
        "1. A model based on tf-idf\n",
        "2. A model based on MUSE\n",
        "3. A model based on wangchanBERTa\n",
        "\n",
        "**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n",
        "\n",
        "This homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRlx5Mb5zkXw",
        "outputId": "18d913e0-aa6d-435b-931d-591386cb4ba8"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\n",
        "# !pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (0.19.6)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /home/jaf/.local/lib/python3.11/site-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (5.29.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (6.1.1)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: setproctitle in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (75.8.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: peft==0.10.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (24.2)\n",
            "Requirement already satisfied: psutil in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (6.1.1)\n",
            "Requirement already satisfied: pyyaml in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (2.6.0)\n",
            "Requirement already satisfied: transformers in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (4.30.1)\n",
            "Requirement already satisfied: tqdm in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (1.3.0)\n",
            "Requirement already satisfied: safetensors in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (0.5.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from peft==0.10.0) (0.28.1)\n",
            "Requirement already satisfied: filelock in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.12.0)\n",
            "Requirement already satisfied: requests in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from transformers->peft==0.10.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from transformers->peft==0.10.0) (0.13.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# Install transformers and thai2transformers\n",
        "!pip install wandb\n",
        "!pip install -q transformers==4.30.1 datasets evaluate thaixtransformers\n",
        "!pip install -q emoji pythainlp sefr_cut tinydb seqeval sentencepiece pydantic jsonlines\n",
        "!pip install peft==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YprqbOPMc5a"
      },
      "source": [
        "## Import libs for WangChanberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "heICP79cMc5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-14 19:50:21.935838: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-02-14 19:50:21.950874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739537421.965715  100291 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739537421.969564  100291 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-14 19:50:21.985718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import torch\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    CamembertTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "from thaixtransformers import Tokenizer\n",
        "from thaixtransformers.preprocess import process_transformers\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "import evaluate\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDHfX377rnp_"
      },
      "source": [
        "# Model 3 WangchanBERTa\n",
        "\n",
        "We ask you to train a WangchanBERTa-based model.\n",
        "\n",
        "We recommend you use the thaixtransformers fork (which we used in the PoS homework).\n",
        "https://github.com/PyThaiNLP/thaixtransformers\n",
        "\n",
        "The structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n",
        "\n",
        "Which WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n",
        "\n",
        "**Ans:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPaUf4PLMc5k"
      },
      "source": [
        "## Loading cleaned dataset and make it Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 10710\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1339\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1340\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "with open('template_cleaned_dataset.pkl', 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "# Extract tokenized text and labels\n",
        "label_2_num_map, num_2_label_map = dataset[\"label_2_num_map\"], dataset[\"num_2_label_map\"]\n",
        "train_texts, train_labels = dataset[\"train\"][\"input\"], dataset[\"train\"][\"label\"]\n",
        "val_texts, val_labels = dataset[\"val\"][\"input\"], dataset[\"val\"][\"label\"]\n",
        "test_texts, test_labels = dataset[\"test\"][\"input\"], dataset[\"test\"][\"label\"]\n",
        "\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
        "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
        "\n",
        "# Create DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "# Display dataset structure\n",
        "print(dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'ซื้อแอร์การ์ดมาจากเซเว่นค่ะ เบอร์นี้นะค่ะ แล้วเติมเงิน หรือสมัครอะไรไม่ได้เลยค่ะ',\n",
              " 'label': 3}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_dict[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Tokenizer for the model\n",
        "- \"airesearch/wangchanberta-base-att-spm-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text : ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร\n",
            "tokens : ['<s>', '', 'ศิลปะ', 'ไม่เป็น', 'เจ้านาย', 'ใคร', '<_>', 'และ', 'ไม่เป็น', 'ขี้ข้า', 'ใคร', '</s>']\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = Tokenizer(MODEL_NAME).from_pretrained(\n",
        "                f'{MODEL_NAME}',\n",
        "                revision='main',\n",
        "                model_max_length=416,)\n",
        "\n",
        "text = 'ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร'\n",
        "print('text :', text)\n",
        "tokens = []\n",
        "for i in tokenizer([text], is_split_into_words=True)['input_ids']:\n",
        "  tokens.append(tokenizer.decode(i))\n",
        "print('tokens :', tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8f22dd0b7b940c3aec1987c73dcc3c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10710 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "674ae23c1afa4aa08a33ad987c55b2e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1339 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd624bf3885b48dca0bad73e70be9d41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1340 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65baa6300af547229d2735e90facd602",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10710 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35f0ac80052f4969b4961b1911d8549e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1339 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "223eeb64805f42da9369594ec7f85fc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1340 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def lower_case_sentences(examples):\n",
        "  lower_cased_examples = examples\n",
        "  lower_cased_examples[\"text\"] = examples[\"text\"].lower()\n",
        "  return lower_cased_examples\n",
        "\n",
        "def preprocess_function(examples, tokenizer):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "dataset_dict_lower = dataset_dict.map(lower_case_sentences)\n",
        "tokenized_dataset = dataset_dict_lower.map(partial(preprocess_function, tokenizer=tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'payment', 1: 'package', 2: 'suspend', 3: 'internet', 4: 'phone_issues', 5: 'service', 6: 'nontruemove', 7: 'balance', 8: 'detail', 9: 'bill', 10: 'credit', 11: 'promotion', 12: 'mobile_setting', 13: 'iservice', 14: 'roaming', 15: 'truemoney', 16: 'information', 17: 'lost_stolen', 18: 'balance_minutes', 19: 'idd', 20: 'garbage', 21: 'ringtone', 22: 'rate', 23: 'loyalty_card', 24: 'contact', 25: 'officer'}\n",
            "num_labels 26\n"
          ]
        }
      ],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "id2label = {k: v for k, v in num_2_label_map.items()}\n",
        "label2id = {k: v for k, v in label_2_num_map.items()}\n",
        "num_labels = len(label2id)\n",
        "\n",
        "# print(label2id)\n",
        "print(id2label)\n",
        "print(\"num_labels\", num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the SequenceClassification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Cloning https://huggingface.co/JeansAthiwat/TRUEvoice_objective_wangchanberta_uncaesd_5epoch into local empty directory.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    f\"{MODEL_NAME}\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label, \n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"TRUEvoice_objective_wangchanberta_uncaesd_5epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaf/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjeansathiwat\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jaf/NLP_NoScope/codes/L06_Sequence_Classification/wandb/run-20250214_195037-1ntl3i95</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jeansathiwat/huggingface/runs/1ntl3i95' target=\"_blank\">starry-eyed-heartthrob-18</a></strong> to <a href='https://wandb.ai/jeansathiwat/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jeansathiwat/huggingface' target=\"_blank\">https://wandb.ai/jeansathiwat/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jeansathiwat/huggingface/runs/1ntl3i95' target=\"_blank\">https://wandb.ai/jeansathiwat/huggingface/runs/1ntl3i95</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6695' max='6695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6695/6695 35:49, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.211100</td>\n",
              "      <td>0.898832</td>\n",
              "      <td>0.744030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.693200</td>\n",
              "      <td>0.849231</td>\n",
              "      <td>0.761194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.506600</td>\n",
              "      <td>0.888763</td>\n",
              "      <td>0.764925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.380600</td>\n",
              "      <td>0.941494</td>\n",
              "      <td>0.776866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.317400</td>\n",
              "      <td>0.984565</td>\n",
              "      <td>0.776119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6695, training_loss=0.6852920332802508, metrics={'train_runtime': 2151.5685, 'train_samples_per_second': 24.889, 'train_steps_per_second': 3.112, 'total_flos': 1.14502644180288e+16, 'train_loss': 0.6852920332802508, 'epoch': 5.0})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a35f275b5df64721863fe51bfc84e1f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 1.00/402M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87af551cda4344d28714f0f3aa900bb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file runs/Feb14_19-50-30_JafPC/events.out.tfevents.1739537437.JafPC.100291.0:   0%|          | 1.00/9.1…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "To https://huggingface.co/JeansAthiwat/TRUEvoice_objective_wangchanberta_uncaesd_5epoch\n",
            "   24e119d..4b5d08f  main -> main\n",
            "\n",
            "To https://huggingface.co/JeansAthiwat/TRUEvoice_objective_wangchanberta_uncaesd_5epoch\n",
            "   4b5d08f..6c7338a  main -> main\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/JeansAthiwat/TRUEvoice_objective_wangchanberta_uncaesd_5epoch/commit/4b5d08f8ea4a849b9c13efe4b410127f94236e20'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \n",
            "The class this function is called from is 'WangchanbertaTokenizer'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    5,    10,  2391,  1501,  5365,   197,     8,   222,  1501, 21325,\n",
              "           197,     6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load pretrained tokenizer from Hugging Face\n",
        "#@title Choose Pretrained Model\n",
        "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "tokenizer = Tokenizer(model_name).from_pretrained(model_name)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "## Load your fine-tuned model from Hugging Face\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"/home/jaf/NLP_NoScope/codes/L06_Sequence_Classification/TRUEvoice_objective_wangchanberta_uncaesd_5epoch\") ## your model path from local or hugging face\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = torch.nn.functional.softmax(logits, dim=1).argmax().item()\n",
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
        "from typing import List, Dict\n",
        "\n",
        "def get_true_and_predicted_labels(\n",
        "    dataset: List[Dict],\n",
        "    model: PreTrainedModel,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    text_key: str = 'text',\n",
        "    label_key: str = 'label',\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        ") -> (List[int], List[int]):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for entry in dataset:\n",
        "            # Extract true label\n",
        "            y_true.append(entry[label_key])\n",
        "\n",
        "            # Tokenize input text\n",
        "            inputs = tokenizer(entry[text_key], return_tensors=\"pt\", truncation=True, padding=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(**inputs).logits\n",
        "\n",
        "            # Get predicted label\n",
        "            predicted_label = torch.argmax(torch.nn.functional.softmax(outputs, dim=1), dim=1).item()\n",
        "            y_pred.append(predicted_label)\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Assuming 'tokenized_dataset' is a dictionary containing your datasets\n",
        "val_data = tokenized_dataset[\"validation\"]\n",
        "test_data = tokenized_dataset[\"test\"]\n",
        "\n",
        "# Get true and predicted labels for the test set\n",
        "y_test_true, y_test_pred = get_true_and_predicted_labels(test_data, model, tokenizer)\n",
        "\n",
        "# Get true and predicted labels for the validation set\n",
        "y_val_true, y_val_pred = get_true_and_predicted_labels(val_data, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 0.7647498132935027\n",
            "Test accuracy: 0.7611940298507462\n"
          ]
        }
      ],
      "source": [
        "val_acc = accuracy_score(y_val_true, y_val_pred)\n",
        "test_acc = accuracy_score(y_test_true, y_test_pred)\n",
        "\n",
        "print(f\"Validation accuracy: {val_acc}\")\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
