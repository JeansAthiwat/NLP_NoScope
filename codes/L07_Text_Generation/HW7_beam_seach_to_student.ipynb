{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "pDRbfJqgPfdr",
      "metadata": {
        "id": "pDRbfJqgPfdr"
      },
      "source": [
        "# HW7: Beam Search Decoding - News Headline Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fSDxXAqSPeBB",
      "metadata": {
        "id": "fSDxXAqSPeBB"
      },
      "source": [
        "In this exercise, you are going to learn and implement decoding techniques for sequence generation. Usually, the sequence is generated word-by-word from a model. In each step, the model predicted the most likely word based on the predicted words in previous steps (this is called auto-regressive decoding).\n",
        "\n",
        "As such, it is very important how you decide on what to predicted at each step, as it will be conditioned on to predicted all of the following steps. We will implement two of main decoding techniques introduced in the lecture: **Greedy Decoding** and **Beam Search Decoding**. Greedy Decoding immediately chooses the word with best score at each step, while Beam Search Decoding focuses on the sequence that give the best score overall.\n",
        "\n",
        "To complete this exercise, you will need to complete the methods for decoding for a text generation model trained on [New York Times Comments and Headlines dataset](https://www.kaggle.com/aashita/nyt-comments). The model is trained to predict a headline for the news given seed text. You do not need to train any model model in this exercise as we provide both the pretrained model and dictionary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YFlSAvCfiZXf",
      "metadata": {
        "id": "YFlSAvCfiZXf"
      },
      "source": [
        "## Download model and vocab and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "q5gRmwtdiYjp",
      "metadata": {
        "id": "q5gRmwtdiYjp"
      },
      "outputs": [],
      "source": [
        "# !wget -O vocab.txt https://www.dropbox.com/s/ht12ua9vpkep6l8/hw9_vocab.txt?dl=0\n",
        "# !wget -O model.bin https://www.dropbox.com/s/okmri7cnd729rr5/hw9_model.bin?dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "i9ITxmo5i-s6",
      "metadata": {
        "id": "i9ITxmo5i-s6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "VcDZCYkEi0b4",
      "metadata": {
        "id": "VcDZCYkEi0b4"
      },
      "outputs": [],
      "source": [
        "class RNNmodel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dropout_rate):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, 128, num_layers=2,\n",
        "                     batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedding = self.embedding(src)\n",
        "        output,_ = self.rnn(embedding)\n",
        "        output = self.dropout(output)\n",
        "        prediction = self.fc2(output)\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6bZt9R0JjL8l",
      "metadata": {
        "id": "6bZt9R0JjL8l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNNmodel(\n",
              "  (embedding): Embedding(10054, 64)\n",
              "  (rnn): LSTM(64, 128, num_layers=2, batch_first=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc2): Linear(in_features=128, out_features=10054, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"vocab.txt\") as f:\n",
        "  vocab_file = f.readlines()\n",
        "embedding_dim = 64\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = RNNmodel(len(vocab_file), embedding_dim, dropout_rate)\n",
        "model.load_state_dict(torch.load(\"model.bin\",map_location='cpu'))\n",
        "model.eval()\n",
        "model.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "mOEWXsXzjpn6",
      "metadata": {
        "id": "mOEWXsXzjpn6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size: 10054\n",
            "['<unk>', '<pad>', '<eos>', 'the', 'a', 'to', 'of', 's', 'in', 'for', 'and', 'trump', 'unknown', 'on', 'is', 'with', 'new', '']\n"
          ]
        }
      ],
      "source": [
        "vocab = [v.strip() for v in vocab_file]\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab Size: {vocab_size}\")\n",
        "print(vocab[:18])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "sem3jbjoF_d8",
      "metadata": {
        "id": "sem3jbjoF_d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "[3, 4, 6, 5, 0]\n",
            "['the', 'a', 'of', 'to', '<unk>']\n",
            "the a of to <unk>\n"
          ]
        }
      ],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "tokenizer = Tokenizer(WordLevel(stoi, unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenized_text = tokenizer.encode(\"the a of to unknowns\")\n",
        "print(tokenized_text)\n",
        "print(tokenized_text.ids)\n",
        "print(tokenized_text.tokens)\n",
        "print(tokenizer.decode(tokenized_text.ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rSt1yuR19co-",
      "metadata": {
        "id": "rSt1yuR19co-"
      },
      "source": [
        "## 1. TODO: Greedy decode\n",
        "Normally, in sequence generation task, the model will continue generating tokens until an end-of-sequence symbol appear or the maximum length is reached. For this task:\n",
        "- The end-of-sequence symbol is \"< eos >\" and its index is 2\n",
        "- Use the maximum generation length of 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "oUCajb2BvKnN",
      "metadata": {
        "id": "oUCajb2BvKnN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "eos_token = '<eos>'\n",
        "eos_index = vocab.index(eos_token)\n",
        "max_gen_length = 15\n",
        "print(eos_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e6638613",
      "metadata": {
        "id": "e6638613"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(seed_text, tokenizer):\n",
        "    \"\"\"Greedy decodes with seed text.\n",
        "\n",
        "        Args:\n",
        "        seed_text: The seed string to be used as initial input to the model.\n",
        "        tokenizer: The tokenizer for converting word to index and back.\n",
        "\n",
        "        Your code should do the followings:\n",
        "          1. Convert current_text to sequences of indices\n",
        "          2. Predict the next token using the model and choose the token with the highest score as output\n",
        "          3. Append the predicted index to current_text\n",
        "          4. Loop until completion\n",
        "          5. Return text prediction and a list of probabilities of each step\n",
        "\n",
        "        You do not need to stop early when end-of-sequence token is generated and can continue decoding\n",
        "        until max_gen_length is reached. We can filter the eos token out later.\n",
        "    \"\"\"\n",
        "    ids_seq = tokenizer.encode(seed_text).ids\n",
        "    \n",
        "    seq_probs = []\n",
        "    output_ids_seq = ids_seq.copy()\n",
        "    \n",
        "    ids_seq = torch.tensor(ids_seq, requires_grad=False).unsqueeze(0)\n",
        "    \n",
        "    while len(output_ids_seq) < max_gen_length:\n",
        "        logits = model(ids_seq)\n",
        "        last_token_logits = logits.squeeze(0)[-1]\n",
        "        probs = torch.softmax(last_token_logits, dim=-1)\n",
        "        seq_probs.append(torch.max(probs).item())\n",
        "        output_ids_seq.append(torch.argmax(probs).item())\n",
        "        # print(output_ids_seq[-1])\n",
        "        # print(tokenizer.decode(output_ids_seq))\n",
        "        ids_seq = torch.tensor(output_ids_seq).unsqueeze(0)\n",
        "        # print(ids_seq.shape)\n",
        "    \n",
        "    output = tokenizer.decode(output_ids_seq)\n",
        "    return output,probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9f78f6c8",
      "metadata": {
        "id": "9f78f6c8"
      },
      "outputs": [],
      "source": [
        "def clean_output(text, eos_token):\n",
        "    \"\"\"Drop eos_token and every words that follow\"\"\"\n",
        "    return text.split(eos_token)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bdd42c8a",
      "metadata": {
        "id": "bdd42c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "to encourage creativity in the new york bill \n",
            "america s lethal export \n",
            "people to balloon to make a criminal with a dog with a callous rival \n",
            "next phenom english clubs 2 call another deal in the same arrivals \n",
            "picture perfect chapter a spot of view of banning care \n",
            "on the catwalk in saudi arabia \n"
          ]
        }
      ],
      "source": [
        "sample_seeds = [\"to\", \"america\", \"people\", \"next\", \"picture\", \"on\"]\n",
        "for seed in sample_seeds:\n",
        "    output, probs = greedy_decode(seed, tokenizer)\n",
        "    output = clean_output(output, eos_token)\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h99jCVvjUvFT",
      "metadata": {
        "id": "h99jCVvjUvFT"
      },
      "source": [
        "Your output should be:\n",
        "\n",
        "*   to encourage creativity in the new york bill\n",
        "*   america s lethal export\n",
        "*   people to balloon to make a criminal with a dog with a callous rival\n",
        "*   next phenom english clubs 2 call another deal in the same arrivals\n",
        "*   picture perfect chapter a spot of view of banning care  \n",
        "*   on the catwalk in saudi arabia\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7553f608",
      "metadata": {
        "id": "7553f608"
      },
      "source": [
        "## 2. TODO: Beam search decode\n",
        "\n",
        "Another well-known decoding method is beam search decoding that focuses more on the overall sequence score.\n",
        "\n",
        "Instead of greedily choosing the token with the highest score for each step, beam search decoding expands all possible next tokens and keeps the __k__ most likely sequence at each step, where __k__ is a user-specified beam size. A sequence score is also calculated according user-specified cal_score() function.\n",
        "The beam with the highest score after the decoding process is done will be the output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8kArvA-6xLmQ",
      "metadata": {
        "id": "8kArvA-6xLmQ"
      },
      "source": [
        "There are a few things that you need to know before implementing a beam search decoder:\n",
        "- When the eos token is produced, you can stop expanding that beam\n",
        "- However, the ended beams must be sorted together with active beams\n",
        "- The decoding ends when every beams are either ended or reached the maximum length, but for this task, you can continue decoding until the max_gen_len is reached\n",
        "- We usually work with probability in log scale to avoid numerical underflow. You should use np.log(score) before any calculation\n",
        "- **As probabilities for some classes will be very small, you must add a very small value to the score before taking log e.g np.log(prob + 0.00000001)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MQRZftUYxcCU",
      "metadata": {
        "id": "MQRZftUYxcCU"
      },
      "source": [
        "#### Sequence Score\n",
        "The naive way to calculate the sequence score is to __multiply every token scores__ together. However, doing so will make the decoder prefer shorter sequence as you multiply the sequence score with a value between \\[0,1\\] for every tokens in the sequence. Thus, we usually normalize the sequence score with its length by calculating its __geometric mean__ instead.\n",
        "\n",
        "**You should do this in log scale**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d76c6f52",
      "metadata": {
        "id": "d76c6f52"
      },
      "outputs": [],
      "source": [
        "def cal_score(score_list, length, normalized=False): #cal score for each beam from a list of probs\n",
        "    # sum all elements in the list\n",
        "    seq_score = np.sum(score_list)\n",
        "    if normalized:\n",
        "        seq_score /= length\n",
        "    return seq_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "69868a8a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[5, 30]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_seed = tokenizer.encode(\"to be\").ids\n",
        "encoded_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bb1dd75a",
      "metadata": {
        "id": "bb1dd75a"
      },
      "outputs": [],
      "source": [
        "def beam_search_decode(seed_text, max_gen_len, tokenizer, beam_size=5, normalized=False):\n",
        "    \"\"\"We will do beam search decoing using seed text in this function.\n",
        "\n",
        "    Output:\n",
        "    beams: A list of top k beams after the decoding ended, each beam is a list of\n",
        "      [seed_text, list of scores, length]\n",
        "\n",
        "    Your code should do the followings:\n",
        "    1.Loop until max_gen_len is reached.\n",
        "    2.During each step, loop thorugh each beam and use it to predict the next word.\n",
        "      If a beam is already ended, continues without expanding.\n",
        "    3.Sort all hypotheses according to cal_score().\n",
        "    4.Keep top k hypotheses to be used at the next step.\n",
        "    \"\"\"\n",
        "    # For each beam we will store (generated text, list of scores, and current length, is_finished)\n",
        "    # Add initial beam\n",
        "    beams = [[[seed_text], [], 0, False]]\n",
        "    for _ in range(max_gen_len):\n",
        "        new_beams = []\n",
        "        # Loop through the top k beams (garunteed to be sorted and have only k elements)\n",
        "        for beam in beams:\n",
        "            if beam[3]:\n",
        "                new_beams.append(beam)\n",
        "                continue\n",
        "            # Get the current text and length\n",
        "            current_text = beam[0][-1]\n",
        "            list_of_scores = beam[1]\n",
        "            length = beam[2]\n",
        "\n",
        "            print(current_text)\n",
        "            # predict the next token from the current text by tokenizing it and pass it to the model\n",
        "            tokenized_text = tokenizer.encode(current_text).ids\n",
        "            tokenized_text = torch.tensor(tokenized_text, requires_grad=False).unsqueeze(0)\n",
        "            logits = model(tokenized_text)\n",
        "            last_token_logits = logits.squeeze(0)[-1]\n",
        "            probs = np.log(torch.softmax(last_token_logits, dim=-1).detach())\n",
        "            # append the first k best tokens to the current text first (we will filter out the top k later)\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, beam_size)\n",
        "            for i in range(beam_size):\n",
        "                #append the word to the new text (dont forget to decode the word)\n",
        "                new_text = current_text + ' ' + vocab[top_k_indices[i].item()]\n",
        "                #append the score to the new list of scores\n",
        "                new_scores = list_of_scores.copy()\n",
        "                new_scores.append(top_k_probs[i].item())\n",
        "                #append the new beam to the list of new beams\n",
        "                new_beams.append([[new_text], new_scores, length+1, top_k_indices[i].item() == eos_index])\n",
        "        # Sort the new beams\n",
        "        new_beams.sort(key=lambda x: cal_score(x[1], x[2], normalized), reverse=True)\n",
        "        # Keep only the top k beams\n",
        "        beams = new_beams[:beam_size]\n",
        "                \n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "    return beams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i_eqGDA09zqk",
      "metadata": {
        "id": "i_eqGDA09zqk"
      },
      "source": [
        "## 3. Generate!\n",
        "Generate 6 sentences based on the given seed texts.\n",
        "\n",
        "Decode with the provided seed texts with beam_size 5. Compare the results between greedy, normalized, and unnormalized decoding.\n",
        "\n",
        "Print the result using greedy decoding and top 2 results each using unnormalized and normalized decoing for each seed text.\n",
        "\n",
        "Also, print scores of each candidate according to cal_score(). Use normalization for greedy decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d40a3cb0",
      "metadata": {
        "id": "d40a3cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "to\n",
            "to encourage\n",
            "to predict\n",
            "to finance\n",
            "to consult\n",
            "to enjoy\n",
            "to encourage creativity\n",
            "to consult exploring\n",
            "to finance season\n",
            "to predict gentrification\n",
            "to consult cabby\n",
            "to consult exploring recipes\n",
            "to encourage creativity in\n",
            "to encourage creativity for\n",
            "to finance season is\n",
            "to predict gentrification look\n",
            "to encourage creativity in the\n",
            "to consult exploring recipes up\n",
            "to consult exploring recipes and\n",
            "to consult exploring recipes for\n",
            "to consult exploring recipes with\n",
            "to consult exploring recipes up the\n",
            "to consult exploring recipes and the\n",
            "to consult exploring recipes with the\n",
            "to consult exploring recipes up a\n",
            "to consult exploring recipes for new\n",
            "to consult exploring recipes up the pacific\n",
            "to consult exploring recipes up the least\n",
            "to consult exploring recipes for new jersey\n",
            "to consult exploring recipes up the ocean\n",
            "to consult exploring recipes with the family\n",
            "to consult exploring recipes up the pacific northwest\n",
            "to consult exploring recipes up the least of\n",
            "to consult exploring recipes up the least of the\n",
            "to consult exploring recipes up the least of the week\n",
            "america\n",
            "america s\n",
            "america is\n",
            "america and\n",
            "america to\n",
            "america of\n",
            "america is a\n",
            "america s lethal\n",
            "america s uncivil\n",
            "america s desert\n",
            "america s handling\n",
            "america s lethal export\n",
            "america s desert aisles\n",
            "america s handling of\n",
            "america s handling and\n",
            "america s handling is\n",
            "america s handling and antiapartheid\n",
            "america s handling of statin\n",
            "america s handling is free\n",
            "america s handling is free inquiry\n",
            "america s handling is free with\n",
            "america s handling is free with a\n",
            "america s handling is free with a watchful\n",
            "america s handling is free with a watchful task\n",
            "people\n",
            "people to\n",
            "people of\n",
            "people and\n",
            "people is\n",
            "people roars\n",
            "people to balloon\n",
            "people to break\n",
            "people to throw\n",
            "people to track\n",
            "people to text\n",
            "people to balloon to\n",
            "people to balloon s\n",
            "people to break on\n",
            "people to balloon for\n",
            "people to throw at\n",
            "people to break on a\n",
            "people to break on the\n",
            "people to balloon for a\n",
            "people to balloon for the\n",
            "people to throw at the\n",
            "people to break on a child\n",
            "people to balloon for a criminal\n",
            "people to break on the bench\n",
            "people to break on a vitamin\n",
            "people to balloon for a desperate\n",
            "people to break on a vitamin on\n",
            "people to balloon for a criminal with\n",
            "people to break on a vitamin with\n",
            "people to break on the bench to\n",
            "people to balloon for a criminal with a\n",
            "people to break on the bench to be\n",
            "people to break on a vitamin with a\n",
            "people to balloon for a criminal with trump\n",
            "people to balloon for a criminal with a dog\n",
            "people to balloon for a criminal with trump s\n",
            "people to balloon for a criminal with a second\n",
            "people to balloon for a criminal with a dog s\n",
            "people to balloon for a criminal with a second fiddle\n",
            "people to balloon for a criminal with a dog with\n",
            "people to balloon for a criminal with a dog with a\n",
            "people to balloon for a criminal with a dog s favorite\n",
            "people to balloon for a criminal with a dog s favorite student\n",
            "people to balloon for a criminal with a dog with a furor\n",
            "next\n",
            "next phenom\n",
            "next s\n",
            "next a\n",
            "next and\n",
            "next the\n",
            "next phenom english\n",
            "next phenom appeal\n",
            "next s blist\n",
            "next and a\n",
            "next phenom offers\n",
            "next phenom english clubs\n",
            "next phenom appeal of\n",
            "next s blist revue\n",
            "next phenom appeal and\n",
            "next phenom appeal to\n",
            "next phenom english clubs 2\n",
            "next phenom english clubs 1\n",
            "next phenom english clubs the\n",
            "next phenom english clubs stand\n",
            "next phenom english clubs 1 a\n",
            "next phenom english clubs 1 the\n",
            "next phenom english clubs stand a\n",
            "next phenom english clubs stand the\n",
            "next phenom english clubs 1 a chance\n",
            "next phenom english clubs 1 a spot\n",
            "next phenom english clubs 1 a glowing\n",
            "next phenom english clubs 1 the new\n",
            "next phenom english clubs 1 a chance to\n",
            "next phenom english clubs 1 a spot and\n",
            "next phenom english clubs 1 a chance for\n",
            "next phenom english clubs 1 a spot of\n",
            "next phenom english clubs 1 a chance to get\n",
            "next phenom english clubs 1 a chance to the\n",
            "next phenom english clubs 1 a chance to a\n",
            "next phenom english clubs 1 a chance to be\n",
            "next phenom english clubs 1 a chance to get a\n",
            "next phenom english clubs 1 a chance to get the\n",
            "next phenom english clubs 1 a chance to get an\n",
            "next phenom english clubs 1 a chance to be back\n",
            "next phenom english clubs 1 a chance to get a new\n",
            "next phenom english clubs 1 a chance to get a political\n",
            "next phenom english clubs 1 a chance to get a good\n",
            "next phenom english clubs 1 a chance to get a good act\n",
            "next phenom english clubs 1 a chance to get a new good\n",
            "next phenom english clubs 1 a chance to get a new life\n",
            "next phenom english clubs 1 a chance to get a good act into\n",
            "next phenom english clubs 1 a chance to get a good act into a\n",
            "next phenom english clubs 1 a chance to get a good act into a library\n",
            "picture\n",
            "picture perfect\n",
            "picture korean\n",
            "picture strengthens\n",
            "picture contest\n",
            "picture gdawg\n",
            "picture korean a\n",
            "picture perfect chapter\n",
            "picture perfect parole\n",
            "picture perfect city\n",
            "picture perfect use\n",
            "picture perfect use a\n",
            "picture perfect parole and\n",
            "picture perfect parole on\n",
            "picture perfect use coffee\n",
            "picture korean a bonanza\n",
            "picture korean a bonanza of\n",
            "picture korean a bonanza for\n",
            "picture perfect use coffee feels\n",
            "picture perfect parole and new\n",
            "picture korean a bonanza of none\n",
            "picture korean a bonanza of contemplation\n",
            "picture korean a bonanza of pancakes\n",
            "picture korean a bonanza of increased\n",
            "picture korean a bonanza of contemplation times\n",
            "picture korean a bonanza of increased market\n",
            "picture korean a bonanza of none peace\n",
            "picture korean a bonanza of contemplation times of\n",
            "picture korean a bonanza of contemplation times and\n",
            "picture korean a bonanza of contemplation times s\n",
            "picture korean a bonanza of contemplation times and the\n",
            "picture korean a bonanza of contemplation times of trump\n",
            "picture korean a bonanza of contemplation times of the\n",
            "picture korean a bonanza of contemplation times of trump s\n",
            "picture korean a bonanza of contemplation times and the times\n",
            "picture korean a bonanza of contemplation times of the cancer\n",
            "picture korean a bonanza of contemplation times of trump s son\n",
            "picture korean a bonanza of contemplation times and the times s\n",
            "picture korean a bonanza of contemplation times of trump s prime\n",
            "picture korean a bonanza of contemplation times and the times s cancer\n",
            "picture korean a bonanza of contemplation times of trump s prime directive\n",
            "on\n",
            "on the\n",
            "on a\n",
            "on trump\n",
            "on your\n",
            "on new\n",
            "on trump s\n",
            "on the catwalk\n",
            "on the world\n",
            "on the whole30\n",
            "on the billboard\n",
            "on the billboard chart\n",
            "on the whole30 diet\n",
            "on the catwalk in\n",
            "on the catwalk of\n",
            "on the catwalk on\n",
            "on the whole30 diet vowing\n",
            "on the catwalk in saudi\n",
            "on the whole30 diet season\n",
            "on the whole30 diet dead\n",
            "on the whole30 diet vowing to\n",
            "on the whole30 diet vowing for\n",
            "on the catwalk in saudi arabia\n",
            "on the whole30 diet vowing of\n",
            "on the whole30 diet vowing to eat\n",
            "on the whole30 diet vowing to be\n",
            "on the whole30 diet vowing to keep\n",
            "on the whole30 diet vowing to eat smarter\n",
            "on the whole30 diet vowing to eat carbs\n",
            "on the whole30 diet vowing to eat salad\n",
            "on the whole30 diet vowing to eat smarter carbs\n",
            "on the whole30 diet vowing to eat carbs to\n",
            "on the whole30 diet vowing to eat carbs for\n",
            "on the whole30 diet vowing to eat smarter carbs for\n",
            "on the whole30 diet vowing to eat smarter carbs to\n",
            "on the whole30 diet vowing to eat smarter carbs and\n",
            "on the whole30 diet vowing to eat smarter carbs to be\n",
            "on the whole30 diet vowing to eat smarter carbs for because\n",
            "on the whole30 diet vowing to eat smarter carbs for not\n",
            "on the whole30 diet vowing to eat smarter carbs to be insufficient\n",
            "on the whole30 diet vowing to eat smarter carbs for because you\n",
            "on the whole30 diet vowing to eat smarter carbs for because we\n",
            "on the whole30 diet vowing to eat smarter carbs for because you like\n",
            "on the whole30 diet vowing to eat smarter carbs for because you want\n"
          ]
        }
      ],
      "source": [
        "sample_seeds = [\"to\", \"america\", \"people\", \"next\", \"picture\", \"on\"]\n",
        "for seed in sample_seeds:\n",
        "    output = beam_search_decode(seed, max_gen_length, tokenizer, beam_size=5, normalized=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "icfu6pOzWUSt",
      "metadata": {
        "id": "icfu6pOzWUSt"
      },
      "source": [
        "Your output should be:\n",
        "\n",
        "\n",
        "```\n",
        "-Greedy-\n",
        "to encourage creativity in the new york bill  0.12\n",
        "-Unnormalized-\n",
        "To Consult Exploring Recipes For New Jersey 0.00\n",
        "To Consult Exploring Recipes Up The Pacific Northwest 0.00\n",
        "-Normalized-\n",
        "To Consult Exploring Recipes Up The Pacific Northwest 0.17\n",
        "To Consult Exploring Recipes Up The Least Of The Week 0.16\n",
        "\n",
        "-Greedy-\n",
        "america s lethal export  0.35\n",
        "-Unnormalized-\n",
        "America S Lethal Export 0.02\n",
        "America S Desert Aisles 0.01\n",
        "-Normalized-\n",
        "America S Lethal Export 0.25\n",
        "America S Desert Aisles 0.20\n",
        "\n",
        "-Greedy-\n",
        "people to balloon to make a criminal with a dog with a callous rival  0.16\n",
        "-Unnormalized-\n",
        "People To Balloon For A Criminal 0.00\n",
        "People To Balloon For A Criminal With Trump 0.00\n",
        "-Normalized-\n",
        "People To Balloon For A Criminal With A Second Fiddle 0.13\n",
        "People To Balloon For A Criminal With Trump 0.13\n",
        "\n",
        "-Greedy-\n",
        "next phenom english clubs 2 call another deal in the same arrivals  0.15\n",
        "-Unnormalized-\n",
        "Next S Blist Revue 0.00\n",
        "Next Phenom English Clubs 1 A Chance To Be Back 0.00\n",
        "-Normalized-\n",
        "Next S Blist Revue 0.14\n",
        "Next Phenom English Clubs 1 A Chance To Be Back 0.14\n",
        "\n",
        "-Greedy-\n",
        "picture perfect chapter a spot of view of banning care  0.09\n",
        "-Unnormalized-\n",
        "Picture Perfect Use Coffee 0.00\n",
        "Picture Korean A Bonanza Of Pancakes 0.00\n",
        "-Normalized-\n",
        "Picture Korean A Bonanza Of Contemplation Times Of Trump S Son 0.12\n",
        "Picture Korean A Bonanza Of Pancakes 0.07\n",
        "\n",
        "-Greedy-\n",
        "on the catwalk in saudi arabia  0.25\n",
        "-Unnormalized-\n",
        "On The Billboard Chart 0.00\n",
        "On The Catwalk In Saudi Arabia 0.00\n",
        "-Normalized-\n",
        "On The Whole30 Diet Vowing To Eat Smarter Carbs To Be 0.27\n",
        "On The Whole30 Diet Vowing To Eat Smarter Carbs For Because 0.26\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tquJVskBeM9m",
      "metadata": {
        "id": "tquJVskBeM9m"
      },
      "source": [
        "# Answer Questions in MyCourseVille!\n",
        "\n",
        "Use the seed word \"usa\" to answer questions in MCV."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
