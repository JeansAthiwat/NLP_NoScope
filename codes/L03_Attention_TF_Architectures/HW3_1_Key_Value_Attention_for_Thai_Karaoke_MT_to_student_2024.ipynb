{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfUmXr1D1ZSR"
      },
      "source": [
        "# Key-Value Attention for Thai Karaoke Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
        "\n",
        "In this homework, you will create an MT model with attention mechnism that coverts names of Thai 2019 MP candidates from Thai script to Roman(Latin) script. E.g. นิยม-->niyom\n",
        "\n",
        "The use of Pytorch Lightning is optional but recommended. You can use Pytorch if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "18KMSkqZ-Pt-"
      },
      "outputs": [],
      "source": [
        "!pip install lightning wandb -q\n",
        "# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKCBCWKARZEx",
        "outputId": "6ad8a585-fd68-44ee-deac-37143137cbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ka2TN8IV1ZSU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "mpl.font_manager.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n",
        "mpl.rc('font', family='TH Sarabun New')\n",
        "import torch\n",
        "# import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-f_s6vX1ZSZ"
      },
      "source": [
        "## Load Dataset\n",
        "We have generated a toy dataset using names of Thai MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
        "\n",
        "```\n",
        "ไกรสีห์ kraisi\n",
        "พัชรี phatri\n",
        "ธีระ thira\n",
        "วุฒิกร wutthikon\n",
        "ไสว sawai\n",
        "สัมภาษณ์  samphat\n",
        "วศิน wasin\n",
        "ทินวัฒน์ thinwat\n",
        "ศักดินัย sakdinai\n",
        "สุรศักดิ์ surasak\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jte-Csrf-4kd",
        "outputId": "a1ba364b-64c2-4875-873c-90bc1808a4be"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L9zXp7KH1ZSa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('mp_name_th_en.csv') as csvfile:\n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    name_th = []\n",
        "    name_en = []\n",
        "    for row in readCSV:\n",
        "        temp_th = row[0]\n",
        "        temp_en = row[1]\n",
        "\n",
        "        name_th.append(temp_th)\n",
        "        name_en.append(temp_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZCsqrXxu1ZSe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ไกรสีห์ kraisi\n",
            "พัชรี phatri\n",
            "ธีระ thira\n",
            "วุฒิกร wutthikon\n",
            "ไสว sawai\n",
            "สัมภาษณ์  samphat\n",
            "วศิน wasin\n",
            "ทินวัฒน์ thinwat\n",
            "ศักดินัย sakdinai\n",
            "สุรศักดิ์ surasak\n"
          ]
        }
      ],
      "source": [
        "for th, en in zip(name_th[:10],name_en[:10]):\n",
        "    print(th,en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvW8xqT81ZSh"
      },
      "source": [
        "## TODO1: Preprocess dataset\n",
        "* You will need 2 vocabularies (1 for input and another for output)\n",
        "* DON'T FORGET TO INCLUDE special token for padding (for both input and output)\n",
        "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rv1Xd9A1ZSi",
        "outputId": "dece74ae-a492-41a7-f07d-2798157c7fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 10887 lines and 65 unique characters in your input data.\n"
          ]
        }
      ],
      "source": [
        "#Preprocessing\n",
        "input_chars = list(set(''.join(name_th)))\n",
        "output_chars = list(set(''.join(name_en)))\n",
        "data_size, vocab_size = len(name_th), len(input_chars)+1\n",
        "output_vocab_size = len(output_chars)+2#+2 for special end of sentence token/PADDING\n",
        "print('There are %d lines and %d unique characters in your input data.' % (data_size, vocab_size))\n",
        "maxlen = len( max(name_th, key=len)) #max input length\n",
        "maxlen_out = len( max(name_en, key=len)) #max output length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example:\n",
            "[4, 24, 19, 47, 56, 4, 34, 20, 54, 2, 60, 6]\n",
            "['ค', 'น', 'ด', 'ำ', 'โ', 'ค', 'ร', 'ต', 'เ', 'ก', '่', 'ง']\n",
            "\n",
            "Check X and Y of dataset\n",
            "torch.Size([10887, 20])\n",
            "torch.Size([10887, 20])\n",
            "\n",
            "Example:\n",
            "tensor([58,  2, 34, 39, 49, 40, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0]) \n",
            " ['ไ', 'ก', 'ร', 'ส', 'ี', 'ห', '์', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "tensor([12, 18,  3, 11, 19, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0]) \n",
            " ['k', 'r', 'a', 'i', 's', 'i', '<EOW>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ],
      "source": [
        "# Written by Jeans\n",
        "maxlen_out += 1 # add 1 for <EOW> token???\n",
        "\n",
        "#create  (EMBEDDING)  dictionaries to convert character to integer and vice versa\n",
        "input_chars = ['<PAD>'] + sorted(input_chars)\n",
        "output_chars = ['<PAD>','<EOW>'] + sorted(output_chars)\n",
        "# print(input_chars)\n",
        "input_char_to_idx = { char: idx for idx, char in enumerate(input_chars)}\n",
        "input_idx_to_char = { idx: char for idx, char in enumerate(input_chars)}\n",
        "input_encode = lambda x: [input_char_to_idx[char] for char in x]\n",
        "input_decode = lambda l: [input_idx_to_char[idx] for idx in l]\n",
        "\n",
        "output_char_to_idx = { char: idx for idx, char in enumerate(output_chars)}\n",
        "output_idx_to_char = { idx: char for idx, char in enumerate(output_chars)}\n",
        "output_encode = lambda x: [output_char_to_idx[char] for char in x]\n",
        "output_decode = lambda l: [output_idx_to_char[idx] for idx in l]\n",
        "\n",
        "# Example\n",
        "print(\"Example:\")\n",
        "print(input_encode(\"คนดำโครตเก่ง\"))\n",
        "print(input_decode(input_encode(\"คนดำโครตเก่ง\")))\n",
        "\n",
        "# print(\"Encoder Mapper:\")\n",
        "# print(input_char_to_idx)\n",
        "\n",
        "# print(\"Decoder Mapper:\")\n",
        "# print(output_char_to_idx)\n",
        "\n",
        "\n",
        "#PROCESS DATA FOR MODEL\n",
        "# tokenize and embed inputs\n",
        "# tokenize and embed outputs\n",
        "# pad inputs\n",
        "X = []\n",
        "for word in name_th:\n",
        "    word = [char for char in word]\n",
        "    X.append(torch.tensor(input_encode(word))) # convert to tensor shape (len(X), len(word))\n",
        "X = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0) # pad all sequences to same length with 0 : <PAD>\n",
        "\n",
        "Y = []\n",
        "for word in name_en:\n",
        "    word = [char for char in word] + ['<EOW>']\n",
        "    Y.append(torch.tensor(output_encode(word))) # convert to tensor shape (len(Y), len(word))\n",
        "Y = nn.utils.rnn.pad_sequence(Y, batch_first=True, padding_value=0).long() # pad all sequences to same length with 0 : <PAD>\n",
        "print(\"\\nCheck X and Y of dataset\")\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(\"\\nExample:\")\n",
        "print(X[0], \"\\n\", input_decode(list(X[0].detach().numpy().astype(int)))) # 0 is the first word in the dataset)\n",
        "print(Y[0], \"\\n\", output_decode(list(Y[0].detach().numpy().astype(int)))) # 0 is the first word in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo381I_t1ZSm",
        "outputId": "4467516a-90c8-477d-bc43-bb071b17a956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max input length: 20\n",
            "Max output length: 20\n"
          ]
        }
      ],
      "source": [
        "print(\"Max input length:\", maxlen)\n",
        "print(\"Max output length:\", maxlen_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W3aXyJBEC-j_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-yirzlseC9NS"
      },
      "outputs": [],
      "source": [
        "class NameDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    #FILL CODE HERE pass\n",
        "    self.encoded_X = X.long()\n",
        "    self.encoded_label = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #FILL CODE HERE pass\n",
        "    return {\"x\" :self.encoded_X[idx], \"y\":self.encoded_label[idx]}\n",
        "\n",
        "  def __len__(self):\n",
        "    #FILL CODE HERE pass\n",
        "    return len(self.encoded_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qUPAB7LTDFOy"
      },
      "outputs": [],
      "source": [
        "class NameDataModule(L.LightningDataModule):\n",
        "\n",
        "  def __init__(self, train_data, y, batch_size, num_workers=0):\n",
        "      super().__init__()\n",
        "      self.train_data = train_data\n",
        "      self.y = y\n",
        "      self.batch_size = batch_size\n",
        "      self.num_workers = num_workers\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "    pass\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    '''turn encoded inputs into one-hot vectors'''\n",
        "    batched_one_hot_x = torch.stack([F.one_hot(dp[\"x\"], num_classes=vocab_size) for dp in batch])\n",
        "    batched_y = torch.stack([dp[\"y\"] for dp in batch])\n",
        "    return {\"x\": batched_one_hot_x.float(), \"y\": batched_y}    \n",
        "\n",
        "  def train_dataloader(self):\n",
        "    train_dataset = NameDataset(self.train_data, self.y)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                                batch_size = self.batch_size,\n",
        "                                shuffle = True,\n",
        "                                collate_fn = self.collate_fn,\n",
        "                                num_workers = self.num_workers)\n",
        "    return train_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSG1FqK1ZSy"
      },
      "source": [
        "# Attention Mechanism\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAlOrhbismQp"
      },
      "source": [
        "## TODO 2: Code your own (key-value) attention mechnism\n",
        "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
        "* fill code for one_step_attention function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "avnlc6p9BZDv"
      },
      "outputs": [],
      "source": [
        "def one_step_attention(h, s_prev, attention_layer, key_ff_layer):\n",
        "    '''\n",
        "    h.shape = (batch_size, seq_len, hidden_size)\n",
        "    '''\n",
        "    #Split into Key-Value\n",
        "    key, value = torch.split(h, h.shape[2]//2, dim=2) # (b, seq_len, hidden_size//2)\n",
        "    #do concat with s_prev.\n",
        "    #hint: you will need to use s_prev.repeat(...) somehow so that it has the same dimension as the key\n",
        "    #hint2: s_prev.unsqueeze() could also be useful\n",
        "    s_prev = s_prev.unsqueeze(1).repeat((1, h.shape[1], 1))\n",
        "    concatenated = torch.cat((key, s_prev), dim=2) # (b, seq_len, hidden_size)\n",
        "\n",
        "    #Attention function###\n",
        "    # use layer(s) from your model to calculate attention_scores and then softmax\n",
        "    # calculate a context vector\n",
        "    f_attn = F.tanh(attention_layer(concatenated)) # (b, seq_len, 1)\n",
        "    f_attn = F.relu(key_ff_layer(f_attn))\n",
        "    \n",
        "    attention_scores = F.softmax(f_attn, dim=1) # (b, seq_len, 1)\n",
        "    \n",
        "    tmp = torch.mul(attention_scores, value) # (b, seq_len, hidden_size//2)\n",
        "    context = torch.sum(tmp,dim=1)\n",
        "    \n",
        "    return context, attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zWN02ZtuOIU"
      },
      "source": [
        "# Translation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0phyUQYg1ZS8"
      },
      "source": [
        "## TODO3: Create and train your encoder/decoder model here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ji_rUPhK1ZS9"
      },
      "outputs": [],
      "source": [
        "class AttentionModel(L.LightningModule):\n",
        "    def __init__(self, learning_rate, criterion):\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_h = 64 #hidden dimensions for encoder\n",
        "        self.n_s = 64 #hidden dimensions for decoder\n",
        "\n",
        "        #encoder can be any RNN of your choice\n",
        "        bi_direction = True\n",
        "        self.num_directions = 2 if bi_direction else 1\n",
        "        self.lstm = nn.LSTM(len(input_char_to_idx), self.n_h, bidirectional=bi_direction, batch_first=True)\n",
        "        #decoder has to be (any) RNNCell since we will need to calculate attention for each timestep manually\n",
        "        self.decoder_lstm_cell = nn.LSTMCell(self.n_s, self.n_s)\n",
        "        self.output_layer = nn.Linear(self.n_s, len(output_char_to_idx))\n",
        "        #attention\n",
        "        self.attention_layer = nn.Linear((self.n_h//2)*2*self.num_directions, (self.n_h//2))\n",
        "        self.key_ff_layer = nn.Linear((self.n_h//2), 1)\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def forward(self, src, return_attention=False): #use return_attention only when you want to get the attention scores for visualizing\n",
        "        #pass the input to the encoder\n",
        "        lstm_out, _ = self.lstm(src)\n",
        "        #Initialize the LSTM states. We have to do this since we are using LSTMCell (https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
        "        #These states will get updated while we are decoding\n",
        "        decoder_s = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "        decoder_c = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "\n",
        "        #Iterate until max_output_length (Decoding)\n",
        "        prediction = torch.zeros((src.shape[0], maxlen_out, len(output_char_to_idx))).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "        attention_scores = [] #to store the score for each step\n",
        "        for t in range(maxlen_out):\n",
        "            \n",
        "            #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
        "            context, attention_score = one_step_attention(lstm_out, decoder_s, self.attention_layer, self.key_ff_layer)\n",
        "            attention_scores.append(attention_score)\n",
        "\n",
        "            # Feed the context vector to the decoder.\n",
        "            decoder_s, decoder_c = self.decoder_lstm_cell(context, (decoder_s, decoder_c))\n",
        "            # Pass the decoder hidden output to the output layer (softmax)\n",
        "            out = self.output_layer(decoder_s)\n",
        "            # Put the predicted output into the list for this timestep\n",
        "            prediction[:, t] = out\n",
        "\n",
        "        return (prediction, attention_scores if return_attention else None)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src = batch['x']\n",
        "        target = batch['y']\n",
        "        prediction,_ = self(src)\n",
        "        prediction = prediction.reshape(-1, len(output_char_to_idx))\n",
        "        target = target.reshape(-1)\n",
        "        loss = self.criterion(prediction, target)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        src = batch['x']\n",
        "        with torch.no_grad():\n",
        "          prediction, attention_scores = self(src, return_attention=True)\n",
        "          prediction = F.softmax(prediction, dim=-1)\n",
        "          prediction = torch.argmax(prediction, dim=-1)\n",
        "          for pred in prediction:\n",
        "            # print(\"\".join(output_char_to_idx.lookup_tokens(pred.cpu().numpy())))\n",
        "            print(\"\".join(output_decode(pred.cpu().numpy())))\n",
        "            \n",
        "        return prediction, attention_scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pSM9dgDcCz1E"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.01\n",
        "model = AttentionModel(lr, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RqrvmJalDLzF"
      },
      "outputs": [],
      "source": [
        "data_module = NameDataModule(X, Y, batch_size=4096)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_sFjzKX8SECo"
      },
      "outputs": [],
      "source": [
        "from lightning import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "wandb_logger = WandbLogger(project=\"hw3.1_attention\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OGWSzS-X1ZTO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(\n",
        "    max_epochs=1000,\n",
        "    # logger=wandb_logger\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7ZMi782c1ZTQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type             | Params | Mode \n",
            "---------------------------------------------------------------\n",
            "0 | lstm              | LSTM             | 67.1 K | train\n",
            "1 | decoder_lstm_cell | LSTMCell         | 33.3 K | train\n",
            "2 | output_layer      | Linear           | 1.6 K  | train\n",
            "3 | attention_layer   | Linear           | 4.1 K  | train\n",
            "4 | key_ff_layer      | Linear           | 33     | train\n",
            "5 | criterion         | CrossEntropyLoss | 0      | train\n",
            "---------------------------------------------------------------\n",
            "106 K     Trainable params\n",
            "0         Non-trainable params\n",
            "106 K     Total params\n",
            "0.424     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 999: 100%|██████████| 6/6 [00:00<00:00, -11.50it/s, v_num=5]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 999: 100%|██████████| 6/6 [00:00<00:00, -11.67it/s, v_num=5]\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5BLw1Ir1ZTT"
      },
      "source": [
        "# Test Your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRLjZzBMtCdA"
      },
      "source": [
        "## TODO4: Test your model on 5 examples of your choice including your name!\n",
        "\n",
        "Example Output:\n",
        "```\n",
        "prayutthatha</s></s>aa</s></s>a</s>\n",
        "somchai</s></s></s></s>a</s></s>a</s></s></s></s></s>\n",
        "thanathon</s></s></s></s></s></s></s></s></s></s></s>\n",
        "newin</s>i</s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "suthep</s>he</s></s></s></s></s></s></s></s></s></s></s>\n",
        "prawit</s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "chatchachatti</s></s>i</s></s></s></s>\n",
        "```\n",
        "\n",
        "<font color='blue'>Paste your model predictions in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "6stNACsUP9h-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "EXAMPLES = ['ประยุทธ','สมชาย','ธนาธร','เนวิน','สุเทพ','ประวิตร์','ชัชชาติ']\n",
        "\n",
        "#Writenn by Jeans\n",
        "\n",
        "predict_data = []\n",
        "for line in EXAMPLES:\n",
        "    line = [l for l in line] #change from string to list\n",
        "    predict_data.append(torch.tensor(input_encode(line)))\n",
        "\n",
        "print(len(predict_data))\n",
        "\n",
        "def collate_fn_(batch):\n",
        "    batched_one_hot_x = torch.stack([F.one_hot(dp[\"x\"], num_classes=vocab_size) for dp in batch])\n",
        "    return {\"x\": batched_one_hot_x.float()}    \n",
        "\n",
        "predict_data = nn.utils.rnn.pad_sequence(predict_data, batch_first = True)\n",
        "predict_dataset = NameDataset(predict_data, [torch.tensor(0)]*len(predict_data))\n",
        "predict_loader = DataLoader(predict_dataset,\n",
        "                          batch_size = 1,\n",
        "                          shuffle = False,\n",
        "                          collate_fn = collate_fn_,\n",
        "                          num_workers = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kbolC8XIhR3t"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AttentionModel(\n",
              "  (lstm): LSTM(65, 64, batch_first=True, bidirectional=True)\n",
              "  (decoder_lstm_cell): LSTMCell(64, 64)\n",
              "  (output_layer): Linear(in_features=64, out_features=24, bias=True)\n",
              "  (attention_layer): Linear(in_features=128, out_features=32, bias=True)\n",
              "  (key_ff_layer): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "LsN71S9uQ9wo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]prayuttha<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  14%|█▍        | 1/7 [00:00<00:00, 62.71it/s]somchai<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  29%|██▊       | 2/7 [00:00<00:00, 59.25it/s]thanathon<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  43%|████▎     | 3/7 [00:00<00:00, 60.95it/s]newin<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  57%|█████▋    | 4/7 [00:00<00:00, 64.27it/s]suthep<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  71%|███████▏  | 5/7 [00:00<00:00, 64.53it/s]prawi<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  86%|████████▌ | 6/7 [00:00<00:00, 61.69it/s]chatchachatti<EOW><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 64.06it/s]\n"
          ]
        }
      ],
      "source": [
        "output = trainer.predict(model, predict_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o3893RL1ZT8"
      },
      "source": [
        "## TODO 5: Show your visualization of attention scores on one of your example\n",
        "\n",
        "<font color='blue'>Paste your visualization image in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ประยุทธ prayuttha~__________\n",
            "Attention (1, 8, 1)\n",
            "สมชาย somchai~____________\n",
            "Attention (1, 8, 1)\n",
            "ธนาธร thanathon~__________\n",
            "Attention (1, 8, 1)\n",
            "เนวิน newin~______________\n",
            "Attention (1, 8, 1)\n",
            "สุเทพ suthep~_____________\n",
            "Attention (1, 8, 1)\n",
            "ประวิตร์ prawi~______________\n",
            "Attention (1, 8, 1)\n",
            "ชัชชาติ chatchachatti~______\n",
            "Attention (1, 8, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(tensor([[17, 18,  3, 23, 21, 20, 20, 10,  3,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[9.9999e-01],\n",
              "            [1.8731e-06],\n",
              "            [3.6887e-07],\n",
              "            [4.2577e-06],\n",
              "            [5.6997e-07],\n",
              "            [3.6887e-07],\n",
              "            [6.9808e-07],\n",
              "            [7.5843e-07]]]),\n",
              "   tensor([[[0.0435],\n",
              "            [0.8825],\n",
              "            [0.0345],\n",
              "            [0.0090],\n",
              "            [0.0074],\n",
              "            [0.0083],\n",
              "            [0.0083],\n",
              "            [0.0066]]]),\n",
              "   tensor([[[0.0317],\n",
              "            [0.1582],\n",
              "            [0.6249],\n",
              "            [0.1138],\n",
              "            [0.0213],\n",
              "            [0.0180],\n",
              "            [0.0171],\n",
              "            [0.0150]]]),\n",
              "   tensor([[[0.0054],\n",
              "            [0.0032],\n",
              "            [0.0040],\n",
              "            [0.8128],\n",
              "            [0.1364],\n",
              "            [0.0258],\n",
              "            [0.0093],\n",
              "            [0.0032]]]),\n",
              "   tensor([[[0.0595],\n",
              "            [0.0017],\n",
              "            [0.0040],\n",
              "            [0.2297],\n",
              "            [0.5987],\n",
              "            [0.0635],\n",
              "            [0.0235],\n",
              "            [0.0194]]]),\n",
              "   tensor([[[3.7976e-03],\n",
              "            [2.1603e-04],\n",
              "            [3.6672e-04],\n",
              "            [2.6233e-02],\n",
              "            [4.2359e-01],\n",
              "            [5.0448e-01],\n",
              "            [3.1187e-02],\n",
              "            [1.0122e-02]]]),\n",
              "   tensor([[[4.4587e-03],\n",
              "            [7.1906e-04],\n",
              "            [1.7901e-04],\n",
              "            [6.3875e-03],\n",
              "            [1.3369e-02],\n",
              "            [6.8911e-01],\n",
              "            [2.6773e-01],\n",
              "            [1.8051e-02]]]),\n",
              "   tensor([[[5.9091e-03],\n",
              "            [9.7007e-04],\n",
              "            [2.6241e-04],\n",
              "            [1.0314e-03],\n",
              "            [4.0429e-03],\n",
              "            [5.0329e-01],\n",
              "            [4.5398e-01],\n",
              "            [3.0513e-02]]]),\n",
              "   tensor([[[6.3314e-03],\n",
              "            [2.0668e-03],\n",
              "            [7.3346e-04],\n",
              "            [1.7020e-03],\n",
              "            [4.6245e-03],\n",
              "            [9.8511e-02],\n",
              "            [7.6588e-01],\n",
              "            [1.2015e-01]]]),\n",
              "   tensor([[[0.0053],\n",
              "            [0.0070],\n",
              "            [0.0060],\n",
              "            [0.0160],\n",
              "            [0.0114],\n",
              "            [0.0747],\n",
              "            [0.3809],\n",
              "            [0.4987]]]),\n",
              "   tensor([[[0.0114],\n",
              "            [0.0135],\n",
              "            [0.0138],\n",
              "            [0.0114],\n",
              "            [0.0114],\n",
              "            [0.0856],\n",
              "            [0.3699],\n",
              "            [0.4830]]]),\n",
              "   tensor([[[0.0254],\n",
              "            [0.0279],\n",
              "            [0.0254],\n",
              "            [0.0254],\n",
              "            [0.0254],\n",
              "            [0.0587],\n",
              "            [0.2327],\n",
              "            [0.5791]]]),\n",
              "   tensor([[[0.0550],\n",
              "            [0.0550],\n",
              "            [0.0578],\n",
              "            [0.0550],\n",
              "            [0.0550],\n",
              "            [0.1073],\n",
              "            [0.2765],\n",
              "            [0.3384]]]),\n",
              "   tensor([[[0.0500],\n",
              "            [0.0500],\n",
              "            [0.0798],\n",
              "            [0.0500],\n",
              "            [0.0500],\n",
              "            [0.0969],\n",
              "            [0.2722],\n",
              "            [0.3511]]]),\n",
              "   tensor([[[0.0508],\n",
              "            [0.0508],\n",
              "            [0.0877],\n",
              "            [0.0508],\n",
              "            [0.0508],\n",
              "            [0.0955],\n",
              "            [0.2741],\n",
              "            [0.3395]]]),\n",
              "   tensor([[[0.0515],\n",
              "            [0.0515],\n",
              "            [0.0848],\n",
              "            [0.0515],\n",
              "            [0.0515],\n",
              "            [0.0988],\n",
              "            [0.2911],\n",
              "            [0.3192]]]),\n",
              "   tensor([[[0.0450],\n",
              "            [0.0450],\n",
              "            [0.0843],\n",
              "            [0.0450],\n",
              "            [0.0450],\n",
              "            [0.1004],\n",
              "            [0.3479],\n",
              "            [0.2873]]]),\n",
              "   tensor([[[0.0374],\n",
              "            [0.0374],\n",
              "            [0.0898],\n",
              "            [0.0374],\n",
              "            [0.0374],\n",
              "            [0.1035],\n",
              "            [0.4096],\n",
              "            [0.2475]]]),\n",
              "   tensor([[[0.0366],\n",
              "            [0.0366],\n",
              "            [0.0924],\n",
              "            [0.0366],\n",
              "            [0.0366],\n",
              "            [0.1001],\n",
              "            [0.3933],\n",
              "            [0.2678]]]),\n",
              "   tensor([[[0.0376],\n",
              "            [0.0376],\n",
              "            [0.0914],\n",
              "            [0.0376],\n",
              "            [0.0376],\n",
              "            [0.0968],\n",
              "            [0.3826],\n",
              "            [0.2786]]])]),\n",
              " (tensor([[19, 16, 14,  5, 10,  3, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[1.0000e+00],\n",
              "            [1.0134e-08],\n",
              "            [2.2978e-08],\n",
              "            [7.9007e-09],\n",
              "            [1.1162e-08],\n",
              "            [7.9007e-09],\n",
              "            [7.9007e-09],\n",
              "            [7.9007e-09]]]),\n",
              "   tensor([[[0.5179],\n",
              "            [0.2148],\n",
              "            [0.0479],\n",
              "            [0.0438],\n",
              "            [0.0450],\n",
              "            [0.0439],\n",
              "            [0.0435],\n",
              "            [0.0432]]]),\n",
              "   tensor([[[0.0102],\n",
              "            [0.9515],\n",
              "            [0.0242],\n",
              "            [0.0029],\n",
              "            [0.0049],\n",
              "            [0.0025],\n",
              "            [0.0021],\n",
              "            [0.0018]]]),\n",
              "   tensor([[[0.0164],\n",
              "            [0.0144],\n",
              "            [0.8887],\n",
              "            [0.0252],\n",
              "            [0.0382],\n",
              "            [0.0066],\n",
              "            [0.0052],\n",
              "            [0.0053]]]),\n",
              "   tensor([[[0.0325],\n",
              "            [0.0011],\n",
              "            [0.8158],\n",
              "            [0.0674],\n",
              "            [0.0458],\n",
              "            [0.0145],\n",
              "            [0.0115],\n",
              "            [0.0115]]]),\n",
              "   tensor([[[0.0517],\n",
              "            [0.0006],\n",
              "            [0.1478],\n",
              "            [0.4153],\n",
              "            [0.2587],\n",
              "            [0.0427],\n",
              "            [0.0406],\n",
              "            [0.0426]]]),\n",
              "   tensor([[[0.0066],\n",
              "            [0.0010],\n",
              "            [0.0077],\n",
              "            [0.0346],\n",
              "            [0.8892],\n",
              "            [0.0337],\n",
              "            [0.0179],\n",
              "            [0.0094]]]),\n",
              "   tensor([[[0.0047],\n",
              "            [0.0043],\n",
              "            [0.0125],\n",
              "            [0.0273],\n",
              "            [0.4588],\n",
              "            [0.2767],\n",
              "            [0.1462],\n",
              "            [0.0696]]]),\n",
              "   tensor([[[0.0116],\n",
              "            [0.0163],\n",
              "            [0.0191],\n",
              "            [0.0125],\n",
              "            [0.2791],\n",
              "            [0.1634],\n",
              "            [0.3066],\n",
              "            [0.1914]]]),\n",
              "   tensor([[[0.0969],\n",
              "            [0.0783],\n",
              "            [0.0783],\n",
              "            [0.0783],\n",
              "            [0.1890],\n",
              "            [0.1840],\n",
              "            [0.2026],\n",
              "            [0.0926]]]),\n",
              "   tensor([[[0.1179],\n",
              "            [0.0875],\n",
              "            [0.0875],\n",
              "            [0.0875],\n",
              "            [0.2018],\n",
              "            [0.1961],\n",
              "            [0.1342],\n",
              "            [0.0875]]]),\n",
              "   tensor([[[0.1105],\n",
              "            [0.0961],\n",
              "            [0.0961],\n",
              "            [0.0961],\n",
              "            [0.1839],\n",
              "            [0.1856],\n",
              "            [0.1354],\n",
              "            [0.0961]]]),\n",
              "   tensor([[[0.1071],\n",
              "            [0.0940],\n",
              "            [0.0940],\n",
              "            [0.0940],\n",
              "            [0.1906],\n",
              "            [0.1925],\n",
              "            [0.1339],\n",
              "            [0.0940]]]),\n",
              "   tensor([[[0.1159],\n",
              "            [0.0846],\n",
              "            [0.0846],\n",
              "            [0.0846],\n",
              "            [0.2201],\n",
              "            [0.1944],\n",
              "            [0.1312],\n",
              "            [0.0846]]]),\n",
              "   tensor([[[0.1201],\n",
              "            [0.0631],\n",
              "            [0.0631],\n",
              "            [0.0631],\n",
              "            [0.3004],\n",
              "            [0.2049],\n",
              "            [0.1198],\n",
              "            [0.0654]]]),\n",
              "   tensor([[[0.1208],\n",
              "            [0.0635],\n",
              "            [0.0635],\n",
              "            [0.0635],\n",
              "            [0.3020],\n",
              "            [0.2031],\n",
              "            [0.1183],\n",
              "            [0.0652]]]),\n",
              "   tensor([[[0.1198],\n",
              "            [0.0608],\n",
              "            [0.0608],\n",
              "            [0.0608],\n",
              "            [0.3139],\n",
              "            [0.2035],\n",
              "            [0.1155],\n",
              "            [0.0648]]]),\n",
              "   tensor([[[0.1209],\n",
              "            [0.0618],\n",
              "            [0.0618],\n",
              "            [0.0618],\n",
              "            [0.3117],\n",
              "            [0.2024],\n",
              "            [0.1150],\n",
              "            [0.0646]]]),\n",
              "   tensor([[[0.1226],\n",
              "            [0.0614],\n",
              "            [0.0614],\n",
              "            [0.0614],\n",
              "            [0.3148],\n",
              "            [0.2008],\n",
              "            [0.1134],\n",
              "            [0.0643]]]),\n",
              "   tensor([[[0.1240],\n",
              "            [0.0616],\n",
              "            [0.0616],\n",
              "            [0.0616],\n",
              "            [0.3143],\n",
              "            [0.1998],\n",
              "            [0.1129],\n",
              "            [0.0641]]])]),\n",
              " (tensor([[20, 10,  3, 15,  3, 20, 10, 16, 15,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[9.9997e-01],\n",
              "            [1.2411e-05],\n",
              "            [3.4205e-06],\n",
              "            [3.8283e-06],\n",
              "            [3.1087e-06],\n",
              "            [1.0677e-06],\n",
              "            [9.3898e-07],\n",
              "            [9.0807e-07]]]),\n",
              "   tensor([[[0.9646],\n",
              "            [0.0059],\n",
              "            [0.0053],\n",
              "            [0.0052],\n",
              "            [0.0051],\n",
              "            [0.0047],\n",
              "            [0.0047],\n",
              "            [0.0046]]]),\n",
              "   tensor([[[0.3369],\n",
              "            [0.2424],\n",
              "            [0.0807],\n",
              "            [0.0742],\n",
              "            [0.0720],\n",
              "            [0.0679],\n",
              "            [0.0653],\n",
              "            [0.0607]]]),\n",
              "   tensor([[[0.0221],\n",
              "            [0.8340],\n",
              "            [0.0693],\n",
              "            [0.0251],\n",
              "            [0.0141],\n",
              "            [0.0122],\n",
              "            [0.0119],\n",
              "            [0.0113]]]),\n",
              "   tensor([[[0.0303],\n",
              "            [0.0797],\n",
              "            [0.5689],\n",
              "            [0.2232],\n",
              "            [0.0344],\n",
              "            [0.0219],\n",
              "            [0.0210],\n",
              "            [0.0206]]]),\n",
              "   tensor([[[0.0043],\n",
              "            [0.0012],\n",
              "            [0.0309],\n",
              "            [0.9218],\n",
              "            [0.0222],\n",
              "            [0.0067],\n",
              "            [0.0063],\n",
              "            [0.0067]]]),\n",
              "   tensor([[[0.0164],\n",
              "            [0.0029],\n",
              "            [0.0317],\n",
              "            [0.8322],\n",
              "            [0.0649],\n",
              "            [0.0163],\n",
              "            [0.0170],\n",
              "            [0.0185]]]),\n",
              "   tensor([[[0.0468],\n",
              "            [0.0013],\n",
              "            [0.0105],\n",
              "            [0.4662],\n",
              "            [0.2615],\n",
              "            [0.0736],\n",
              "            [0.0715],\n",
              "            [0.0686]]]),\n",
              "   tensor([[[0.0047],\n",
              "            [0.0035],\n",
              "            [0.0025],\n",
              "            [0.0647],\n",
              "            [0.8142],\n",
              "            [0.0500],\n",
              "            [0.0406],\n",
              "            [0.0198]]]),\n",
              "   tensor([[[0.0064],\n",
              "            [0.0105],\n",
              "            [0.0084],\n",
              "            [0.0423],\n",
              "            [0.6663],\n",
              "            [0.2005],\n",
              "            [0.0435],\n",
              "            [0.0222]]]),\n",
              "   tensor([[[0.0296],\n",
              "            [0.0088],\n",
              "            [0.0094],\n",
              "            [0.0336],\n",
              "            [0.2691],\n",
              "            [0.3229],\n",
              "            [0.2062],\n",
              "            [0.1204]]]),\n",
              "   tensor([[[0.0460],\n",
              "            [0.0096],\n",
              "            [0.0096],\n",
              "            [0.0102],\n",
              "            [0.2220],\n",
              "            [0.3062],\n",
              "            [0.2720],\n",
              "            [0.1243]]]),\n",
              "   tensor([[[0.1517],\n",
              "            [0.0463],\n",
              "            [0.0463],\n",
              "            [0.0463],\n",
              "            [0.2051],\n",
              "            [0.2472],\n",
              "            [0.1660],\n",
              "            [0.0910]]]),\n",
              "   tensor([[[0.1291],\n",
              "            [0.0604],\n",
              "            [0.0604],\n",
              "            [0.0604],\n",
              "            [0.2493],\n",
              "            [0.2121],\n",
              "            [0.1474],\n",
              "            [0.0810]]]),\n",
              "   tensor([[[0.1514],\n",
              "            [0.0669],\n",
              "            [0.0669],\n",
              "            [0.0735],\n",
              "            [0.2378],\n",
              "            [0.1895],\n",
              "            [0.1386],\n",
              "            [0.0752]]]),\n",
              "   tensor([[[0.1449],\n",
              "            [0.0568],\n",
              "            [0.0568],\n",
              "            [0.0742],\n",
              "            [0.2575],\n",
              "            [0.1900],\n",
              "            [0.1411],\n",
              "            [0.0786]]]),\n",
              "   tensor([[[0.2052],\n",
              "            [0.0435],\n",
              "            [0.0435],\n",
              "            [0.0769],\n",
              "            [0.2721],\n",
              "            [0.1601],\n",
              "            [0.1198],\n",
              "            [0.0789]]]),\n",
              "   tensor([[[0.1715],\n",
              "            [0.0350],\n",
              "            [0.0350],\n",
              "            [0.0717],\n",
              "            [0.3060],\n",
              "            [0.1667],\n",
              "            [0.1256],\n",
              "            [0.0885]]]),\n",
              "   tensor([[[0.1639],\n",
              "            [0.0408],\n",
              "            [0.0408],\n",
              "            [0.0770],\n",
              "            [0.2966],\n",
              "            [0.1706],\n",
              "            [0.1265],\n",
              "            [0.0839]]]),\n",
              "   tensor([[[0.1975],\n",
              "            [0.0401],\n",
              "            [0.0401],\n",
              "            [0.0803],\n",
              "            [0.2856],\n",
              "            [0.1577],\n",
              "            [0.1178],\n",
              "            [0.0811]]])]),\n",
              " (tensor([[15,  7, 22, 11, 15,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[3.7732e-04],\n",
              "            [9.9948e-01],\n",
              "            [1.1168e-04],\n",
              "            [1.3203e-05],\n",
              "            [5.7201e-06],\n",
              "            [3.1417e-06],\n",
              "            [2.2403e-06],\n",
              "            [2.0572e-06]]]),\n",
              "   tensor([[[0.0051],\n",
              "            [0.7649],\n",
              "            [0.1520],\n",
              "            [0.0161],\n",
              "            [0.0159],\n",
              "            [0.0157],\n",
              "            [0.0152],\n",
              "            [0.0151]]]),\n",
              "   tensor([[[8.5204e-04],\n",
              "            [3.8509e-02],\n",
              "            [8.6916e-01],\n",
              "            [3.2473e-02],\n",
              "            [1.7263e-02],\n",
              "            [1.5248e-02],\n",
              "            [1.3796e-02],\n",
              "            [1.2697e-02]]]),\n",
              "   tensor([[[1.8029e-04],\n",
              "            [2.6425e-02],\n",
              "            [3.8497e-01],\n",
              "            [4.4258e-01],\n",
              "            [9.8336e-02],\n",
              "            [2.0609e-02],\n",
              "            [1.3891e-02],\n",
              "            [1.3002e-02]]]),\n",
              "   tensor([[[7.8625e-05],\n",
              "            [1.3544e-02],\n",
              "            [3.7842e-03],\n",
              "            [7.9230e-02],\n",
              "            [7.7039e-01],\n",
              "            [8.4664e-02],\n",
              "            [2.6529e-02],\n",
              "            [2.1778e-02]]]),\n",
              "   tensor([[[0.0020],\n",
              "            [0.0209],\n",
              "            [0.0135],\n",
              "            [0.0064],\n",
              "            [0.1253],\n",
              "            [0.5787],\n",
              "            [0.2087],\n",
              "            [0.0446]]]),\n",
              "   tensor([[[0.0193],\n",
              "            [0.0281],\n",
              "            [0.0585],\n",
              "            [0.0193],\n",
              "            [0.0507],\n",
              "            [0.1610],\n",
              "            [0.3545],\n",
              "            [0.3086]]]),\n",
              "   tensor([[[0.0839],\n",
              "            [0.2923],\n",
              "            [0.0839],\n",
              "            [0.0839],\n",
              "            [0.0839],\n",
              "            [0.0839],\n",
              "            [0.1583],\n",
              "            [0.1298]]]),\n",
              "   tensor([[[0.0865],\n",
              "            [0.2695],\n",
              "            [0.0877],\n",
              "            [0.0865],\n",
              "            [0.0865],\n",
              "            [0.1007],\n",
              "            [0.1536],\n",
              "            [0.1289]]]),\n",
              "   tensor([[[0.1028],\n",
              "            [0.2132],\n",
              "            [0.1028],\n",
              "            [0.1028],\n",
              "            [0.1028],\n",
              "            [0.1041],\n",
              "            [0.1430],\n",
              "            [0.1284]]]),\n",
              "   tensor([[[0.1045],\n",
              "            [0.2247],\n",
              "            [0.1045],\n",
              "            [0.1045],\n",
              "            [0.1045],\n",
              "            [0.1055],\n",
              "            [0.1340],\n",
              "            [0.1177]]]),\n",
              "   tensor([[[0.1033],\n",
              "            [0.2286],\n",
              "            [0.1033],\n",
              "            [0.1033],\n",
              "            [0.1033],\n",
              "            [0.1091],\n",
              "            [0.1356],\n",
              "            [0.1135]]]),\n",
              "   tensor([[[0.0987],\n",
              "            [0.2394],\n",
              "            [0.0987],\n",
              "            [0.0987],\n",
              "            [0.0987],\n",
              "            [0.1139],\n",
              "            [0.1407],\n",
              "            [0.1112]]]),\n",
              "   tensor([[[0.0915],\n",
              "            [0.2591],\n",
              "            [0.0915],\n",
              "            [0.0915],\n",
              "            [0.0915],\n",
              "            [0.1212],\n",
              "            [0.1467],\n",
              "            [0.1067]]]),\n",
              "   tensor([[[0.0874],\n",
              "            [0.2678],\n",
              "            [0.0874],\n",
              "            [0.0874],\n",
              "            [0.0874],\n",
              "            [0.1269],\n",
              "            [0.1510],\n",
              "            [0.1046]]]),\n",
              "   tensor([[[0.0872],\n",
              "            [0.2674],\n",
              "            [0.0872],\n",
              "            [0.0872],\n",
              "            [0.0872],\n",
              "            [0.1295],\n",
              "            [0.1512],\n",
              "            [0.1032]]]),\n",
              "   tensor([[[0.0873],\n",
              "            [0.2674],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.1314],\n",
              "            [0.1502],\n",
              "            [0.1017]]]),\n",
              "   tensor([[[0.0873],\n",
              "            [0.2676],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.1329],\n",
              "            [0.1497],\n",
              "            [0.1007]]]),\n",
              "   tensor([[[0.0873],\n",
              "            [0.2675],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.1338],\n",
              "            [0.1493],\n",
              "            [0.1002]]]),\n",
              "   tensor([[[0.0874],\n",
              "            [0.2673],\n",
              "            [0.0874],\n",
              "            [0.0874],\n",
              "            [0.0874],\n",
              "            [0.1344],\n",
              "            [0.1490],\n",
              "            [0.0998]]])]),\n",
              " (tensor([[19, 21, 20, 10,  7, 17,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[1.0000e+00],\n",
              "            [2.6023e-08],\n",
              "            [1.1888e-08],\n",
              "            [1.1888e-08],\n",
              "            [4.3942e-08],\n",
              "            [1.1888e-08],\n",
              "            [1.1888e-08],\n",
              "            [1.1888e-08]]]),\n",
              "   tensor([[[0.1035],\n",
              "            [0.8376],\n",
              "            [0.0157],\n",
              "            [0.0122],\n",
              "            [0.0080],\n",
              "            [0.0077],\n",
              "            [0.0077],\n",
              "            [0.0076]]]),\n",
              "   tensor([[[0.0073],\n",
              "            [0.0133],\n",
              "            [0.0079],\n",
              "            [0.9530],\n",
              "            [0.0107],\n",
              "            [0.0031],\n",
              "            [0.0024],\n",
              "            [0.0022]]]),\n",
              "   tensor([[[2.6895e-02],\n",
              "            [3.9522e-04],\n",
              "            [1.6566e-04],\n",
              "            [9.2340e-01],\n",
              "            [3.0137e-02],\n",
              "            [7.0877e-03],\n",
              "            [6.0905e-03],\n",
              "            [5.8246e-03]]]),\n",
              "   tensor([[[5.0437e-02],\n",
              "            [9.4317e-05],\n",
              "            [6.1431e-05],\n",
              "            [6.4313e-01],\n",
              "            [2.3859e-01],\n",
              "            [2.5856e-02],\n",
              "            [2.1884e-02],\n",
              "            [1.9945e-02]]]),\n",
              "   tensor([[[3.6619e-02],\n",
              "            [2.1917e-04],\n",
              "            [2.1917e-04],\n",
              "            [3.9086e-02],\n",
              "            [7.6520e-01],\n",
              "            [7.0641e-02],\n",
              "            [4.4897e-02],\n",
              "            [4.3116e-02]]]),\n",
              "   tensor([[[0.0262],\n",
              "            [0.0013],\n",
              "            [0.0015],\n",
              "            [0.0137],\n",
              "            [0.3225],\n",
              "            [0.4923],\n",
              "            [0.1152],\n",
              "            [0.0274]]]),\n",
              "   tensor([[[0.0111],\n",
              "            [0.0111],\n",
              "            [0.0111],\n",
              "            [0.0763],\n",
              "            [0.1224],\n",
              "            [0.2215],\n",
              "            [0.3032],\n",
              "            [0.2431]]]),\n",
              "   tensor([[[0.1286],\n",
              "            [0.0808],\n",
              "            [0.0808],\n",
              "            [0.0808],\n",
              "            [0.0808],\n",
              "            [0.1266],\n",
              "            [0.2348],\n",
              "            [0.1869]]]),\n",
              "   tensor([[[0.1471],\n",
              "            [0.0944],\n",
              "            [0.0944],\n",
              "            [0.0944],\n",
              "            [0.0944],\n",
              "            [0.1247],\n",
              "            [0.2056],\n",
              "            [0.1452]]]),\n",
              "   tensor([[[0.1180],\n",
              "            [0.1154],\n",
              "            [0.1154],\n",
              "            [0.1180],\n",
              "            [0.1154],\n",
              "            [0.1154],\n",
              "            [0.1639],\n",
              "            [0.1386]]]),\n",
              "   tensor([[[0.1169],\n",
              "            [0.1169],\n",
              "            [0.1169],\n",
              "            [0.1269],\n",
              "            [0.1169],\n",
              "            [0.1169],\n",
              "            [0.1569],\n",
              "            [0.1319]]]),\n",
              "   tensor([[[0.1181],\n",
              "            [0.1181],\n",
              "            [0.1181],\n",
              "            [0.1247],\n",
              "            [0.1181],\n",
              "            [0.1181],\n",
              "            [0.1581],\n",
              "            [0.1266]]]),\n",
              "   tensor([[[0.1207],\n",
              "            [0.1150],\n",
              "            [0.1025],\n",
              "            [0.1267],\n",
              "            [0.1025],\n",
              "            [0.1175],\n",
              "            [0.1833],\n",
              "            [0.1318]]]),\n",
              "   tensor([[[0.1325],\n",
              "            [0.1213],\n",
              "            [0.0838],\n",
              "            [0.1105],\n",
              "            [0.0838],\n",
              "            [0.1383],\n",
              "            [0.2011],\n",
              "            [0.1286]]]),\n",
              "   tensor([[[0.1348],\n",
              "            [0.1207],\n",
              "            [0.0813],\n",
              "            [0.1164],\n",
              "            [0.0813],\n",
              "            [0.1393],\n",
              "            [0.2002],\n",
              "            [0.1261]]]),\n",
              "   tensor([[[0.1408],\n",
              "            [0.1178],\n",
              "            [0.0809],\n",
              "            [0.1163],\n",
              "            [0.0809],\n",
              "            [0.1411],\n",
              "            [0.1987],\n",
              "            [0.1235]]]),\n",
              "   tensor([[[0.1406],\n",
              "            [0.1161],\n",
              "            [0.0783],\n",
              "            [0.1195],\n",
              "            [0.0802],\n",
              "            [0.1424],\n",
              "            [0.1988],\n",
              "            [0.1240]]]),\n",
              "   tensor([[[0.1400],\n",
              "            [0.1136],\n",
              "            [0.0789],\n",
              "            [0.1252],\n",
              "            [0.0805],\n",
              "            [0.1410],\n",
              "            [0.1978],\n",
              "            [0.1231]]]),\n",
              "   tensor([[[0.1407],\n",
              "            [0.1122],\n",
              "            [0.0785],\n",
              "            [0.1262],\n",
              "            [0.0808],\n",
              "            [0.1415],\n",
              "            [0.1974],\n",
              "            [0.1227]]])]),\n",
              " (tensor([[17, 18,  3, 22, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[9.9999e-01],\n",
              "            [1.9320e-06],\n",
              "            [1.8238e-06],\n",
              "            [3.7360e-06],\n",
              "            [1.3358e-06],\n",
              "            [1.0820e-06],\n",
              "            [1.0710e-06],\n",
              "            [1.0608e-06]]]),\n",
              "   tensor([[[0.0493],\n",
              "            [0.8786],\n",
              "            [0.0321],\n",
              "            [0.0106],\n",
              "            [0.0073],\n",
              "            [0.0073],\n",
              "            [0.0073],\n",
              "            [0.0076]]]),\n",
              "   tensor([[[0.0346],\n",
              "            [0.1867],\n",
              "            [0.5440],\n",
              "            [0.1727],\n",
              "            [0.0185],\n",
              "            [0.0160],\n",
              "            [0.0154],\n",
              "            [0.0121]]]),\n",
              "   tensor([[[0.0083],\n",
              "            [0.0043],\n",
              "            [0.0163],\n",
              "            [0.8753],\n",
              "            [0.0823],\n",
              "            [0.0059],\n",
              "            [0.0045],\n",
              "            [0.0031]]]),\n",
              "   tensor([[[0.0305],\n",
              "            [0.0009],\n",
              "            [0.0008],\n",
              "            [0.1065],\n",
              "            [0.6897],\n",
              "            [0.1113],\n",
              "            [0.0333],\n",
              "            [0.0270]]]),\n",
              "   tensor([[[8.2366e-03],\n",
              "            [2.2331e-03],\n",
              "            [6.2540e-04],\n",
              "            [3.7059e-03],\n",
              "            [5.5677e-03],\n",
              "            [2.3830e-01],\n",
              "            [7.1117e-01],\n",
              "            [3.0165e-02]]]),\n",
              "   tensor([[[0.0207],\n",
              "            [0.0243],\n",
              "            [0.0255],\n",
              "            [0.0215],\n",
              "            [0.0207],\n",
              "            [0.1969],\n",
              "            [0.6054],\n",
              "            [0.0850]]]),\n",
              "   tensor([[[0.0702],\n",
              "            [0.0702],\n",
              "            [0.1017],\n",
              "            [0.0702],\n",
              "            [0.0702],\n",
              "            [0.1443],\n",
              "            [0.4027],\n",
              "            [0.0702]]]),\n",
              "   tensor([[[0.1023],\n",
              "            [0.0852],\n",
              "            [0.0852],\n",
              "            [0.0852],\n",
              "            [0.0852],\n",
              "            [0.1851],\n",
              "            [0.2759],\n",
              "            [0.0959]]]),\n",
              "   tensor([[[0.1029],\n",
              "            [0.1029],\n",
              "            [0.1029],\n",
              "            [0.1029],\n",
              "            [0.1029],\n",
              "            [0.1560],\n",
              "            [0.2269],\n",
              "            [0.1029]]]),\n",
              "   tensor([[[0.0959],\n",
              "            [0.0941],\n",
              "            [0.0941],\n",
              "            [0.0941],\n",
              "            [0.0941],\n",
              "            [0.1909],\n",
              "            [0.2427],\n",
              "            [0.0941]]]),\n",
              "   tensor([[[0.1005],\n",
              "            [0.1005],\n",
              "            [0.1005],\n",
              "            [0.1005],\n",
              "            [0.1005],\n",
              "            [0.1707],\n",
              "            [0.2265],\n",
              "            [0.1005]]]),\n",
              "   tensor([[[0.0950],\n",
              "            [0.0881],\n",
              "            [0.0881],\n",
              "            [0.0881],\n",
              "            [0.0881],\n",
              "            [0.2073],\n",
              "            [0.2568],\n",
              "            [0.0886]]]),\n",
              "   tensor([[[0.0829],\n",
              "            [0.0659],\n",
              "            [0.0659],\n",
              "            [0.0659],\n",
              "            [0.0659],\n",
              "            [0.2577],\n",
              "            [0.3133],\n",
              "            [0.0824]]]),\n",
              "   tensor([[[0.0727],\n",
              "            [0.0546],\n",
              "            [0.0546],\n",
              "            [0.0546],\n",
              "            [0.0546],\n",
              "            [0.2916],\n",
              "            [0.3414],\n",
              "            [0.0759]]]),\n",
              "   tensor([[[0.0670],\n",
              "            [0.0514],\n",
              "            [0.0514],\n",
              "            [0.0514],\n",
              "            [0.0514],\n",
              "            [0.3016],\n",
              "            [0.3519],\n",
              "            [0.0738]]]),\n",
              "   tensor([[[0.0646],\n",
              "            [0.0502],\n",
              "            [0.0505],\n",
              "            [0.0502],\n",
              "            [0.0502],\n",
              "            [0.3054],\n",
              "            [0.3559],\n",
              "            [0.0730]]]),\n",
              "   tensor([[[0.0634],\n",
              "            [0.0497],\n",
              "            [0.0503],\n",
              "            [0.0497],\n",
              "            [0.0497],\n",
              "            [0.3065],\n",
              "            [0.3578],\n",
              "            [0.0727]]]),\n",
              "   tensor([[[0.0629],\n",
              "            [0.0496],\n",
              "            [0.0501],\n",
              "            [0.0496],\n",
              "            [0.0496],\n",
              "            [0.3071],\n",
              "            [0.3584],\n",
              "            [0.0726]]]),\n",
              "   tensor([[[0.0627],\n",
              "            [0.0496],\n",
              "            [0.0499],\n",
              "            [0.0496],\n",
              "            [0.0496],\n",
              "            [0.3076],\n",
              "            [0.3585],\n",
              "            [0.0724]]])]),\n",
              " (tensor([[ 5, 10,  3, 20,  5, 10,  3,  5, 10,  3, 20, 20, 11,  1,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[9.9999e-01],\n",
              "            [1.1403e-07],\n",
              "            [3.5543e-06],\n",
              "            [1.4191e-06],\n",
              "            [1.1403e-07],\n",
              "            [4.3523e-07],\n",
              "            [2.8408e-07],\n",
              "            [1.4300e-07]]]),\n",
              "   tensor([[[0.9799],\n",
              "            [0.0059],\n",
              "            [0.0031],\n",
              "            [0.0024],\n",
              "            [0.0020],\n",
              "            [0.0025],\n",
              "            [0.0021],\n",
              "            [0.0021]]]),\n",
              "   tensor([[[0.1801],\n",
              "            [0.4815],\n",
              "            [0.0858],\n",
              "            [0.0555],\n",
              "            [0.0468],\n",
              "            [0.0544],\n",
              "            [0.0499],\n",
              "            [0.0461]]]),\n",
              "   tensor([[[0.0168],\n",
              "            [0.0815],\n",
              "            [0.8248],\n",
              "            [0.0270],\n",
              "            [0.0093],\n",
              "            [0.0213],\n",
              "            [0.0100],\n",
              "            [0.0093]]]),\n",
              "   tensor([[[2.6314e-02],\n",
              "            [6.9571e-04],\n",
              "            [8.0423e-01],\n",
              "            [1.3386e-01],\n",
              "            [6.1111e-03],\n",
              "            [1.7048e-02],\n",
              "            [6.7444e-03],\n",
              "            [4.9975e-03]]]),\n",
              "   tensor([[[1.0770e-02],\n",
              "            [1.7740e-04],\n",
              "            [9.2696e-01],\n",
              "            [3.5938e-02],\n",
              "            [4.5579e-03],\n",
              "            [9.9586e-03],\n",
              "            [6.9364e-03],\n",
              "            [4.7026e-03]]]),\n",
              "   tensor([[[4.4900e-02],\n",
              "            [5.4303e-04],\n",
              "            [5.9679e-01],\n",
              "            [1.5922e-01],\n",
              "            [4.2062e-02],\n",
              "            [6.0773e-02],\n",
              "            [5.3701e-02],\n",
              "            [4.2010e-02]]]),\n",
              "   tensor([[[1.1455e-02],\n",
              "            [3.8489e-04],\n",
              "            [2.1865e-02],\n",
              "            [8.6273e-01],\n",
              "            [1.8290e-02],\n",
              "            [4.9425e-02],\n",
              "            [2.1565e-02],\n",
              "            [1.4286e-02]]]),\n",
              "   tensor([[[1.9413e-02],\n",
              "            [2.2000e-04],\n",
              "            [2.2838e-02],\n",
              "            [7.7219e-01],\n",
              "            [6.2388e-02],\n",
              "            [7.1366e-02],\n",
              "            [2.9535e-02],\n",
              "            [2.2048e-02]]]),\n",
              "   tensor([[[0.0193],\n",
              "            [0.0005],\n",
              "            [0.0093],\n",
              "            [0.4050],\n",
              "            [0.2361],\n",
              "            [0.1739],\n",
              "            [0.0834],\n",
              "            [0.0723]]]),\n",
              "   tensor([[[0.0037],\n",
              "            [0.0009],\n",
              "            [0.0050],\n",
              "            [0.0835],\n",
              "            [0.0748],\n",
              "            [0.5989],\n",
              "            [0.1495],\n",
              "            [0.0837]]]),\n",
              "   tensor([[[2.9976e-03],\n",
              "            [5.5540e-04],\n",
              "            [2.6892e-03],\n",
              "            [1.3762e-02],\n",
              "            [8.9794e-03],\n",
              "            [6.1170e-01],\n",
              "            [2.9775e-01],\n",
              "            [6.1570e-02]]]),\n",
              "   tensor([[[0.0062],\n",
              "            [0.0013],\n",
              "            [0.0045],\n",
              "            [0.0148],\n",
              "            [0.0139],\n",
              "            [0.2482],\n",
              "            [0.4739],\n",
              "            [0.2371]]]),\n",
              "   tensor([[[0.0391],\n",
              "            [0.0108],\n",
              "            [0.0311],\n",
              "            [0.0856],\n",
              "            [0.0423],\n",
              "            [0.1758],\n",
              "            [0.3637],\n",
              "            [0.2515]]]),\n",
              "   tensor([[[0.0554],\n",
              "            [0.0108],\n",
              "            [0.0108],\n",
              "            [0.0528],\n",
              "            [0.0278],\n",
              "            [0.1586],\n",
              "            [0.3626],\n",
              "            [0.3212]]]),\n",
              "   tensor([[[0.0945],\n",
              "            [0.0195],\n",
              "            [0.0195],\n",
              "            [0.0308],\n",
              "            [0.0510],\n",
              "            [0.1931],\n",
              "            [0.3073],\n",
              "            [0.2842]]]),\n",
              "   tensor([[[0.0811],\n",
              "            [0.0226],\n",
              "            [0.0226],\n",
              "            [0.0328],\n",
              "            [0.0365],\n",
              "            [0.1931],\n",
              "            [0.3356],\n",
              "            [0.2757]]]),\n",
              "   tensor([[[0.0700],\n",
              "            [0.0235],\n",
              "            [0.0235],\n",
              "            [0.0357],\n",
              "            [0.0235],\n",
              "            [0.2021],\n",
              "            [0.3802],\n",
              "            [0.2414]]]),\n",
              "   tensor([[[0.0736],\n",
              "            [0.0255],\n",
              "            [0.0255],\n",
              "            [0.0407],\n",
              "            [0.0255],\n",
              "            [0.1998],\n",
              "            [0.4075],\n",
              "            [0.2020]]]),\n",
              "   tensor([[[0.0768],\n",
              "            [0.0264],\n",
              "            [0.0264],\n",
              "            [0.0432],\n",
              "            [0.0264],\n",
              "            [0.1895],\n",
              "            [0.4139],\n",
              "            [0.1973]]])])]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i, pred in enumerate(output):\n",
        "    # print(len(pred))\n",
        "    embedding , attention = pred\n",
        "    word = output_decode(list(embedding.detach().numpy()[0]))\n",
        "    word = ''.join(word).replace('<EOW>', '~').replace('<PAD>', '_')\n",
        "    print(EXAMPLES[i], word)\n",
        "    print(\"Attention\", attention[0].detach().numpy().shape)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "WHysSqYJ1ZUA"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "XdktVnMv1ZTh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "7\n",
            "(1, 8, 1)\n"
          ]
        }
      ],
      "source": [
        "prediction, attention_scores = zip(*output)\n",
        "\n",
        "prediction = prediction[0].detach().numpy()\n",
        "attention_scores = attention_scores[0][0].detach().numpy()\n",
        "print(len(prediction))\n",
        "print(len(attention_scores))\n",
        "print(.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "BF6HD99lYlgQ",
        "outputId": "caaa0716-5b99-43e8-950f-dd0127d0fbb7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'attn_viz' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ax \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mheatmap(\u001b[43mattn_viz\u001b[49m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      2\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_yticklabels(output_text,rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xticklabels(xlabels,rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'attn_viz' is not defined"
          ]
        }
      ],
      "source": [
        "ax = sns.heatmap(attn_viz, linewidth=0.5)\n",
        "ax.set_yticklabels(output_text,rotation=30)\n",
        "ax.set_xticklabels(xlabels,rotation=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1UkIsCztaMS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
