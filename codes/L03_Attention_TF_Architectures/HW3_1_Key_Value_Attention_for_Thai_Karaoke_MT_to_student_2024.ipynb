{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfUmXr1D1ZSR"
      },
      "source": [
        "# Key-Value Attention for Thai Karaoke Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
        "\n",
        "In this homework, you will create an MT model with attention mechnism that coverts names of Thai 2019 MP candidates from Thai script to Roman(Latin) script. E.g. นิยม-->niyom\n",
        "\n",
        "The use of Pytorch Lightning is optional but recommended. You can use Pytorch if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "18KMSkqZ-Pt-"
      },
      "outputs": [],
      "source": [
        "!pip install lightning wandb -q\n",
        "# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKCBCWKARZEx",
        "outputId": "6ad8a585-fd68-44ee-deac-37143137cbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ka2TN8IV1ZSU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "mpl.font_manager.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n",
        "mpl.rc('font', family='TH Sarabun New')\n",
        "import torch\n",
        "# import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-f_s6vX1ZSZ"
      },
      "source": [
        "## Load Dataset\n",
        "We have generated a toy dataset using names of Thai MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
        "\n",
        "```\n",
        "ไกรสีห์ kraisi\n",
        "พัชรี phatri\n",
        "ธีระ thira\n",
        "วุฒิกร wutthikon\n",
        "ไสว sawai\n",
        "สัมภาษณ์  samphat\n",
        "วศิน wasin\n",
        "ทินวัฒน์ thinwat\n",
        "ศักดินัย sakdinai\n",
        "สุรศักดิ์ surasak\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jte-Csrf-4kd",
        "outputId": "a1ba364b-64c2-4875-873c-90bc1808a4be"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L9zXp7KH1ZSa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('mp_name_th_en.csv') as csvfile:\n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    name_th = []\n",
        "    name_en = []\n",
        "    for row in readCSV:\n",
        "        temp_th = row[0]\n",
        "        temp_en = row[1]\n",
        "\n",
        "        name_th.append(temp_th)\n",
        "        name_en.append(temp_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZCsqrXxu1ZSe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ไกรสีห์ kraisi\n",
            "พัชรี phatri\n",
            "ธีระ thira\n",
            "วุฒิกร wutthikon\n",
            "ไสว sawai\n",
            "สัมภาษณ์  samphat\n",
            "วศิน wasin\n",
            "ทินวัฒน์ thinwat\n",
            "ศักดินัย sakdinai\n",
            "สุรศักดิ์ surasak\n"
          ]
        }
      ],
      "source": [
        "for th, en in zip(name_th[:10],name_en[:10]):\n",
        "    print(th,en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvW8xqT81ZSh"
      },
      "source": [
        "## TODO1: Preprocess dataset\n",
        "* You will need 2 vocabularies (1 for input and another for output)\n",
        "* DON'T FORGET TO INCLUDE special token for padding (for both input and output)\n",
        "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rv1Xd9A1ZSi",
        "outputId": "dece74ae-a492-41a7-f07d-2798157c7fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 10887 lines and 65 unique characters in your input data.\n"
          ]
        }
      ],
      "source": [
        "#Preprocessing\n",
        "input_chars = list(set(''.join(name_th)))\n",
        "output_chars = list(set(''.join(name_en)))\n",
        "data_size, vocab_size = len(name_th), len(input_chars)+1\n",
        "output_vocab_size = len(output_chars)+2#+2 for special end of sentence token/PADDING\n",
        "print('There are %d lines and %d unique characters in your input data.' % (data_size, vocab_size))\n",
        "maxlen = len( max(name_th, key=len)) #max input length\n",
        "maxlen_out = len( max(name_en, key=len)) #max output length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example:\n",
            "[4, 24, 19, 47, 56, 4, 34, 20, 54, 2, 60, 6]\n",
            "['ค', 'น', 'ด', 'ำ', 'โ', 'ค', 'ร', 'ต', 'เ', 'ก', '่', 'ง']\n",
            "\n",
            "Check X and Y of dataset\n",
            "torch.Size([10887, 20])\n",
            "torch.Size([10887, 20])\n",
            "\n",
            "Example:\n",
            "tensor([58,  2, 34, 39, 49, 40, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0]) \n",
            " ['ไ', 'ก', 'ร', 'ส', 'ี', 'ห', '์', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "tensor([12, 18,  3, 11, 19, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0]) \n",
            " ['k', 'r', 'a', 'i', 's', 'i', '<EOW>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ],
      "source": [
        "# Written by Jeans\n",
        "maxlen_out += 1 # add 1 for <EOW> token???\n",
        "\n",
        "#create  (EMBEDDING)  dictionaries to convert character to integer and vice versa\n",
        "input_chars = ['<PAD>'] + sorted(input_chars)\n",
        "output_chars = ['<PAD>','<EOW>'] + sorted(output_chars)\n",
        "# print(input_chars)\n",
        "input_char_to_idx = { char: idx for idx, char in enumerate(input_chars)}\n",
        "input_idx_to_char = { idx: char for idx, char in enumerate(input_chars)}\n",
        "input_encode = lambda x: [input_char_to_idx[char] for char in x]\n",
        "input_decode = lambda l: [input_idx_to_char[idx] for idx in l]\n",
        "\n",
        "output_char_to_idx = { char: idx for idx, char in enumerate(output_chars)}\n",
        "output_idx_to_char = { idx: char for idx, char in enumerate(output_chars)}\n",
        "output_encode = lambda x: [output_char_to_idx[char] for char in x]\n",
        "output_decode = lambda l: [output_idx_to_char[idx] for idx in l]\n",
        "\n",
        "# Example\n",
        "print(\"Example:\")\n",
        "print(input_encode(\"คนดำโครตเก่ง\"))\n",
        "print(input_decode(input_encode(\"คนดำโครตเก่ง\")))\n",
        "\n",
        "# print(\"Encoder Mapper:\")\n",
        "# print(input_char_to_idx)\n",
        "\n",
        "# print(\"Decoder Mapper:\")\n",
        "# print(output_char_to_idx)\n",
        "\n",
        "\n",
        "#PROCESS DATA FOR MODEL\n",
        "# tokenize and embed inputs\n",
        "# tokenize and embed outputs\n",
        "# pad inputs\n",
        "X = []\n",
        "for word in name_th:\n",
        "    word = [char for char in word]\n",
        "    X.append(torch.tensor(input_encode(word))) # convert to tensor shape (len(X), len(word))\n",
        "X = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0) # pad all sequences to same length with 0 : <PAD>\n",
        "\n",
        "Y = []\n",
        "for word in name_en:\n",
        "    word = [char for char in word] + ['<EOW>']\n",
        "    Y.append(torch.tensor(output_encode(word))) # convert to tensor shape (len(Y), len(word))\n",
        "Y = nn.utils.rnn.pad_sequence(Y, batch_first=True, padding_value=0).long() # pad all sequences to same length with 0 : <PAD>\n",
        "print(\"\\nCheck X and Y of dataset\")\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(\"\\nExample:\")\n",
        "print(X[0], \"\\n\", input_decode(list(X[0].detach().numpy().astype(int)))) # 0 is the first word in the dataset)\n",
        "print(Y[0], \"\\n\", output_decode(list(Y[0].detach().numpy().astype(int)))) # 0 is the first word in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo381I_t1ZSm",
        "outputId": "4467516a-90c8-477d-bc43-bb071b17a956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max input length: 20\n",
            "Max output length: 20\n"
          ]
        }
      ],
      "source": [
        "print(\"Max input length:\", maxlen)\n",
        "print(\"Max output length:\", maxlen_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W3aXyJBEC-j_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-yirzlseC9NS"
      },
      "outputs": [],
      "source": [
        "class NameDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    #FILL CODE HERE pass\n",
        "    self.encoded_X = X.long()\n",
        "    self.encoded_label = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #FILL CODE HERE pass\n",
        "    return {\"x\" :self.encoded_X[idx], \"y\":self.encoded_label[idx]}\n",
        "\n",
        "  def __len__(self):\n",
        "    #FILL CODE HERE pass\n",
        "    return len(self.encoded_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qUPAB7LTDFOy"
      },
      "outputs": [],
      "source": [
        "class NameDataModule(L.LightningDataModule):\n",
        "\n",
        "  def __init__(self, train_data, y, batch_size, num_workers=0):\n",
        "      super().__init__()\n",
        "      self.train_data = train_data\n",
        "      self.y = y\n",
        "      self.batch_size = batch_size\n",
        "      self.num_workers = num_workers\n",
        "\n",
        "\n",
        "  def setup(self, stage: str):\n",
        "    pass\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    '''turn encoded inputs into one-hot vectors'''\n",
        "    batched_one_hot_x = torch.stack([F.one_hot(dp[\"x\"], num_classes=vocab_size) for dp in batch])\n",
        "    batched_y = torch.stack([dp[\"y\"] for dp in batch])\n",
        "    return {\"x\": batched_one_hot_x.float(), \"y\": batched_y}    \n",
        "\n",
        "  def train_dataloader(self):\n",
        "    train_dataset = NameDataset(self.train_data, self.y)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                                batch_size = self.batch_size,\n",
        "                                shuffle = True,\n",
        "                                collate_fn = self.collate_fn,\n",
        "                                num_workers = self.num_workers)\n",
        "    return train_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSG1FqK1ZSy"
      },
      "source": [
        "# Attention Mechanism\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAlOrhbismQp"
      },
      "source": [
        "## TODO 2: Code your own (key-value) attention mechnism\n",
        "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
        "* fill code for one_step_attention function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "avnlc6p9BZDv"
      },
      "outputs": [],
      "source": [
        "def one_step_attention(h, s_prev, attention_layer, key_ff_layer):\n",
        "    '''\n",
        "    h.shape = (batch_size, seq_len, hidden_size)\n",
        "    '''\n",
        "    #Split into Key-Value\n",
        "    key, value = torch.split(h, h.shape[2]//2, dim=2) # (b, seq_len, hidden_size//2)\n",
        "    #do concat with s_prev.\n",
        "    #hint: you will need to use s_prev.repeat(...) somehow so that it has the same dimension as the key\n",
        "    #hint2: s_prev.unsqueeze() could also be useful\n",
        "    s_prev = s_prev.unsqueeze(1).repeat((1, h.shape[1], 1))\n",
        "    concatenated = torch.cat((key, s_prev), dim=2) # (b, seq_len, hidden_size)\n",
        "\n",
        "    #Attention function###\n",
        "    # use layer(s) from your model to calculate attention_scores and then softmax\n",
        "    # calculate a context vector\n",
        "    f_attn = F.tanh(attention_layer(concatenated)) # (b, seq_len, 1)\n",
        "    f_attn = F.relu(key_ff_layer(f_attn))\n",
        "    \n",
        "    attention_scores = F.softmax(f_attn, dim=1) # (b, seq_len, 1)\n",
        "    \n",
        "    tmp = torch.mul(attention_scores, value) # (b, seq_len, hidden_size//2)\n",
        "    context = torch.sum(tmp,dim=1)\n",
        "    \n",
        "    return context, attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zWN02ZtuOIU"
      },
      "source": [
        "# Translation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0phyUQYg1ZS8"
      },
      "source": [
        "## TODO3: Create and train your encoder/decoder model here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ji_rUPhK1ZS9"
      },
      "outputs": [],
      "source": [
        "class AttentionModel(L.LightningModule):\n",
        "    def __init__(self, learning_rate, criterion):\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_h = 64 #hidden dimensions for encoder\n",
        "        self.n_s = 64 #hidden dimensions for decoder\n",
        "\n",
        "        #encoder can be any RNN of your choice\n",
        "        bi_direction = True\n",
        "        self.num_directions = 2 if bi_direction else 1\n",
        "        self.lstm = nn.LSTM(len(input_char_to_idx), self.n_h, bidirectional=bi_direction, batch_first=True)\n",
        "        #decoder has to be (any) RNNCell since we will need to calculate attention for each timestep manually\n",
        "        self.decoder_lstm_cell = nn.LSTMCell(self.n_s, self.n_s)\n",
        "        self.output_layer = nn.Linear(self.n_s, len(output_char_to_idx))\n",
        "        #attention\n",
        "        self.attention_layer = nn.Linear((self.n_h//2)*2*self.num_directions, (self.n_h//2))\n",
        "        self.key_ff_layer = nn.Linear((self.n_h//2), 1)\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def forward(self, src, return_attention=False): #use return_attention only when you want to get the attention scores for visualizing\n",
        "        #pass the input to the encoder\n",
        "        lstm_out, _ = self.lstm(src)\n",
        "        #Initialize the LSTM states. We have to do this since we are using LSTMCell (https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
        "        #These states will get updated while we are decoding\n",
        "        decoder_s = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "        decoder_c = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "\n",
        "        #Iterate until max_output_length (Decoding)\n",
        "        prediction = torch.zeros((src.shape[0], maxlen_out, len(output_char_to_idx))).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "        attention_scores = [] #to store the score for each step\n",
        "        for t in range(maxlen_out):\n",
        "            \n",
        "            #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
        "            context, attention_score = one_step_attention(lstm_out, decoder_s, self.attention_layer, self.key_ff_layer)\n",
        "            attention_scores.append(attention_score)\n",
        "\n",
        "            # Feed the context vector to the decoder.\n",
        "            decoder_s, decoder_c = self.decoder_lstm_cell(context, (decoder_s, decoder_c))\n",
        "            # Pass the decoder hidden output to the output layer (softmax)\n",
        "            out = self.output_layer(decoder_s)\n",
        "            # Put the predicted output into the list for this timestep\n",
        "            prediction[:, t] = out\n",
        "\n",
        "        return (prediction, attention_scores if return_attention else None)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src = batch['x']\n",
        "        target = batch['y']\n",
        "        prediction,_ = self(src)\n",
        "        prediction = prediction.reshape(-1, len(output_char_to_idx))\n",
        "        target = target.reshape(-1)\n",
        "        loss = self.criterion(prediction, target)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        src = batch['x']\n",
        "        with torch.no_grad():\n",
        "          prediction, attention_scores = self(src, return_attention=True)\n",
        "          prediction = F.softmax(prediction, dim=-1)\n",
        "          prediction = torch.argmax(prediction, dim=-1)\n",
        "          for pred in prediction:\n",
        "            # print(\"\".join(output_char_to_idx.lookup_tokens(pred.cpu().numpy())))\n",
        "            print(\"\".join(output_decode(pred.cpu().numpy())))\n",
        "            \n",
        "        return prediction, attention_scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pSM9dgDcCz1E"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.01\n",
        "model = AttentionModel(lr, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RqrvmJalDLzF"
      },
      "outputs": [],
      "source": [
        "data_module = NameDataModule(X, Y, batch_size=4096)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_sFjzKX8SECo"
      },
      "outputs": [],
      "source": [
        "from lightning import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "wandb_logger = WandbLogger(project=\"hw3.1_attention\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OGWSzS-X1ZTO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(\n",
        "    max_epochs=1000,\n",
        "    # logger=wandb_logger\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7ZMi782c1ZTQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type             | Params | Mode \n",
            "---------------------------------------------------------------\n",
            "0 | lstm              | LSTM             | 67.1 K | train\n",
            "1 | decoder_lstm_cell | LSTMCell         | 33.3 K | train\n",
            "2 | output_layer      | Linear           | 1.6 K  | train\n",
            "3 | attention_layer   | Linear           | 4.1 K  | train\n",
            "4 | key_ff_layer      | Linear           | 33     | train\n",
            "5 | criterion         | CrossEntropyLoss | 0      | train\n",
            "---------------------------------------------------------------\n",
            "106 K     Trainable params\n",
            "0         Non-trainable params\n",
            "106 K     Total params\n",
            "0.424     Total estimated model params size (MB)\n",
            "6         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 999: 100%|██████████| 6/6 [00:00<00:00, -11.50it/s, v_num=5]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 999: 100%|██████████| 6/6 [00:00<00:00, -11.67it/s, v_num=5]\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5BLw1Ir1ZTT"
      },
      "source": [
        "# Test Your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRLjZzBMtCdA"
      },
      "source": [
        "## TODO4: Test your model on 5 examples of your choice including your name!\n",
        "\n",
        "Example Output:\n",
        "```\n",
        "prayutthatha</s></s>aa</s></s>a</s>\n",
        "somchai</s></s></s></s>a</s></s>a</s></s></s></s></s>\n",
        "thanathon</s></s></s></s></s></s></s></s></s></s></s>\n",
        "newin</s>i</s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "suthep</s>he</s></s></s></s></s></s></s></s></s></s></s>\n",
        "prawit</s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "chatchachatti</s></s>i</s></s></s></s>\n",
        "```\n",
        "\n",
        "<font color='blue'>Paste your model predictions in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6stNACsUP9h-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "EXAMPLES = ['ประยุทธ','สมชาย','ธนาธร','เนวิน','สุเทพ','ประวิตร์','ชัชชาติ']\n",
        "\n",
        "#Writenn by Jeans\n",
        "\n",
        "predict_data = []\n",
        "for line in EXAMPLES:\n",
        "    line = [l for l in line] #change from string to list\n",
        "    predict_data.append(torch.tensor(input_encode(line)))\n",
        "\n",
        "print(len(predict_data))\n",
        "\n",
        "def collate_fn_(batch):\n",
        "    batched_one_hot_x = torch.stack([F.one_hot(dp[\"x\"], num_classes=vocab_size) for dp in batch])\n",
        "    return {\"x\": batched_one_hot_x.float()}    \n",
        "\n",
        "predict_data = nn.utils.rnn.pad_sequence(predict_data, batch_first = True)\n",
        "predict_dataset = NameDataset(predict_data, [torch.tensor(0)]*len(predict_data))\n",
        "predict_loader = DataLoader(predict_dataset,\n",
        "                          batch_size = 1,\n",
        "                          shuffle = False,\n",
        "                          collate_fn = collate_fn_,\n",
        "                          num_workers = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kbolC8XIhR3t"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AttentionModel(\n",
              "  (lstm): LSTM(65, 64, batch_first=True, bidirectional=True)\n",
              "  (decoder_lstm_cell): LSTMCell(64, 64)\n",
              "  (output_layer): Linear(in_features=64, out_features=24, bias=True)\n",
              "  (attention_layer): Linear(in_features=128, out_features=32, bias=True)\n",
              "  (key_ff_layer): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LsN71S9uQ9wo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]prayuttha<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  14%|█▍        | 1/7 [00:00<00:00, 62.66it/s]somchai<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  29%|██▊       | 2/7 [00:00<00:00, 78.73it/s]thanathon<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  43%|████▎     | 3/7 [00:00<00:00, 86.39it/s]newin<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  57%|█████▋    | 4/7 [00:00<00:00, 95.03it/s]suthep<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  71%|███████▏  | 5/7 [00:00<00:00, 99.11it/s]prawi<EOW><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0:  86%|████████▌ | 6/7 [00:00<00:00, 93.28it/s]chatchachatti<EOW><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 95.09it/s]\n"
          ]
        }
      ],
      "source": [
        "output = trainer.predict(model, predict_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o3893RL1ZT8"
      },
      "source": [
        "## TODO 5: Show your visualization of attention scores on one of your example\n",
        "\n",
        "<font color='blue'>Paste your visualization image in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(tensor([[17, 18,  3, 23, 21, 20, 20, 10,  3,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[9.9999e-01],\n",
              "            [2.4487e-06],\n",
              "            [2.2238e-06],\n",
              "            [1.1131e-07],\n",
              "            [1.8300e-07],\n",
              "            [1.7011e-07],\n",
              "            [1.5669e-07],\n",
              "            [2.8970e-08]]]),\n",
              "   tensor([[[0.0613],\n",
              "            [0.8628],\n",
              "            [0.0294],\n",
              "            [0.0100],\n",
              "            [0.0088],\n",
              "            [0.0096],\n",
              "            [0.0096],\n",
              "            [0.0085]]]),\n",
              "   tensor([[[0.0329],\n",
              "            [0.1714],\n",
              "            [0.6240],\n",
              "            [0.1014],\n",
              "            [0.0205],\n",
              "            [0.0178],\n",
              "            [0.0170],\n",
              "            [0.0150]]]),\n",
              "   tensor([[[0.0056],\n",
              "            [0.0037],\n",
              "            [0.0051],\n",
              "            [0.8189],\n",
              "            [0.1297],\n",
              "            [0.0242],\n",
              "            [0.0093],\n",
              "            [0.0033]]]),\n",
              "   tensor([[[0.0613],\n",
              "            [0.0019],\n",
              "            [0.0043],\n",
              "            [0.2388],\n",
              "            [0.5889],\n",
              "            [0.0616],\n",
              "            [0.0237],\n",
              "            [0.0196]]]),\n",
              "   tensor([[[3.8782e-03],\n",
              "            [2.1610e-04],\n",
              "            [3.7479e-04],\n",
              "            [2.8879e-02],\n",
              "            [4.4997e-01],\n",
              "            [4.7602e-01],\n",
              "            [3.0572e-02],\n",
              "            [1.0093e-02]]]),\n",
              "   tensor([[[4.3476e-03],\n",
              "            [7.0818e-04],\n",
              "            [1.7573e-04],\n",
              "            [6.6189e-03],\n",
              "            [1.3955e-02],\n",
              "            [6.9746e-01],\n",
              "            [2.5921e-01],\n",
              "            [1.7519e-02]]]),\n",
              "   tensor([[[6.0465e-03],\n",
              "            [9.6525e-04],\n",
              "            [2.5993e-04],\n",
              "            [1.0499e-03],\n",
              "            [4.2170e-03],\n",
              "            [5.1800e-01],\n",
              "            [4.3950e-01],\n",
              "            [2.9964e-02]]]),\n",
              "   tensor([[[6.4681e-03],\n",
              "            [2.0293e-03],\n",
              "            [7.1860e-04],\n",
              "            [1.6801e-03],\n",
              "            [4.6778e-03],\n",
              "            [1.0250e-01],\n",
              "            [7.6502e-01],\n",
              "            [1.1691e-01]]]),\n",
              "   tensor([[[0.0052],\n",
              "            [0.0068],\n",
              "            [0.0057],\n",
              "            [0.0156],\n",
              "            [0.0111],\n",
              "            [0.0741],\n",
              "            [0.3920],\n",
              "            [0.4895]]]),\n",
              "   tensor([[[0.0112],\n",
              "            [0.0133],\n",
              "            [0.0135],\n",
              "            [0.0112],\n",
              "            [0.0112],\n",
              "            [0.0859],\n",
              "            [0.3670],\n",
              "            [0.4868]]]),\n",
              "   tensor([[[0.0251],\n",
              "            [0.0281],\n",
              "            [0.0251],\n",
              "            [0.0251],\n",
              "            [0.0251],\n",
              "            [0.0581],\n",
              "            [0.2302],\n",
              "            [0.5833]]]),\n",
              "   tensor([[[0.0550],\n",
              "            [0.0550],\n",
              "            [0.0579],\n",
              "            [0.0550],\n",
              "            [0.0550],\n",
              "            [0.1075],\n",
              "            [0.2742],\n",
              "            [0.3403]]]),\n",
              "   tensor([[[0.0499],\n",
              "            [0.0499],\n",
              "            [0.0796],\n",
              "            [0.0499],\n",
              "            [0.0499],\n",
              "            [0.0965],\n",
              "            [0.2720],\n",
              "            [0.3524]]]),\n",
              "   tensor([[[0.0508],\n",
              "            [0.0508],\n",
              "            [0.0874],\n",
              "            [0.0508],\n",
              "            [0.0508],\n",
              "            [0.0954],\n",
              "            [0.2749],\n",
              "            [0.3392]]]),\n",
              "   tensor([[[0.0516],\n",
              "            [0.0516],\n",
              "            [0.0847],\n",
              "            [0.0516],\n",
              "            [0.0516],\n",
              "            [0.0988],\n",
              "            [0.2909],\n",
              "            [0.3194]]]),\n",
              "   tensor([[[0.0452],\n",
              "            [0.0452],\n",
              "            [0.0844],\n",
              "            [0.0452],\n",
              "            [0.0452],\n",
              "            [0.1004],\n",
              "            [0.3458],\n",
              "            [0.2885]]]),\n",
              "   tensor([[[0.0375],\n",
              "            [0.0375],\n",
              "            [0.0897],\n",
              "            [0.0375],\n",
              "            [0.0375],\n",
              "            [0.1034],\n",
              "            [0.4094],\n",
              "            [0.2476]]]),\n",
              "   tensor([[[0.0366],\n",
              "            [0.0366],\n",
              "            [0.0924],\n",
              "            [0.0366],\n",
              "            [0.0366],\n",
              "            [0.1002],\n",
              "            [0.3938],\n",
              "            [0.2673]]]),\n",
              "   tensor([[[0.0376],\n",
              "            [0.0376],\n",
              "            [0.0914],\n",
              "            [0.0376],\n",
              "            [0.0376],\n",
              "            [0.0969],\n",
              "            [0.3826],\n",
              "            [0.2787]]])]),\n",
              " (tensor([[19, 16, 14,  5, 10,  3, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[1.0000e+00],\n",
              "            [6.2120e-10],\n",
              "            [6.2120e-10],\n",
              "            [6.2120e-10],\n",
              "            [6.2120e-10],\n",
              "            [6.2120e-10],\n",
              "            [6.2120e-10],\n",
              "            [6.2120e-10]]]),\n",
              "   tensor([[[0.5615],\n",
              "            [0.1660],\n",
              "            [0.0484],\n",
              "            [0.0448],\n",
              "            [0.0459],\n",
              "            [0.0449],\n",
              "            [0.0445],\n",
              "            [0.0440]]]),\n",
              "   tensor([[[0.0128],\n",
              "            [0.9514],\n",
              "            [0.0204],\n",
              "            [0.0032],\n",
              "            [0.0050],\n",
              "            [0.0029],\n",
              "            [0.0024],\n",
              "            [0.0020]]]),\n",
              "   tensor([[[0.0183],\n",
              "            [0.0171],\n",
              "            [0.8868],\n",
              "            [0.0238],\n",
              "            [0.0363],\n",
              "            [0.0067],\n",
              "            [0.0054],\n",
              "            [0.0055]]]),\n",
              "   tensor([[[0.0327],\n",
              "            [0.0012],\n",
              "            [0.8194],\n",
              "            [0.0648],\n",
              "            [0.0445],\n",
              "            [0.0144],\n",
              "            [0.0115],\n",
              "            [0.0114]]]),\n",
              "   tensor([[[0.0528],\n",
              "            [0.0006],\n",
              "            [0.1560],\n",
              "            [0.4126],\n",
              "            [0.2504],\n",
              "            [0.0431],\n",
              "            [0.0411],\n",
              "            [0.0434]]]),\n",
              "   tensor([[[0.0071],\n",
              "            [0.0010],\n",
              "            [0.0083],\n",
              "            [0.0371],\n",
              "            [0.8843],\n",
              "            [0.0338],\n",
              "            [0.0184],\n",
              "            [0.0099]]]),\n",
              "   tensor([[[0.0047],\n",
              "            [0.0042],\n",
              "            [0.0123],\n",
              "            [0.0281],\n",
              "            [0.4684],\n",
              "            [0.2768],\n",
              "            [0.1397],\n",
              "            [0.0657]]]),\n",
              "   tensor([[[0.0116],\n",
              "            [0.0162],\n",
              "            [0.0195],\n",
              "            [0.0128],\n",
              "            [0.2779],\n",
              "            [0.1633],\n",
              "            [0.3071],\n",
              "            [0.1918]]]),\n",
              "   tensor([[[0.0968],\n",
              "            [0.0776],\n",
              "            [0.0776],\n",
              "            [0.0776],\n",
              "            [0.1894],\n",
              "            [0.1849],\n",
              "            [0.2036],\n",
              "            [0.0926]]]),\n",
              "   tensor([[[0.1179],\n",
              "            [0.0878],\n",
              "            [0.0878],\n",
              "            [0.0878],\n",
              "            [0.2012],\n",
              "            [0.1955],\n",
              "            [0.1342],\n",
              "            [0.0878]]]),\n",
              "   tensor([[[0.1103],\n",
              "            [0.0959],\n",
              "            [0.0959],\n",
              "            [0.0959],\n",
              "            [0.1843],\n",
              "            [0.1861],\n",
              "            [0.1357],\n",
              "            [0.0959]]]),\n",
              "   tensor([[[0.1072],\n",
              "            [0.0942],\n",
              "            [0.0942],\n",
              "            [0.0942],\n",
              "            [0.1902],\n",
              "            [0.1920],\n",
              "            [0.1338],\n",
              "            [0.0942]]]),\n",
              "   tensor([[[0.1158],\n",
              "            [0.0846],\n",
              "            [0.0846],\n",
              "            [0.0846],\n",
              "            [0.2201],\n",
              "            [0.1946],\n",
              "            [0.1312],\n",
              "            [0.0846]]]),\n",
              "   tensor([[[0.1202],\n",
              "            [0.0632],\n",
              "            [0.0632],\n",
              "            [0.0632],\n",
              "            [0.3000],\n",
              "            [0.2049],\n",
              "            [0.1199],\n",
              "            [0.0654]]]),\n",
              "   tensor([[[0.1208],\n",
              "            [0.0635],\n",
              "            [0.0635],\n",
              "            [0.0635],\n",
              "            [0.3022],\n",
              "            [0.2031],\n",
              "            [0.1182],\n",
              "            [0.0652]]]),\n",
              "   tensor([[[0.1199],\n",
              "            [0.0609],\n",
              "            [0.0609],\n",
              "            [0.0609],\n",
              "            [0.3138],\n",
              "            [0.2034],\n",
              "            [0.1155],\n",
              "            [0.0648]]]),\n",
              "   tensor([[[0.1210],\n",
              "            [0.0618],\n",
              "            [0.0618],\n",
              "            [0.0618],\n",
              "            [0.3117],\n",
              "            [0.2024],\n",
              "            [0.1150],\n",
              "            [0.0646]]]),\n",
              "   tensor([[[0.1227],\n",
              "            [0.0614],\n",
              "            [0.0614],\n",
              "            [0.0614],\n",
              "            [0.3147],\n",
              "            [0.2008],\n",
              "            [0.1134],\n",
              "            [0.0643]]]),\n",
              "   tensor([[[0.1241],\n",
              "            [0.0616],\n",
              "            [0.0616],\n",
              "            [0.0616],\n",
              "            [0.3143],\n",
              "            [0.1997],\n",
              "            [0.1129],\n",
              "            [0.0641]]])]),\n",
              " (tensor([[20, 10,  3, 15,  3, 20, 10, 16, 15,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[1.0000e+00],\n",
              "            [1.6072e-09],\n",
              "            [1.6712e-09],\n",
              "            [3.2123e-09],\n",
              "            [1.7002e-09],\n",
              "            [1.3685e-09],\n",
              "            [1.3685e-09],\n",
              "            [1.3685e-09]]]),\n",
              "   tensor([[[0.9504],\n",
              "            [0.0085],\n",
              "            [0.0072],\n",
              "            [0.0082],\n",
              "            [0.0068],\n",
              "            [0.0064],\n",
              "            [0.0063],\n",
              "            [0.0062]]]),\n",
              "   tensor([[[0.3130],\n",
              "            [0.2491],\n",
              "            [0.0858],\n",
              "            [0.0777],\n",
              "            [0.0751],\n",
              "            [0.0698],\n",
              "            [0.0673],\n",
              "            [0.0622]]]),\n",
              "   tensor([[[0.0183],\n",
              "            [0.8354],\n",
              "            [0.0772],\n",
              "            [0.0252],\n",
              "            [0.0125],\n",
              "            [0.0108],\n",
              "            [0.0105],\n",
              "            [0.0100]]]),\n",
              "   tensor([[[0.0304],\n",
              "            [0.0778],\n",
              "            [0.5611],\n",
              "            [0.2314],\n",
              "            [0.0348],\n",
              "            [0.0222],\n",
              "            [0.0214],\n",
              "            [0.0209]]]),\n",
              "   tensor([[[0.0040],\n",
              "            [0.0011],\n",
              "            [0.0283],\n",
              "            [0.9249],\n",
              "            [0.0224],\n",
              "            [0.0065],\n",
              "            [0.0061],\n",
              "            [0.0065]]]),\n",
              "   tensor([[[0.0163],\n",
              "            [0.0029],\n",
              "            [0.0311],\n",
              "            [0.8327],\n",
              "            [0.0654],\n",
              "            [0.0163],\n",
              "            [0.0170],\n",
              "            [0.0184]]]),\n",
              "   tensor([[[0.0468],\n",
              "            [0.0013],\n",
              "            [0.0105],\n",
              "            [0.4650],\n",
              "            [0.2628],\n",
              "            [0.0736],\n",
              "            [0.0715],\n",
              "            [0.0686]]]),\n",
              "   tensor([[[0.0047],\n",
              "            [0.0035],\n",
              "            [0.0025],\n",
              "            [0.0646],\n",
              "            [0.8147],\n",
              "            [0.0499],\n",
              "            [0.0404],\n",
              "            [0.0197]]]),\n",
              "   tensor([[[0.0064],\n",
              "            [0.0105],\n",
              "            [0.0084],\n",
              "            [0.0424],\n",
              "            [0.6642],\n",
              "            [0.2017],\n",
              "            [0.0439],\n",
              "            [0.0224]]]),\n",
              "   tensor([[[0.0297],\n",
              "            [0.0088],\n",
              "            [0.0094],\n",
              "            [0.0337],\n",
              "            [0.2691],\n",
              "            [0.3230],\n",
              "            [0.2060],\n",
              "            [0.1203]]]),\n",
              "   tensor([[[0.0466],\n",
              "            [0.0097],\n",
              "            [0.0097],\n",
              "            [0.0101],\n",
              "            [0.2223],\n",
              "            [0.3075],\n",
              "            [0.2708],\n",
              "            [0.1232]]]),\n",
              "   tensor([[[0.1517],\n",
              "            [0.0466],\n",
              "            [0.0466],\n",
              "            [0.0466],\n",
              "            [0.2053],\n",
              "            [0.2469],\n",
              "            [0.1656],\n",
              "            [0.0909]]]),\n",
              "   tensor([[[0.1289],\n",
              "            [0.0603],\n",
              "            [0.0603],\n",
              "            [0.0603],\n",
              "            [0.2497],\n",
              "            [0.2121],\n",
              "            [0.1475],\n",
              "            [0.0810]]]),\n",
              "   tensor([[[0.1516],\n",
              "            [0.0670],\n",
              "            [0.0670],\n",
              "            [0.0736],\n",
              "            [0.2378],\n",
              "            [0.1894],\n",
              "            [0.1385],\n",
              "            [0.0752]]]),\n",
              "   tensor([[[0.1446],\n",
              "            [0.0569],\n",
              "            [0.0569],\n",
              "            [0.0742],\n",
              "            [0.2575],\n",
              "            [0.1902],\n",
              "            [0.1412],\n",
              "            [0.0786]]]),\n",
              "   tensor([[[0.2052],\n",
              "            [0.0436],\n",
              "            [0.0436],\n",
              "            [0.0769],\n",
              "            [0.2720],\n",
              "            [0.1601],\n",
              "            [0.1199],\n",
              "            [0.0789]]]),\n",
              "   tensor([[[0.1718],\n",
              "            [0.0350],\n",
              "            [0.0350],\n",
              "            [0.0717],\n",
              "            [0.3059],\n",
              "            [0.1666],\n",
              "            [0.1255],\n",
              "            [0.0884]]]),\n",
              "   tensor([[[0.1637],\n",
              "            [0.0407],\n",
              "            [0.0407],\n",
              "            [0.0769],\n",
              "            [0.2968],\n",
              "            [0.1707],\n",
              "            [0.1265],\n",
              "            [0.0840]]]),\n",
              "   tensor([[[0.1975],\n",
              "            [0.0401],\n",
              "            [0.0401],\n",
              "            [0.0803],\n",
              "            [0.2855],\n",
              "            [0.1577],\n",
              "            [0.1178],\n",
              "            [0.0810]]])]),\n",
              " (tensor([[15,  7, 22, 11, 15,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[9.3564e-04],\n",
              "            [9.8545e-01],\n",
              "            [9.4706e-03],\n",
              "            [1.1235e-03],\n",
              "            [2.5537e-03],\n",
              "            [2.7920e-04],\n",
              "            [7.7286e-05],\n",
              "            [1.0568e-04]]]),\n",
              "   tensor([[[0.0018],\n",
              "            [0.6614],\n",
              "            [0.2585],\n",
              "            [0.0166],\n",
              "            [0.0163],\n",
              "            [0.0159],\n",
              "            [0.0148],\n",
              "            [0.0148]]]),\n",
              "   tensor([[[0.0012],\n",
              "            [0.0623],\n",
              "            [0.8273],\n",
              "            [0.0397],\n",
              "            [0.0211],\n",
              "            [0.0187],\n",
              "            [0.0157],\n",
              "            [0.0139]]]),\n",
              "   tensor([[[1.5167e-04],\n",
              "            [2.4644e-02],\n",
              "            [3.4867e-01],\n",
              "            [4.7681e-01],\n",
              "            [1.0170e-01],\n",
              "            [2.1094e-02],\n",
              "            [1.3966e-02],\n",
              "            [1.2971e-02]]]),\n",
              "   tensor([[[8.3679e-05],\n",
              "            [1.5787e-02],\n",
              "            [4.6081e-03],\n",
              "            [9.4540e-02],\n",
              "            [7.4408e-01],\n",
              "            [8.9478e-02],\n",
              "            [2.8105e-02],\n",
              "            [2.3317e-02]]]),\n",
              "   tensor([[[0.0020],\n",
              "            [0.0211],\n",
              "            [0.0131],\n",
              "            [0.0061],\n",
              "            [0.1202],\n",
              "            [0.5715],\n",
              "            [0.2196],\n",
              "            [0.0464]]]),\n",
              "   tensor([[[0.0200],\n",
              "            [0.0295],\n",
              "            [0.0585],\n",
              "            [0.0200],\n",
              "            [0.0507],\n",
              "            [0.1586],\n",
              "            [0.3511],\n",
              "            [0.3115]]]),\n",
              "   tensor([[[0.0852],\n",
              "            [0.2914],\n",
              "            [0.0852],\n",
              "            [0.0852],\n",
              "            [0.0852],\n",
              "            [0.0852],\n",
              "            [0.1550],\n",
              "            [0.1275]]]),\n",
              "   tensor([[[0.0867],\n",
              "            [0.2689],\n",
              "            [0.0867],\n",
              "            [0.0867],\n",
              "            [0.0867],\n",
              "            [0.1016],\n",
              "            [0.1537],\n",
              "            [0.1288]]]),\n",
              "   tensor([[[0.1032],\n",
              "            [0.2130],\n",
              "            [0.1032],\n",
              "            [0.1032],\n",
              "            [0.1032],\n",
              "            [0.1041],\n",
              "            [0.1422],\n",
              "            [0.1279]]]),\n",
              "   tensor([[[0.1044],\n",
              "            [0.2251],\n",
              "            [0.1044],\n",
              "            [0.1044],\n",
              "            [0.1044],\n",
              "            [0.1055],\n",
              "            [0.1341],\n",
              "            [0.1177]]]),\n",
              "   tensor([[[0.1032],\n",
              "            [0.2291],\n",
              "            [0.1032],\n",
              "            [0.1032],\n",
              "            [0.1032],\n",
              "            [0.1092],\n",
              "            [0.1356],\n",
              "            [0.1134]]]),\n",
              "   tensor([[[0.0979],\n",
              "            [0.2416],\n",
              "            [0.0979],\n",
              "            [0.0979],\n",
              "            [0.0979],\n",
              "            [0.1144],\n",
              "            [0.1413],\n",
              "            [0.1109]]]),\n",
              "   tensor([[[0.0909],\n",
              "            [0.2607],\n",
              "            [0.0909],\n",
              "            [0.0909],\n",
              "            [0.0909],\n",
              "            [0.1218],\n",
              "            [0.1473],\n",
              "            [0.1065]]]),\n",
              "   tensor([[[0.0873],\n",
              "            [0.2682],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.1271],\n",
              "            [0.1512],\n",
              "            [0.1045]]]),\n",
              "   tensor([[[0.0872],\n",
              "            [0.2675],\n",
              "            [0.0872],\n",
              "            [0.0872],\n",
              "            [0.0872],\n",
              "            [0.1296],\n",
              "            [0.1511],\n",
              "            [0.1031]]]),\n",
              "   tensor([[[0.0873],\n",
              "            [0.2676],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.1314],\n",
              "            [0.1502],\n",
              "            [0.1017]]]),\n",
              "   tensor([[[0.0872],\n",
              "            [0.2678],\n",
              "            [0.0872],\n",
              "            [0.0872],\n",
              "            [0.0872],\n",
              "            [0.1329],\n",
              "            [0.1497],\n",
              "            [0.1007]]]),\n",
              "   tensor([[[0.0873],\n",
              "            [0.2677],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.0873],\n",
              "            [0.1338],\n",
              "            [0.1493],\n",
              "            [0.1001]]]),\n",
              "   tensor([[[0.0874],\n",
              "            [0.2674],\n",
              "            [0.0874],\n",
              "            [0.0874],\n",
              "            [0.0874],\n",
              "            [0.1343],\n",
              "            [0.1490],\n",
              "            [0.0998]]])]),\n",
              " (tensor([[19, 21, 20, 10,  7, 17,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[1.0000e+00],\n",
              "            [1.9844e-08],\n",
              "            [1.9844e-08],\n",
              "            [7.9881e-08],\n",
              "            [7.5681e-08],\n",
              "            [5.4930e-08],\n",
              "            [5.0838e-08],\n",
              "            [4.8987e-08]]]),\n",
              "   tensor([[[0.0967],\n",
              "            [0.8426],\n",
              "            [0.0135],\n",
              "            [0.0229],\n",
              "            [0.0074],\n",
              "            [0.0057],\n",
              "            [0.0056],\n",
              "            [0.0056]]]),\n",
              "   tensor([[[0.0066],\n",
              "            [0.0130],\n",
              "            [0.0072],\n",
              "            [0.9551],\n",
              "            [0.0104],\n",
              "            [0.0031],\n",
              "            [0.0025],\n",
              "            [0.0022]]]),\n",
              "   tensor([[[2.6323e-02],\n",
              "            [4.3397e-04],\n",
              "            [2.1073e-04],\n",
              "            [9.2556e-01],\n",
              "            [2.9057e-02],\n",
              "            [6.7987e-03],\n",
              "            [5.9154e-03],\n",
              "            [5.6976e-03]]]),\n",
              "   tensor([[[4.8580e-02],\n",
              "            [9.4301e-05],\n",
              "            [6.0205e-05],\n",
              "            [6.5009e-01],\n",
              "            [2.3513e-01],\n",
              "            [2.5248e-02],\n",
              "            [2.1344e-02],\n",
              "            [1.9452e-02]]]),\n",
              "   tensor([[[3.6916e-02],\n",
              "            [2.1567e-04],\n",
              "            [2.1567e-04],\n",
              "            [4.0289e-02],\n",
              "            [7.6479e-01],\n",
              "            [6.9678e-02],\n",
              "            [4.4777e-02],\n",
              "            [4.3115e-02]]]),\n",
              "   tensor([[[0.0247],\n",
              "            [0.0012],\n",
              "            [0.0014],\n",
              "            [0.0126],\n",
              "            [0.3336],\n",
              "            [0.4926],\n",
              "            [0.1073],\n",
              "            [0.0267]]]),\n",
              "   tensor([[[0.0107],\n",
              "            [0.0107],\n",
              "            [0.0107],\n",
              "            [0.0775],\n",
              "            [0.1255],\n",
              "            [0.2239],\n",
              "            [0.3040],\n",
              "            [0.2369]]]),\n",
              "   tensor([[[0.1260],\n",
              "            [0.0792],\n",
              "            [0.0792],\n",
              "            [0.0792],\n",
              "            [0.0792],\n",
              "            [0.1283],\n",
              "            [0.2385],\n",
              "            [0.1906]]]),\n",
              "   tensor([[[0.1478],\n",
              "            [0.0955],\n",
              "            [0.0955],\n",
              "            [0.0955],\n",
              "            [0.0955],\n",
              "            [0.1236],\n",
              "            [0.2035],\n",
              "            [0.1431]]]),\n",
              "   tensor([[[0.1178],\n",
              "            [0.1145],\n",
              "            [0.1145],\n",
              "            [0.1182],\n",
              "            [0.1145],\n",
              "            [0.1145],\n",
              "            [0.1657],\n",
              "            [0.1404]]]),\n",
              "   tensor([[[0.1169],\n",
              "            [0.1169],\n",
              "            [0.1169],\n",
              "            [0.1272],\n",
              "            [0.1169],\n",
              "            [0.1169],\n",
              "            [0.1565],\n",
              "            [0.1317]]]),\n",
              "   tensor([[[0.1182],\n",
              "            [0.1182],\n",
              "            [0.1182],\n",
              "            [0.1245],\n",
              "            [0.1182],\n",
              "            [0.1182],\n",
              "            [0.1578],\n",
              "            [0.1267]]]),\n",
              "   tensor([[[0.1200],\n",
              "            [0.1148],\n",
              "            [0.1038],\n",
              "            [0.1274],\n",
              "            [0.1038],\n",
              "            [0.1165],\n",
              "            [0.1818],\n",
              "            [0.1319]]]),\n",
              "   tensor([[[0.1322],\n",
              "            [0.1211],\n",
              "            [0.0844],\n",
              "            [0.1108],\n",
              "            [0.0844],\n",
              "            [0.1376],\n",
              "            [0.2007],\n",
              "            [0.1287]]]),\n",
              "   tensor([[[0.1345],\n",
              "            [0.1209],\n",
              "            [0.0812],\n",
              "            [0.1161],\n",
              "            [0.0812],\n",
              "            [0.1394],\n",
              "            [0.2004],\n",
              "            [0.1263]]]),\n",
              "   tensor([[[0.1405],\n",
              "            [0.1180],\n",
              "            [0.0810],\n",
              "            [0.1164],\n",
              "            [0.0810],\n",
              "            [0.1409],\n",
              "            [0.1987],\n",
              "            [0.1236]]]),\n",
              "   tensor([[[0.1407],\n",
              "            [0.1162],\n",
              "            [0.0784],\n",
              "            [0.1191],\n",
              "            [0.0802],\n",
              "            [0.1425],\n",
              "            [0.1989],\n",
              "            [0.1240]]]),\n",
              "   tensor([[[0.1400],\n",
              "            [0.1137],\n",
              "            [0.0789],\n",
              "            [0.1250],\n",
              "            [0.0805],\n",
              "            [0.1410],\n",
              "            [0.1978],\n",
              "            [0.1232]]]),\n",
              "   tensor([[[0.1407],\n",
              "            [0.1123],\n",
              "            [0.0785],\n",
              "            [0.1261],\n",
              "            [0.0808],\n",
              "            [0.1415],\n",
              "            [0.1974],\n",
              "            [0.1227]]])]),\n",
              " (tensor([[17, 18,  3, 22, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[1.0000e+00],\n",
              "            [2.4156e-07],\n",
              "            [1.8604e-08],\n",
              "            [7.1466e-08],\n",
              "            [1.8604e-08],\n",
              "            [1.8604e-08],\n",
              "            [1.8604e-08],\n",
              "            [1.8604e-08]]]),\n",
              "   tensor([[[0.0790],\n",
              "            [0.8000],\n",
              "            [0.0622],\n",
              "            [0.0168],\n",
              "            [0.0103],\n",
              "            [0.0103],\n",
              "            [0.0104],\n",
              "            [0.0111]]]),\n",
              "   tensor([[[0.0365],\n",
              "            [0.1803],\n",
              "            [0.5453],\n",
              "            [0.1752],\n",
              "            [0.0185],\n",
              "            [0.0162],\n",
              "            [0.0156],\n",
              "            [0.0123]]]),\n",
              "   tensor([[[0.0084],\n",
              "            [0.0046],\n",
              "            [0.0183],\n",
              "            [0.8732],\n",
              "            [0.0818],\n",
              "            [0.0060],\n",
              "            [0.0045],\n",
              "            [0.0031]]]),\n",
              "   tensor([[[0.0308],\n",
              "            [0.0009],\n",
              "            [0.0009],\n",
              "            [0.1076],\n",
              "            [0.6903],\n",
              "            [0.1100],\n",
              "            [0.0329],\n",
              "            [0.0267]]]),\n",
              "   tensor([[[8.2876e-03],\n",
              "            [2.1627e-03],\n",
              "            [6.0882e-04],\n",
              "            [3.6125e-03],\n",
              "            [5.5510e-03],\n",
              "            [2.4170e-01],\n",
              "            [7.0780e-01],\n",
              "            [3.0275e-02]]]),\n",
              "   tensor([[[0.0206],\n",
              "            [0.0242],\n",
              "            [0.0254],\n",
              "            [0.0215],\n",
              "            [0.0206],\n",
              "            [0.1976],\n",
              "            [0.6045],\n",
              "            [0.0855]]]),\n",
              "   tensor([[[0.0703],\n",
              "            [0.0703],\n",
              "            [0.1021],\n",
              "            [0.0703],\n",
              "            [0.0703],\n",
              "            [0.1442],\n",
              "            [0.4023],\n",
              "            [0.0703]]]),\n",
              "   tensor([[[0.1025],\n",
              "            [0.0851],\n",
              "            [0.0851],\n",
              "            [0.0851],\n",
              "            [0.0851],\n",
              "            [0.1851],\n",
              "            [0.2760],\n",
              "            [0.0959]]]),\n",
              "   tensor([[[0.1029],\n",
              "            [0.1029],\n",
              "            [0.1029],\n",
              "            [0.1029],\n",
              "            [0.1029],\n",
              "            [0.1559],\n",
              "            [0.2268],\n",
              "            [0.1029]]]),\n",
              "   tensor([[[0.0960],\n",
              "            [0.0941],\n",
              "            [0.0941],\n",
              "            [0.0941],\n",
              "            [0.0941],\n",
              "            [0.1909],\n",
              "            [0.2428],\n",
              "            [0.0941]]]),\n",
              "   tensor([[[0.1005],\n",
              "            [0.1005],\n",
              "            [0.1005],\n",
              "            [0.1005],\n",
              "            [0.1005],\n",
              "            [0.1706],\n",
              "            [0.2264],\n",
              "            [0.1005]]]),\n",
              "   tensor([[[0.0950],\n",
              "            [0.0881],\n",
              "            [0.0881],\n",
              "            [0.0881],\n",
              "            [0.0881],\n",
              "            [0.2072],\n",
              "            [0.2567],\n",
              "            [0.0886]]]),\n",
              "   tensor([[[0.0829],\n",
              "            [0.0660],\n",
              "            [0.0660],\n",
              "            [0.0660],\n",
              "            [0.0660],\n",
              "            [0.2574],\n",
              "            [0.3131],\n",
              "            [0.0824]]]),\n",
              "   tensor([[[0.0728],\n",
              "            [0.0546],\n",
              "            [0.0546],\n",
              "            [0.0546],\n",
              "            [0.0546],\n",
              "            [0.2915],\n",
              "            [0.3413],\n",
              "            [0.0759]]]),\n",
              "   tensor([[[0.0670],\n",
              "            [0.0514],\n",
              "            [0.0514],\n",
              "            [0.0514],\n",
              "            [0.0514],\n",
              "            [0.3016],\n",
              "            [0.3518],\n",
              "            [0.0738]]]),\n",
              "   tensor([[[0.0646],\n",
              "            [0.0502],\n",
              "            [0.0505],\n",
              "            [0.0502],\n",
              "            [0.0502],\n",
              "            [0.3054],\n",
              "            [0.3559],\n",
              "            [0.0730]]]),\n",
              "   tensor([[[0.0634],\n",
              "            [0.0497],\n",
              "            [0.0504],\n",
              "            [0.0497],\n",
              "            [0.0497],\n",
              "            [0.3065],\n",
              "            [0.3578],\n",
              "            [0.0727]]]),\n",
              "   tensor([[[0.0629],\n",
              "            [0.0496],\n",
              "            [0.0501],\n",
              "            [0.0496],\n",
              "            [0.0496],\n",
              "            [0.3071],\n",
              "            [0.3584],\n",
              "            [0.0726]]]),\n",
              "   tensor([[[0.0627],\n",
              "            [0.0496],\n",
              "            [0.0499],\n",
              "            [0.0496],\n",
              "            [0.0496],\n",
              "            [0.3076],\n",
              "            [0.3585],\n",
              "            [0.0724]]])]),\n",
              " (tensor([[ 5, 10,  3, 20,  5, 10,  3,  5, 10,  3, 20, 20, 11,  1,  0,  0,  0,  0,\n",
              "            0,  0]]),\n",
              "  [tensor([[[9.9999e-01],\n",
              "            [5.3100e-07],\n",
              "            [4.6550e-06],\n",
              "            [1.6552e-06],\n",
              "            [1.5394e-08],\n",
              "            [1.0735e-06],\n",
              "            [1.5127e-07],\n",
              "            [1.0556e-07]]]),\n",
              "   tensor([[[0.9798],\n",
              "            [0.0064],\n",
              "            [0.0029],\n",
              "            [0.0023],\n",
              "            [0.0021],\n",
              "            [0.0023],\n",
              "            [0.0022],\n",
              "            [0.0021]]]),\n",
              "   tensor([[[0.1736],\n",
              "            [0.4872],\n",
              "            [0.0858],\n",
              "            [0.0553],\n",
              "            [0.0473],\n",
              "            [0.0543],\n",
              "            [0.0501],\n",
              "            [0.0465]]]),\n",
              "   tensor([[[0.0165],\n",
              "            [0.0795],\n",
              "            [0.8287],\n",
              "            [0.0267],\n",
              "            [0.0090],\n",
              "            [0.0209],\n",
              "            [0.0097],\n",
              "            [0.0090]]]),\n",
              "   tensor([[[2.6432e-02],\n",
              "            [7.0139e-04],\n",
              "            [8.0377e-01],\n",
              "            [1.3421e-01],\n",
              "            [6.1274e-03],\n",
              "            [1.7018e-02],\n",
              "            [6.7449e-03],\n",
              "            [5.0037e-03]]]),\n",
              "   tensor([[[1.0843e-02],\n",
              "            [1.8477e-04],\n",
              "            [9.2754e-01],\n",
              "            [3.5261e-02],\n",
              "            [4.5710e-03],\n",
              "            [9.9225e-03],\n",
              "            [6.9534e-03],\n",
              "            [4.7259e-03]]]),\n",
              "   tensor([[[4.4414e-02],\n",
              "            [5.4909e-04],\n",
              "            [6.0201e-01],\n",
              "            [1.5682e-01],\n",
              "            [4.1565e-02],\n",
              "            [6.0102e-02],\n",
              "            [5.2988e-02],\n",
              "            [4.1548e-02]]]),\n",
              "   tensor([[[1.1544e-02],\n",
              "            [3.8373e-04],\n",
              "            [2.2328e-02],\n",
              "            [8.6230e-01],\n",
              "            [1.8231e-02],\n",
              "            [4.9401e-02],\n",
              "            [2.1549e-02],\n",
              "            [1.4268e-02]]]),\n",
              "   tensor([[[1.9448e-02],\n",
              "            [2.1951e-04],\n",
              "            [2.3346e-02],\n",
              "            [7.7294e-01],\n",
              "            [6.1728e-02],\n",
              "            [7.0859e-02],\n",
              "            [2.9479e-02],\n",
              "            [2.1980e-02]]]),\n",
              "   tensor([[[0.0197],\n",
              "            [0.0005],\n",
              "            [0.0096],\n",
              "            [0.4081],\n",
              "            [0.2347],\n",
              "            [0.1720],\n",
              "            [0.0832],\n",
              "            [0.0723]]]),\n",
              "   tensor([[[0.0038],\n",
              "            [0.0009],\n",
              "            [0.0051],\n",
              "            [0.0854],\n",
              "            [0.0768],\n",
              "            [0.5951],\n",
              "            [0.1487],\n",
              "            [0.0843]]]),\n",
              "   tensor([[[2.9961e-03],\n",
              "            [5.5622e-04],\n",
              "            [2.6778e-03],\n",
              "            [1.4101e-02],\n",
              "            [9.0633e-03],\n",
              "            [6.1439e-01],\n",
              "            [2.9526e-01],\n",
              "            [6.0957e-02]]]),\n",
              "   tensor([[[0.0061],\n",
              "            [0.0013],\n",
              "            [0.0044],\n",
              "            [0.0146],\n",
              "            [0.0137],\n",
              "            [0.2508],\n",
              "            [0.4734],\n",
              "            [0.2357]]]),\n",
              "   tensor([[[0.0385],\n",
              "            [0.0108],\n",
              "            [0.0313],\n",
              "            [0.0851],\n",
              "            [0.0422],\n",
              "            [0.1759],\n",
              "            [0.3646],\n",
              "            [0.2517]]]),\n",
              "   tensor([[[0.0552],\n",
              "            [0.0108],\n",
              "            [0.0108],\n",
              "            [0.0528],\n",
              "            [0.0279],\n",
              "            [0.1586],\n",
              "            [0.3624],\n",
              "            [0.3215]]]),\n",
              "   tensor([[[0.0943],\n",
              "            [0.0195],\n",
              "            [0.0195],\n",
              "            [0.0308],\n",
              "            [0.0507],\n",
              "            [0.1926],\n",
              "            [0.3080],\n",
              "            [0.2847]]]),\n",
              "   tensor([[[0.0811],\n",
              "            [0.0226],\n",
              "            [0.0226],\n",
              "            [0.0328],\n",
              "            [0.0364],\n",
              "            [0.1929],\n",
              "            [0.3357],\n",
              "            [0.2759]]]),\n",
              "   tensor([[[0.0700],\n",
              "            [0.0235],\n",
              "            [0.0235],\n",
              "            [0.0357],\n",
              "            [0.0235],\n",
              "            [0.2022],\n",
              "            [0.3803],\n",
              "            [0.2412]]]),\n",
              "   tensor([[[0.0736],\n",
              "            [0.0255],\n",
              "            [0.0255],\n",
              "            [0.0407],\n",
              "            [0.0255],\n",
              "            [0.1999],\n",
              "            [0.4075],\n",
              "            [0.2019]]]),\n",
              "   tensor([[[0.0768],\n",
              "            [0.0264],\n",
              "            [0.0264],\n",
              "            [0.0432],\n",
              "            [0.0264],\n",
              "            [0.1895],\n",
              "            [0.4140],\n",
              "            [0.1973]]])])]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WHysSqYJ1ZUA"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'seaborn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdktVnMv1ZTh"
      },
      "outputs": [],
      "source": [
        "prediction, attention_scores = zip(*output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "BF6HD99lYlgQ",
        "outputId": "caaa0716-5b99-43e8-950f-dd0127d0fbb7"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(attn_viz, linewidth=0.5)\n",
        "ax.set_yticklabels(output_text,rotation=30)\n",
        "ax.set_xticklabels(xlabels,rotation=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1UkIsCztaMS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
