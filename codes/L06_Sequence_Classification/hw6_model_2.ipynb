{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ8FRFIYMc5X"
      },
      "source": [
        "# HOMEWORK 6: TEXT CLASSIFICATION\n",
        "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
        "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
        "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n",
        "\n",
        "We will focus only on the Object Classification task for this homework.\n",
        "\n",
        "In this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n",
        "\n",
        "You will need to build 3 different models.\n",
        "\n",
        "1. A model based on tf-idf\n",
        "2. A model based on MUSE\n",
        "3. A model based on wangchanBERTa\n",
        "\n",
        "**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n",
        "\n",
        "This homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRlx5Mb5zkXw",
        "outputId": "18d913e0-aa6d-435b-931d-591386cb4ba8"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\n",
        "# !pip install pythainlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YprqbOPMc5a"
      },
      "source": [
        "## Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "heICP79cMc5e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from IPython.display import display\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#My import \n",
        "np.random.seed(42)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPaUf4PLMc5k"
      },
      "source": [
        "## Loading cleaned dataset from my folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('template_cleaned_dataset.pkl', 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "# Extract tokenized text and labels\n",
        "label_2_num_map, num_2_label_map = dataset[\"label_2_num_map\"], dataset[\"num_2_label_map\"]\n",
        "train_texts, train_labels = dataset[\"train\"][\"input\"], dataset[\"train\"][\"label\"]\n",
        "val_texts, val_labels = dataset[\"val\"][\"input\"], dataset[\"val\"][\"label\"]\n",
        "test_texts, test_labels = dataset[\"test\"][\"input\"], dataset[\"test\"][\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wql2YeU8qFQ6"
      },
      "source": [
        "# Model 2 MUSE\n",
        "\n",
        "Build a simple logistic regression model using features from the MUSE model.\n",
        "\n",
        "Which MUSE model will you use? Why?\n",
        "\n",
        "**Ans:** \n",
        "\n",
        "- I use sentence-transformers/use-cmlm-multilingual. as there are more likes and the other one doesn't have native support for hugging face.\n",
        "\n",
        "MUSE is typically used with tensorflow. However, there are some pytorch conversions made by some people.\n",
        "\n",
        "- https://huggingface.co/sentence-transformers/use-cmlm-multilingual\n",
        "- https://huggingface.co/dayyass/universal-sentence-encoder-multilingual-large-3-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libs for MUSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d3UtkpaLnctH"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'type' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from pythainlp.tokenize import word_tokenize\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thai_stopwords\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mset\u001b[39m(thai_stopwords()))\n",
            "File \u001b[0;32m~/anaconda3/envs/colab-env/lib/python3.8/site-packages/pythainlp/__init__.py:41\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# All Thai characters that are presented in Unicode\u001b[39;00m\n\u001b[1;32m     36\u001b[0m thai_characters \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     37\u001b[0m     [thai_letters, thai_punctuations, thai_digits, thai_symbols]\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msoundex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m soundex\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspell\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m correct, spell\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n",
            "File \u001b[0;32m~/anaconda3/envs/colab-env/lib/python3.8/site-packages/pythainlp/soundex/__init__.py:18\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThai soundex\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mHas three systems to choose from: Udom83 (default), LK82, and MetaSound\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoundex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlk82\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprayut_and_somchaip\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msoundex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlk82\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lk82\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msoundex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetasound\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metasound\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msoundex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mudom83\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m udom83\n",
            "File \u001b[0;32m~/anaconda3/envs/colab-env/lib/python3.8/site-packages/pythainlp/soundex/lk82.py:19\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThai soundex - LK82 system\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mhttps://gist.github.com/korakot/0b772e09340cac2f493868da035597e8\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_tonemark\n\u001b[1;32m     21\u001b[0m _TRANS1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mกขฃคฅฆงจฉชฌซศษสญยฎดฏตณนฐฑฒถทธบปผพภฝฟมรลฬฤฦวหฮอ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mกกกกกกงจชชชซซซซยยดดตตนนททททททบปพพพฟฟมรรรรรวหหอ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m _TRANS2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mกขฃคฅฆงจฉชซฌฎฏฐฑฒดตถทธศษสญณนรลฬฤฦบปพฟภผฝมำยวไใหฮาๅึืเแโุูอ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1111112333333333333333333444444445555555667777889AAABCDEEF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/colab-env/lib/python3.8/site-packages/pythainlp/util/__init__.py:88\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeyboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     83\u001b[0m     eng_to_thai,\n\u001b[1;32m     84\u001b[0m     thai_keyboard_dist,\n\u001b[1;32m     85\u001b[0m     thai_to_eng,\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01memojiconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m emoji_to_thai\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeywords\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_keyword, rank\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     90\u001b[0m     expand_maiyamok,\n\u001b[1;32m     91\u001b[0m     maiyamok,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     reorder_vowels,\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremove_trailing_repeat_consonants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    101\u001b[0m     remove_trailing_repeat_consonants,\n\u001b[1;32m    102\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/colab-env/lib/python3.8/site-packages/pythainlp/util/keywords.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thai_stopwords\n\u001b[1;32m      9\u001b[0m _STOPWORDS \u001b[38;5;241m=\u001b[39m thai_stopwords()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrank\u001b[39m(words: List[\u001b[38;5;28mstr\u001b[39m], exclude_stopwords: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Counter:\n",
            "File \u001b[0;32m~/anaconda3/envs/colab-env/lib/python3.8/site-packages/pythainlp/corpus/__init__.py:103\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _CORPUS_DB_PATH\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     92\u001b[0m     download,\n\u001b[1;32m     93\u001b[0m     get_corpus,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     remove,\n\u001b[1;32m    102\u001b[0m )  \u001b[38;5;66;03m# these imports must come before other pythainlp.corpus.* imports\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    104\u001b[0m     countries,\n\u001b[1;32m    105\u001b[0m     find_synonyms,\n\u001b[1;32m    106\u001b[0m     provinces,\n\u001b[1;32m    107\u001b[0m     thai_dict,\n\u001b[1;32m    108\u001b[0m     thai_family_names,\n\u001b[1;32m    109\u001b[0m     thai_female_names,\n\u001b[1;32m    110\u001b[0m     thai_male_names,\n\u001b[1;32m    111\u001b[0m     thai_negations,\n\u001b[1;32m    112\u001b[0m     thai_orst_words,\n\u001b[1;32m    113\u001b[0m     thai_stopwords,\n\u001b[1;32m    114\u001b[0m     thai_syllables,\n\u001b[1;32m    115\u001b[0m     thai_synonym,\n\u001b[1;32m    116\u001b[0m     thai_synonyms,\n\u001b[1;32m    117\u001b[0m     thai_words,\n\u001b[1;32m    118\u001b[0m     thai_wsd_dict,\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01micu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thai_icu_words\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolubilis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thai_volubilis_words\n",
            "File \u001b[0;32m~/anaconda3/envs/colab-env/lib/python3.8/site-packages/pythainlp/corpus/common.py:59\u001b[0m\n\u001b[1;32m     55\u001b[0m _THAI_MALE_NAMES_FILENAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_names_male_th.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m _THAI_ORST_WORDS: FrozenSet[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m()\n\u001b[0;32m---> 59\u001b[0m _THAI_DICT: \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     60\u001b[0m _THAI_WSD_DICT: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     61\u001b[0m _THAI_SYNONYMS: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
            "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(set(thai_stopwords()))\n",
        "\n",
        "MODEL_NAME = 'sentence-transformers/use-cmlm-multilingual'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaf/anaconda3/envs/colab-env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (10710, 3594)\n",
            "Val shape: (1339, 3594)\n",
            "Test shape: (1340, 3594)\n",
            "Number of features: 3594\n",
            "Sample TF-IDF Features (First 5 training samples):\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 8 stored elements and shape (1, 3594)>\n",
            "  Coords\tValues\n",
            "  (0, 2856)\t0.2885629413746919\n",
            "  (0, 2174)\t0.3644740882130535\n",
            "  (0, 127)\t0.45100667026141533\n",
            "  (0, 1677)\t0.28148034536449795\n",
            "  (0, 3122)\t0.28025673780847243\n",
            "  (0, 3522)\t0.16197780948030555\n",
            "  (0, 1455)\t0.5452004308679377\n",
            "  (0, 2514)\t0.31500429643097466\n"
          ]
        }
      ],
      "source": [
        "pythainlp_tokenizer = lambda x: word_tokenize(x, engine=\"newmm\", keep_whitespace=False)\n",
        "\n",
        "# Extract tokenized text and labels\n",
        "train_texts, train_labels = dataset[\"train\"][\"input\"], dataset[\"train\"][\"label\"]\n",
        "val_texts, val_labels = dataset[\"val\"][\"input\"], dataset[\"val\"][\"label\"]\n",
        "test_texts, test_labels = dataset[\"test\"][\"input\"], dataset[\"test\"][\"label\"]\n",
        "\n",
        "# map texts to lowercase\n",
        "train_texts = [text.lower() for text in train_texts]\n",
        "val_texts = [text.lower() for text in val_texts]\n",
        "test_texts = [text.lower() for text in test_texts]\n",
        "\n",
        "# Initialize and fit TF-IDF Vectorizer on training data\n",
        "vectorizer = TfidfVectorizer(tokenizer=pythainlp_tokenizer, max_df=0.7, min_df=1)\n",
        "X_train_tfidf = vectorizer.fit_transform(train_texts)  # Fit and transform training data\n",
        "\n",
        "X_val_tfidf = vectorizer.transform(val_texts)  # Transform validation data\n",
        "X_test_tfidf = vectorizer.transform(test_texts)  # Transform test data\n",
        "\n",
        "# Print shape of TF-IDF matrices\n",
        "print(\"Train shape:\", X_train_tfidf.shape)\n",
        "print(\"Val shape:\", X_val_tfidf.shape)\n",
        "print(\"Test shape:\", X_test_tfidf.shape)\n",
        "\n",
        "# vectorizer get feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"Number of features:\", len(feature_names))\n",
        "\n",
        "# Print first 5 samples of processed train data\n",
        "print(\"Sample TF-IDF Features (First 5 training samples):\")\n",
        "print(X_train_tfidf[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(MODEL_NAME)\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDHfX377rnp_"
      },
      "source": [
        "# Model 3 WangchanBERTa\n",
        "\n",
        "We ask you to train a WangchanBERTa-based model.\n",
        "\n",
        "We recommend you use the thaixtransformers fork (which we used in the PoS homework).\n",
        "https://github.com/PyThaiNLP/thaixtransformers\n",
        "\n",
        "The structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n",
        "\n",
        "Which WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n",
        "\n",
        "**Ans:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI8SvILyub0m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D7qsVL0BaXS"
      },
      "source": [
        "After you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr9_0DnMBcFZ"
      },
      "source": [
        "# Comparison\n",
        "\n",
        "After you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "colab-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
