{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU5fRQwhEdJy"
   },
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "In this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n",
    "\n",
    "## Ref:\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI9gRZlUE80g"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1pOsV-jaW975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-19 15:01:58--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n",
      "--2025-01-19 15:01:59--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3231076 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘pra-apai-manee-ch1-50.txt.4’\n",
      "\n",
      "pra-apai-manee-ch1- 100%[===================>]   3.08M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2025-01-19 15:01:59 (40.7 MB/s) - ‘pra-apai-manee-ch1-50.txt.4’ saved [3231076/3231076]\n",
      "\n",
      "--2025-01-19 15:02:00--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl [following]\n",
      "--2025-01-19 15:02:01--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2968483 (2.8M) [text/plain]\n",
      "Saving to: ‘kratoo-40000000-40002000.jsonl.4’\n",
      "\n",
      "kratoo-40000000-400 100%[===================>]   2.83M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-01-19 15:02:01 (20.6 MB/s) - ‘kratoo-40000000-40002000.jsonl.4’ saved [2968483/2968483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSiDpG9WE-cT"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OQd7M6gLWPLN"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OifbmMIstzs8"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-FnIDvb1lMuh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pantip_text = []\n",
    "with open('kratoo-40000000-40002000.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\n",
    "sum([len(t) for t in pantip_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yaaQVXZ8A0j1"
   },
   "outputs": [],
   "source": [
    "with open(\"pra-apai-manee-ch1-50.txt\") as f:\n",
    "  pra_apai_manee_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LksJKc9MA5F_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100605"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(t) for t in pra_apai_manee_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RbdfkF-vAoie"
   },
   "outputs": [],
   "source": [
    "pantip_train_text = pantip_text[:int(len(pantip_text)*0.8)]\n",
    "pantip_test_text = pantip_text[int(len(pantip_text)*0.8):]\n",
    "\n",
    "pam_train_text = pra_apai_manee_data[:int(len(pra_apai_manee_data)*0.8)] #pam = pra_apai_manee\n",
    "pam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhwcH0Aot1XI"
   },
   "source": [
    "## Run tokenizer training\n",
    "\n",
    "The Python wrapper provides multiple APIs for training our tokenizers\n",
    "\n",
    "1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n",
    "  <br><br>\n",
    "2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n",
    "<br><br>\n",
    "3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n",
    "<br> Same as no.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3XeFFYw-T_0"
   },
   "source": [
    "### Unigram tokenizer\n",
    "\n",
    "We are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bFCfHphd15g9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files written successfully:\n",
      "Pantip Train File: pantip_train.txt\n",
      "Pantip Test File: pantip_test.txt\n",
      "PAM Train File: pam_train.txt\n",
      "PAM Test File: pam_test.txt\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "\n",
    "# Define file paths for output\n",
    "pantip_train_file = \"pantip_train.txt\"\n",
    "pantip_test_file = \"pantip_test.txt\"\n",
    "pam_train_file = \"pam_train.txt\"\n",
    "pam_test_file = \"pam_test.txt\"\n",
    "\n",
    "# Save the datasets to .txt files\n",
    "with open(pantip_train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(pantip_train_text)\n",
    "\n",
    "with open(pantip_test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(pantip_test_text)\n",
    "\n",
    "with open(pam_train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(pam_train_text)\n",
    "\n",
    "with open(pam_test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(pam_test_text)\n",
    "\n",
    "print(\"Files written successfully:\")\n",
    "print(f\"Pantip Train File: {pantip_train_file}\")\n",
    "print(f\"Pantip Test File: {pantip_test_file}\")\n",
    "print(f\"PAM Train File: {pam_train_file}\")\n",
    "print(f\"PAM Test File: {pam_test_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pam_train.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram_pam\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pam_train.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16314 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=885139\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16314 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=411778\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 275348 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16314\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 32181\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 32181 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=49394 obj=42.2355 num_tokens=128673 num_tokens/piece=2.60503\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40979 obj=36.7371 num_tokens=130046 num_tokens/piece=3.17348\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30367 obj=36.9771 num_tokens=139359 num_tokens/piece=4.58916\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29880 obj=36.5016 num_tokens=139582 num_tokens/piece=4.67142\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22205 obj=37.6771 num_tokens=149629 num_tokens/piece=6.73853\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22104 obj=37.2684 num_tokens=149717 num_tokens/piece=6.7733\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16512 obj=38.7389 num_tokens=161069 num_tokens/piece=9.75466\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16473 obj=38.3341 num_tokens=161183 num_tokens/piece=9.78468\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12338 obj=40.0409 num_tokens=172981 num_tokens/piece=14.0202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12331 obj=39.6638 num_tokens=172997 num_tokens/piece=14.0294\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9245 obj=41.4482 num_tokens=185092 num_tokens/piece=20.0208\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9242 obj=41.1146 num_tokens=185096 num_tokens/piece=20.0277\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6930 obj=43.0791 num_tokens=198272 num_tokens/piece=28.6107\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6930 obj=42.7435 num_tokens=198274 num_tokens/piece=28.611\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5196 obj=44.8072 num_tokens=211985 num_tokens/piece=40.7977\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5196 obj=44.4723 num_tokens=211972 num_tokens/piece=40.7952\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3897 obj=46.7216 num_tokens=226813 num_tokens/piece=58.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3897 obj=46.3429 num_tokens=226820 num_tokens/piece=58.2037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2922 obj=48.8639 num_tokens=244481 num_tokens/piece=83.6691\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2922 obj=48.4277 num_tokens=244676 num_tokens/piece=83.7358\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2191 obj=51.2638 num_tokens=265878 num_tokens/piece=121.35\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2191 obj=50.7382 num_tokens=265883 num_tokens/piece=121.352\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1643 obj=53.8622 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1643 obj=53.2395 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1232 obj=56.564 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1232 obj=55.8787 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=57.1231 num_tokens=332377 num_tokens/piece=302.161\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=56.8968 num_tokens=332377 num_tokens/piece=302.161\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: unigram_pam.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: unigram_pam.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=pam_train_file, model_prefix=\"unigram_pam\", vocab_size=1000, model_type=\"unigram\")\n",
    "\n",
    "sp_pam = spm.SentencePieceProcessor(model_file='unigram_pam.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdXPaoW3_v2T"
   },
   "source": [
    "### Q1 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J1bO3s-z-PLb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKkc1D-hAFxl"
   },
   "source": [
    "### BPE Tokenizer\n",
    "\n",
    "Now try training a BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AiXj57rh-PIv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pam_train.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe_pam\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pam_train.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16314 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=885139\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16314 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16314\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 32181\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8839 min_freq=120\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3834 size=20 all=3652 active=2394 piece=ให\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2412 size=40 all=5419 active=4161 piece=้อง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1781 size=60 all=7328 active=6070 piece=ได\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1469 size=80 all=9357 active=8099 piece=▁พระ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1191 size=100 all=11534 active=10276 piece=าว\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1168 min_freq=139\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1002 size=120 all=13824 active=3191 piece=ูก\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=876 size=140 all=16549 active=5916 piece=เม\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=748 size=160 all=19242 active=8609 piece=ความ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=677 size=180 all=21716 active=11083 piece=กํา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=626 size=200 all=24479 active=13846 piece=สาร\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=623 min_freq=85\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=566 size=220 all=26945 active=3535 piece=ขึ้น\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=509 size=240 all=29932 active=6522 piece=รบ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=470 size=260 all=32718 active=9308 piece=แล\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=436 size=280 all=35246 active=11836 piece=้ํา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=404 size=300 all=37519 active=14109 piece=ฝ้า\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=404 min_freq=48\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=382 size=320 all=39880 active=4139 piece=น้อย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=356 size=340 all=42278 active=6537 piece=นึก\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=336 size=360 all=44997 active=9256 piece=แท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=320 size=380 all=47070 active=11329 piece=ขัด\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301 size=400 all=49390 active=13649 piece=ตรา\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=300 min_freq=33\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=285 size=420 all=51579 active=4541 piece=พู\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=269 size=440 all=53523 active=6485 piece=เลี้ยง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=257 size=460 all=55855 active=8817 piece=่าว\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=244 size=480 all=57964 active=10926 piece=▁ก็\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=234 size=500 all=60309 active=13271 piece=ียก\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=234 min_freq=25\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=520 all=62342 active=4958 piece=าะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=205 size=540 all=64337 active=6953 piece=รุ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=199 size=560 all=66395 active=9011 piece=วาท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=580 all=68294 active=10910 piece=อ่อน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=600 all=70205 active=12821 piece=ชน\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=186 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=620 all=71920 active=5149 piece=▁ซึ่ง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=640 all=73659 active=6888 piece=พักตร์\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=660 all=75411 active=8640 piece=▁ขึ้น\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=159 size=680 all=77310 active=10539 piece=พรั่ง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=700 all=79085 active=12314 piece=อื้น\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=155 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=720 all=80846 active=5691 piece=สัก\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=740 all=82546 active=7391 piece=เชษฐา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=140 size=760 all=84297 active=9142 piece=▁กระ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=780 all=85826 active=10671 piece=กําปั่น\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=800 all=87264 active=12109 piece=▁ศรีสุวรรณ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=131 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=820 all=88742 active=5808 piece=สบาย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=840 all=90098 active=7164 piece=ไหล\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=860 all=91598 active=8664 piece=ชาติ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=880 all=93143 active=10209 piece=ไท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=900 all=94526 active=11592 piece=คํานับ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=113 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=920 all=95958 active=6087 piece=แป\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: bpe_pam.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe_pam.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=pam_train_file, model_prefix=\"bpe_pam\", vocab_size=1000, model_type=\"bpe\")\n",
    "\n",
    "bpe_pam = spm.SentencePieceProcessor(model_file='bpe_pam.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrQwGmL5AMXc"
   },
   "source": [
    "### Q2 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0AXuzyaN-PEr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbb6C6-IS_Ly"
   },
   "source": [
    "These are some of your vocabs. Note that you will see \"▁\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Aa9j6XrTKjyA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ▁ | า | เ | น | ม | ย | ก | ร | ว | ด | ส | ง | บ | ค | มา | อ | ล | จะ | ท | ให้ | ห | ไป | ไม่ | แ | ว่า | พ | ุ | ี | ๏ | ฯ | ข | ช | เป็น | พระ | โ | ที่ | ใจ | ▁จะ | จ | ะ | ิ | ต | ก็ | อยู่ | ป | ได้ | ่ | ไ | เข้า | ู | ▁พระ | ้า | ตาม | ใน | ้ | ▁แล้ว | เหมือน | รา | ศ | เจ้า | เห็น | ลา | กัน | ั | หา | นาง | ทรง | ประ | ์ | ยา | ัก | ํา | ซ | าน | ัง | ฉ | องค์ | ัด | แล้ว | อน | ดู | ถ | ด้วย | มี | ▁จึง | นี้ | ่า | ผ | น้อง | แต่ | ทํา | ▁นาง | ▁ให้ | รัก | พี่ | คิด | ลูก | พา | รู้ | การ | กับ | ัน | หน้า | กระ | วน | ออก | ่อ | เขา | ถึง | ระ | ข้า | ับ | พล | นั่ง | ทั้ง | หน | รับ | ษ | กล | วง | ลง | ฝ | กร | พร | ความ | เสีย | ดี | ขึ้น | อง | ่ง | ธ | ▁แต่ | คน | กลับ | ▁ฝ่าย | ้น | อด | ภ | หรือ | ตร | ือ | ฟัง | แม่ | ▁ไม่ | ไว้ | ยัง | ▁เห็น | นา | ขอ | มิ | น้ํา | หล | ดัง | ▁พอ | ▁ทั้ง | ช่วย | สม | นั้น | ริ | ทัพ | ต้อง | วัน | อา | น้อย | รบ | ิน | อย่า | เอา | จน | เรา | สุด | เสียง | ข้าง | หลัง | ตี | ตัว | ละ | สุ | วัง | ทุก | ่น | ึก | นึก | เฝ้า | นาย | ฝรั่ง | ทูล | เส | วิ | ปล | ▁ถึง | ตาย | ใคร | อก | อั | ตา | เรือ | จึง | แล | ี่ | ั่ง | แสน | สอง | ของ | ็ | ลี | ี้ | จิต | หมาย | ้ม | แจ้ง | ั่น | สั่ง | ราช | พิ | เห | หาย | ้อง | เมือง | เหลือ | กลาง | กษัตริย์ | ยิ่ง | ตรัส | ึง | เลย | เล่า | ทาง | ุด | ศรี | เคย | ไหน | สาม | หนี | ณ | มัน | ื้อ | ค่อย | ชาย | พราหมณ์ | ▁อย่า | ญ | ที | นิ | น่า | สิ้น | ฉัน | กาย | ลังกา | ▁ด้วย | คอย | บอก | สิ | ฟ | สงสาร | พ่อ | ยง | จริง | ชาว | ถาม | ไร | ทหาร | ตั้ง | ▁อัน | เที่ยว | ปร | ผู้ | พวก | สาร | ชม | ศึก | คํา | ▁เป็น | ทอง | อบ | ใหญ่ | ถือ | สาว | พระอภัย | จง | สา | จับ | ั้น | พลาง | ▁มา | ยก | ▁บ้าง | ไพร่ | ลม | ล้วน | ▁ต่าง | ร้อย | พบ | งาม | แกล้ง | อาย | จะได้ | เคียง | อย่าง | เครื่อง | กลัว | ลาย | จํา | ต่าง | สินสมุทร | ▁พวก | ม้า | ลํา | นี่ | ผา | แก้ว | เพราะ | ▁ครั้น | ▁จน | ▁แม้น | สาย | พัน | พระองค์ | พร้อม | วาย | ชิง | ห้อง | ร้อง | สู้ | ▁จง | ลิ | ราย | ล่อ | จาก | ้ว | ท่าน | รอง | เดิน | เรียก | ขัด | เหล่า | กุมาร | ผล | ป่า | ู่ | คู่ | รูป | กิน | พอ | ร่ํา | โฉม | ▁ถ้า | คง | ่าย | ใช้ | ตอบ | หลง | ไล่ | จัด | ดับ | ▁เมื่อ | บน | อ่อน | แสง | คืน | ใส่ | แค้น | รถ | ตรง | แต่ง | แน่ | เชิญ | ชื่น | ถวาย | โห | จร | มิได้ | นอน | ุก | ชวน | เมีย | อาลัย | ้อม | ลับ | ไหว | ▁แม้ | บิดา | หญิง | หลับ | ดอก | กล้า | ขาด | จัก | ไม่มี | บาท | เสนา | ย์ | ช่าง | โศก | วาง | ติด | เสร็จ | ร้อน | คุณ | ผัว | นัก | ความตาม | พักตร์ | หน่อ | ้อย | ▁ซึ่ง | ตะ | ห้าม | พราย | ฟ้า | ไฉน | ใ | ตก | เมื่อ | ยศ | ชล | ดํา | หนึ่ง | ผัน | ใด | สัก | ร้าย | วิ่ง | แก้ | ยาม | ศรีสุวรรณ | ปืน | ฆ่า | ขับ | ขวา | ไฟ | พูด | หมอง | ก็ไม่ | กําลัง | รักษา | เช่น | ุ่ม | ผี | หาญ | เล่น | เนื้อ | รีบ | ถูก | ชัย | บุตรี | ฟัน | บ้าง | เอ๋ย | สงสัย | ผิด | นิ่ง | ชื่อ | เถิด | ผ่อน | หลาน | สี่ | ชาติ | อี | ปาก | ช้า | ึ | แตก | ตรา | รณ | ลอง | ปี | หมอ | เจ้าพราหมณ์ | พี่เลี้ยง | ต่อ | พลอย | โฉมยง | เนตร | หัก | กอด | เชย | ทั้งสอง | ยิ้ม | ค่ํา | นอก | ขวัญ | ซ้ํา | อารมณ์ | ทุกข์ | แขก | เย็น | หนักหนา | ั้ง | ปิด | โปรด | ้ง | กําปั่น | เรียง | แรง | สิ่ง | เศร้า'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vocabs = [sp_pam.id_to_piece(id) for id in range(sp_pam.get_piece_size())]\n",
    "\" | \".join(unigram_vocabs[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2TsXA0UqN5LN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ้า | ่า | อง | ระ | ํา | รา | อย | ่ง | มา | จะ | ัง | ัน | ▁เ | าย | ้ว | ับ | ี่ | ม่ | อน | ให | าม | ้น | ็น | พระ | ีย | าง | กล | ้ง | ัก | หน | ให้ | ไม่ | หล | ่น | ึง | ▁แ | ทั | ตร | าร | ้อง | ไป | ิด | ข้า | ว่า | หม | คร | ือ | ล้ว | เป | เส | ประ | าน | ั่ง | ▁๏ | ▁ฯ | ที่ | อก | เล | ิน | ได | พล | ทร | ัด | นาง | ึก | ได้ | ู่ | ▁จะ | ค์ | ี้ | พร | เป็น | สุ | ทั้ง | อม | ัย | เร | ห็น | ▁จ | ▁พระ | ก็ | ใจ | อา | ื่ | ่าง | ต่ | กร | ิง | วง | วน | ือน | เจ | ู้ | ียง | อยู่ | รร | ตาม | ▁พ | ้วย | าว | ถึง | คล | ั้น | รี | เข | ด้วย | สม | องค์ | สน | าก | ▁แล้ว | เช | ัว | ย์ | ใน | คว | น้ | หมือน | ▁ส | ูก | อบ | กระ | เจ้า | ทรง | ลา | กัน | มี | ่าย | พรา | ิ่ง | เข้า | เห็น | ิต | สง | อด | ณ์ | วย | ้ม | คิด | เม | เก | เด | ▁นาง | วา | ุก | ▁ให้ | ดู | หา | ▁อ | ▁จึง | ทํา | ลง | รัก | เค | แล้ว | ่าน | พี่ | เหมือน | ั่น | ความ | ยง | อย่า | หร | มิ | ืน | ช่ | การ | ัญ | ▁ไม่ | ฝ่าย | ศรี | ้าง | วก | ้อม | ือง | น้อง | ยว | พา | แก | กํา | ่อน | ื่น | หน้า | ยา | ดี | ั้ง | ▁ทั้ง | ปรา | คน | เน | หว | รับ | แต่ | ้าย | ัส | เหล | ดา | สํา | นี้ | สาร | กับ | ลูก | ละ | ▁ต | รู้ | ื่อ | ▁ฝ่าย | ึ่ง | ลัง | าด | ื้ | กา | ขึ | นั่ง | เท | ▁เห็น | ฟัง | ้อย | ไร | ขึ้น | เสีย | ▁แต่ | บุ | สา | ไว | ทุก | กลับ | สุด | ัต | ใคร | น้ํา | ชา | ุด | ทัพ | วัน | สอง | นา | หย | ตา | รบ | ▁มา | ่อ | หรือ | ทู | ยัง | รง | จร | ปร | ▁บ | ไว้ | ดัง | วิ | ช่วย | ปล | ออก | ัตร | เพ | สิ | แจ | แล | ็จ | ิย์ | ▁พอ | มาร | ค่ | วรร | หมณ์ | คํา | เขา | นั้น | กษ | เย | ข้าง | หมา | เว | ไพร | หลัง | จิต | พราหมณ์ | ้ํา | ▁ถึง | ขอ | ทูล | สาม | ื้อ | วาย | อภ | ทาง | ▁แม | วัง | โฉ | ่ม | จน | ▁เป | ัตริย์ | ื่อง | สั่ง | แม่ | ▁ช | ฝ้า | โฉม | ราช | ฝร | ▁ถ | ฝรั่ง | ิ์ | ลม | แต | ▁เป็น | หาร | ื้น | เห | ้อน | ตาย | ุ่ง | ตัว | อย่าง | ลี | ผู้ | น้อย | ฉัน | ตรี | กุ | ษา | ุทร | ถาม | ของ | พร้อม | ชี | สร | เอ | ุง | พลาง | ตี | สมุทร | หาย | ที | วรรณ | เลี้ | นึก | จึง | หมาย | ▁ด้วย | ขว | ียน | ศึก | ่อง | ต้อง | ลัย | บา | พิ | อุ | สุวรรณ | โย | เรา | กลาง | เฝ้า | กษัตริย์ | สะ | แท | สัย | แจ้ง | หญ | ▁อย่า | รํา | ตรัส | อภัย | ผล | เลย | ียว | ไหน | ้าว | แน | ิดา | ริ | สาว | ิ้ม | เมือง | เล่า | ขัด | ค่อย | ภา | โอ | ่ํา | มัน | ชม | ห์ | ชาย | ัล | นาย | ▁เจ | เสียง | ยิ่ง | รู | ๋ย | เปล | เอา | ▁เส | คง | ตรา | ห้า | ินสมุทร | คอย | หญิง | หนี | ้าน | ญา | คุ | บรร | ▁ประ | กาย | ทหาร | ▁อัน | สิ้น | ทธ | ทอง | ักษ | ลังกา | นิ | พู | ศ์ | ่ว | จา | ใหญ | ที่ยว | มน | ไล | จริง | ▁เจ้า | จํา | ▁บ้าง | บอก | ▁ต่าง | ติ | ▁เข้า | ไม | ศร | อั | เคย | เลี้ยง | กรา | แสน | ▁จน | จับ | พบ | ครั้น | จง | พวก | สี | ไข | ษฐ | เกล | คา | รม | พัก | พัน | ซึ่ง | หนัก | นี | ่าว | กรุง | กล้ง | ▁เหมือน | ครา | เคร | ท้าว | ใส | ▁พวก | ตั้ง | หลง | ล้วน | ▁ไป | ผี | ลํา | นัก | ร้อง | ▁จง | ทรา | หนา | ▁ก็ | กลัว | ▁ที่ | เคียง | อาย | เรือ | ▁แม้น | เต | แค | ยก | พราะ | ใหญ่ | ▁ครั้น | ▁น | แก้ว | ถือ | ▁ได้ | เหลือ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_vocabs = [bpe_pam.id_to_piece(id) for id in range(bpe_pam.get_piece_size())]\n",
    "\" | \".join(bpe_vocabs[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu6QnnRfQyFj"
   },
   "source": [
    "### User-defined symbols\n",
    "\n",
    "Another important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n",
    "\n",
    "Refer to the documentation for ways to add these special tokens to your tokenizer.\n",
    "\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFOj62ZEdzT"
   },
   "source": [
    "## Train another tokenizer on another domain\n",
    "\n",
    "Now try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O7-QkA1eMZFf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pantip_train.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram_pantip\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pantip_train.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (7271 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9475 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 20 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=795024\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=185\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8875 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=344183\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 202602 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8875\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 28564\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 28564 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=41593 obj=36.7304 num_tokens=117107 num_tokens/piece=2.81555\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=35185 obj=31.7493 num_tokens=118246 num_tokens/piece=3.36069\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=26169 obj=31.928 num_tokens=126191 num_tokens/piece=4.82216\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25868 obj=31.5765 num_tokens=126408 num_tokens/piece=4.88666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19278 obj=32.403 num_tokens=135618 num_tokens/piece=7.03486\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19222 obj=32.081 num_tokens=135912 num_tokens/piece=7.07065\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14375 obj=33.0697 num_tokens=145771 num_tokens/piece=10.1406\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14365 obj=32.7542 num_tokens=145994 num_tokens/piece=10.1632\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10770 obj=33.8919 num_tokens=156025 num_tokens/piece=14.487\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10766 obj=33.5958 num_tokens=156169 num_tokens/piece=14.5058\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8071 obj=34.8705 num_tokens=166979 num_tokens/piece=20.6888\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8071 obj=34.5957 num_tokens=167077 num_tokens/piece=20.7009\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6053 obj=35.9697 num_tokens=178378 num_tokens/piece=29.4694\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6053 obj=35.6866 num_tokens=178906 num_tokens/piece=29.5566\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4538 obj=37.2058 num_tokens=190787 num_tokens/piece=42.0421\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4538 obj=36.9041 num_tokens=191139 num_tokens/piece=42.1197\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3403 obj=38.5838 num_tokens=204169 num_tokens/piece=59.9968\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3403 obj=38.2589 num_tokens=204177 num_tokens/piece=59.9991\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2552 obj=40.1566 num_tokens=218658 num_tokens/piece=85.681\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2552 obj=39.7848 num_tokens=218876 num_tokens/piece=85.7665\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1914 obj=41.8605 num_tokens=234410 num_tokens/piece=122.471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1914 obj=41.4363 num_tokens=234410 num_tokens/piece=122.471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1435 obj=43.7577 num_tokens=252316 num_tokens/piece=175.83\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1435 obj=43.2646 num_tokens=252333 num_tokens/piece=175.842\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=45.6164 num_tokens=272438 num_tokens/piece=247.671\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=45.0553 num_tokens=272438 num_tokens/piece=247.671\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: unigram_pantip.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: unigram_pantip.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pantip_train.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe_pantip\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pantip_train.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (7271 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9475 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 20 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=795024\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=185\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8875 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8875\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 28564\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7333 min_freq=114\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3173 size=20 all=5804 active=2376 piece=ื่\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2380 size=40 all=7671 active=4243 piece=ีย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1715 size=60 all=9767 active=6339 piece=เร\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1319 size=80 all=11907 active=8479 piece=ค่ะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1051 size=100 all=14341 active=10913 piece=่น\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1041 min_freq=116\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=894 size=120 all=16680 active=3222 piece=▁2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=763 size=140 all=19286 active=5828 piece=วิ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=659 size=160 all=21472 active=8014 piece=ราะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=590 size=180 all=23971 active=10513 piece=ไหม\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=532 size=200 all=26267 active=12809 piece=▁จ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=531 min_freq=75\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=491 size=220 all=28595 active=3607 piece=an\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=449 size=240 all=30763 active=5775 piece=บ้าง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=403 size=260 all=32643 active=7655 piece=พอ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=368 size=280 all=35059 active=10071 piece=re\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=337 size=300 all=37122 active=12134 piece=คา\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=337 min_freq=45\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=316 size=320 all=38924 active=3561 piece=าะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=299 size=340 all=40790 active=5427 piece=//\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=284 size=360 all=42844 active=7481 piece=ัส\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=273 size=380 all=44765 active=9402 piece=ttp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=260 size=400 all=46420 active=11057 piece=ขอ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=260 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=245 size=420 all=48202 active=3961 piece=รี\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=233 size=440 all=50161 active=5920 piece=ทํางาน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220 size=460 all=51692 active=7451 piece=เดิน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=208 size=480 all=53265 active=9024 piece=กระท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=197 size=500 all=55040 active=10799 piece=ic\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=195 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=520 all=56667 active=4333 piece=เพิ่ม\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=180 size=540 all=58098 active=5764 piece=ตลอด\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=169 size=560 all=59824 active=7490 piece=ทุน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=161 size=580 all=61407 active=9073 piece=te\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=156 size=600 all=62699 active=10365 piece=หลัก\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=156 min_freq=21\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=152 size=620 all=64239 active=4586 piece=วิต\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=148 size=640 all=65440 active=5787 piece=เร็\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=142 size=660 all=66481 active=6828 piece=▁9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=138 size=680 all=67726 active=8073 piece=สัย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=700 all=69187 active=9534 piece=โทร\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=132 min_freq=19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=720 all=70453 active=4622 piece=ส์\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=126 size=740 all=71615 active=5784 piece=▁ตอนนี้\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=760 all=72793 active=6962 piece=��ัง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=780 all=73858 active=8027 piece=ข้อมูล\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=800 all=75243 active=9412 piece=คม\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=115 min_freq=17\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: bpe_pantip.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe_pantip.vocab\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "spm.SentencePieceTrainer.train(input=pantip_train_file, model_prefix=\"unigram_pantip\", vocab_size=1000, model_type=\"unigram\")\n",
    "\n",
    "sp_pantip = spm.SentencePieceProcessor(model_file='unigram_pantip.model')\n",
    "\n",
    "spm.SentencePieceTrainer.train(input=pantip_train_file, model_prefix=\"bpe_pantip\", vocab_size=1000, model_type=\"bpe\")\n",
    "\n",
    "bpe_pantip = spm.SentencePieceProcessor(model_file='bpe_pantip.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_pantip: \n",
      "32\n",
      "['▁', 'อ', 'รุ', 'ณ', 'ส', 'ว', 'ั', 'ส', 'ด', 'ิ', '์', '▁', 'ฉัน', 'เอา', 'ม', 'เห', 'สี', 'มา', 'หา', 'ม', '▁', 'ส', 'ว', 'ั', 'ส', 'ดี', '▁', 'ประเทศ', 'ไทย', 'สบาย', 'ดี', 'ไหม']\n",
      "unigram_pra apai manee: \n",
      "29\n",
      "['▁', 'อ', 'ร', 'ุ', 'ณ', 'สวัสดิ์', '▁', 'ฉัน', 'เอา', 'มเหสี', 'มา', 'หา', 'ม', '▁', 'ส', 'ว', 'ั', 'ส', 'ดี', '▁', 'ประเทศ', 'ไ', 'ท', 'ย', 'สบาย', 'ดี', 'ไ', 'ห', 'ม']\n"
     ]
    }
   ],
   "source": [
    "print(\"unigram_pantip: \")\n",
    "print(len(sp_pantip.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str)))\n",
    "print(sp_pantip.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))\n",
    "\n",
    "print(\"unigram_pra apai manee: \")\n",
    "print(len(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str)))\n",
    "print(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5WOVMbONnYv"
   },
   "source": [
    "## Analyse top tokens on different datasets\n",
    "\n",
    "Use your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wbfkGcsUrPYS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Pantip dataset with Unigram tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ที่', 4183),\n",
       " ('เรา', 2568),\n",
       " ('จะ', 2559),\n",
       " ('มา', 2478),\n",
       " ('ไป', 2449),\n",
       " ('ได้', 2432),\n",
       " ('ก็', 2392),\n",
       " ('ไม่', 2222),\n",
       " ('ว่า', 2195),\n",
       " ('มี', 2076),\n",
       " ('เป็น', 2056),\n",
       " ('การ', 1747),\n",
       " ('ให้', 1608),\n",
       " ('ของ', 1475),\n",
       " ('ใน', 1376),\n",
       " ('แล้ว', 1367),\n",
       " ('เลย', 1358),\n",
       " ('ครับ', 1325),\n",
       " ('กัน', 1305),\n",
       " ('นี้', 1291),\n",
       " ('กับ', 1234),\n",
       " ('ค่ะ', 1172),\n",
       " ('ดี', 1155),\n",
       " ('▁แต่', 1107),\n",
       " ('คน', 1065),\n",
       " ('ทํา', 1044),\n",
       " ('ต้อง', 1021),\n",
       " ('มัน', 1000),\n",
       " ('้า', 932),\n",
       " ('มาก', 924),\n",
       " ('อยู่', 910),\n",
       " ('จาก', 903),\n",
       " ('เขา', 900),\n",
       " ('่า', 880),\n",
       " ('ความ', 865),\n",
       " ('ใช้', 767),\n",
       " ('ด้วย', 749),\n",
       " ('แต่', 740),\n",
       " ('อะไร', 736),\n",
       " ('▁เรา', 736),\n",
       " ('ผม', 731),\n",
       " ('ตัว', 727),\n",
       " ('ใจ', 713),\n",
       " ('เรื่อง', 709),\n",
       " ('หา', 686),\n",
       " ('ํา', 682),\n",
       " ('ไม่ได้', 666),\n",
       " ('ดู', 659),\n",
       " ('ีย', 646),\n",
       " ('▁และ', 642),\n",
       " ('วัน', 640),\n",
       " ('พอ', 636),\n",
       " ('▁(', 634),\n",
       " ('และ', 612),\n",
       " ('ับ', 611),\n",
       " ('รับ', 600),\n",
       " ('เข้า', 594),\n",
       " ('แบบ', 591),\n",
       " ('งาน', 588),\n",
       " ('อยาก', 585),\n",
       " ('นั้น', 585),\n",
       " ('ถึง', 585),\n",
       " ('ัง', 582),\n",
       " ('คือ', 580),\n",
       " ('ขึ้น', 579),\n",
       " ('▁แล้ว', 566),\n",
       " ('อย่าง', 543),\n",
       " ('ัน', 536),\n",
       " ('สอบ', 535),\n",
       " ('เวลา', 527),\n",
       " ('ตาม', 524),\n",
       " ('เมื่อ', 520),\n",
       " ('อีก', 518),\n",
       " ('ขอ', 510),\n",
       " ('ปี', 496),\n",
       " ('เพื่อน', 495),\n",
       " ('บ้าง', 494),\n",
       " ('▁1', 493),\n",
       " ('อน', 492),\n",
       " ('เค้า', 490),\n",
       " ('ทาง', 487),\n",
       " ('คะ', 487),\n",
       " ('ทุก', 479),\n",
       " ('ไหน', 476),\n",
       " ('เงิน', 475),\n",
       " ('ยัง', 475),\n",
       " ('ลง', 470),\n",
       " ('▁2', 469),\n",
       " ('ผู้', 468),\n",
       " ('หน้า', 467),\n",
       " ('หรือ', 463),\n",
       " ('▁คือ', 456),\n",
       " ('ยา', 456),\n",
       " ('บอก', 455),\n",
       " ('ิน', 454),\n",
       " ('บ้าน', 453),\n",
       " ('ก่อน', 453),\n",
       " ('ต่อ', 450),\n",
       " ('เหมือน', 445),\n",
       " ('รา', 438),\n",
       " ('ตอน', 436),\n",
       " ('▁หรือ', 435),\n",
       " ('ละ', 434),\n",
       " ('ออก', 432),\n",
       " ('on', 432),\n",
       " ('ที', 430),\n",
       " ('น้ํา', 422),\n",
       " ('นา', 422),\n",
       " ('นะ', 420),\n",
       " ('ลา', 418),\n",
       " ('เรียน', 418),\n",
       " ('ชอบ', 416),\n",
       " ('แฟน', 406),\n",
       " ('ัด', 404),\n",
       " ('เราก็', 401),\n",
       " ('ประ', 398),\n",
       " ('เอา', 397),\n",
       " ('คํา', 394),\n",
       " ('ทั้ง', 391),\n",
       " ('ริ', 391),\n",
       " ('จน', 385),\n",
       " ('00', 379),\n",
       " ('ควร', 374),\n",
       " ('คิด', 374),\n",
       " ('▁เพราะ', 374),\n",
       " ('ซื้อ', 370),\n",
       " ('ัก', 370),\n",
       " ('เเ', 368),\n",
       " ('แม่', 365),\n",
       " ('ติด', 364),\n",
       " ('หน่อย', 362),\n",
       " ('้น', 361),\n",
       " ('กว่า', 361),\n",
       " ('่อ', 360),\n",
       " ('ร์', 360),\n",
       " ('ค่า', 354),\n",
       " ('รี', 353),\n",
       " ('ใคร', 352),\n",
       " ('สามารถ', 349),\n",
       " ('เห็น', 348),\n",
       " ('เดือน', 348),\n",
       " ('วิ', 345),\n",
       " ('รู้', 344),\n",
       " ('in', 341),\n",
       " ('กระ', 341),\n",
       " ('อา', 339),\n",
       " ('ไหม', 338),\n",
       " ('วง', 338),\n",
       " ('▁ผม', 336),\n",
       " ('นะคะ', 336),\n",
       " ('อม', 336),\n",
       " ('ตา', 336),\n",
       " ('เม', 330),\n",
       " ('ส่ง', 328),\n",
       " ('20', 325),\n",
       " ('แค่', 323),\n",
       " ('เห', 323),\n",
       " ('เพราะ', 322),\n",
       " ('คุย', 320),\n",
       " ('รู้สึก', 318),\n",
       " ('ทําให้', 318),\n",
       " ('ar', 318),\n",
       " ('ข้อความ', 315),\n",
       " ('ไว้', 314),\n",
       " ('พี่', 314),\n",
       " ('ช่วย', 314),\n",
       " ('ไทย', 313),\n",
       " ('ไม่มี', 312),\n",
       " ('or', 311),\n",
       " ('▁ถ้า', 307),\n",
       " ('รถ', 307),\n",
       " ('ติ', 307),\n",
       " ('โดย', 306),\n",
       " ('▁แนวข้อสอบ', 306),\n",
       " ('เส', 305),\n",
       " ('ตอนนี้', 305),\n",
       " ('▁ซึ่ง', 303),\n",
       " ('่ง', 303),\n",
       " ('▁S', 300),\n",
       " ('ัย', 298),\n",
       " ('เด', 294),\n",
       " ('er', 292),\n",
       " ('ใหม่', 290),\n",
       " ('เพื่อ', 290),\n",
       " ('an', 289),\n",
       " ('เคย', 289),\n",
       " ('แบบนี้', 288),\n",
       " ('▁อยาก', 288),\n",
       " ('ข้อ', 288),\n",
       " ('เท', 288),\n",
       " ('เจอ', 288),\n",
       " ('เก', 287),\n",
       " ('ระ', 287),\n",
       " ('อร์', 285),\n",
       " ('ชา', 284),\n",
       " ('ทํางาน', 284),\n",
       " ('ผ่าน', 283),\n",
       " ('เล', 283),\n",
       " ('ผล', 282),\n",
       " ('พล', 281),\n",
       " ('▁บาท', 281),\n",
       " ('รุ', 280),\n",
       " ('ราคา', 278),\n",
       " ('ถาม', 276),\n",
       " ('10', 275),\n",
       " ('ถูก', 274),\n",
       " ('ครั้ง', 274),\n",
       " ('อง', 274),\n",
       " ('com', 274),\n",
       " ('อด', 273),\n",
       " ('://', 273),\n",
       " ('ี่', 270),\n",
       " ('กร', 269),\n",
       " ('re', 268),\n",
       " ('เอ', 267),\n",
       " ('...', 267),\n",
       " ('วน', 266),\n",
       " ('ภาพ', 266),\n",
       " ('กลับ', 265),\n",
       " ('ตั้ง', 263),\n",
       " ('▁แก้ไข', 261),\n",
       " ('เค', 261),\n",
       " ('ถ้า', 260),\n",
       " ('บาง', 260),\n",
       " ('เกี่ยวกับ', 260),\n",
       " ('▁C', 260),\n",
       " ('▁http', 259),\n",
       " ('ขอบคุณ', 256),\n",
       " ('ช่วง', 255),\n",
       " ('หลาย', 254),\n",
       " ('เอง', 253),\n",
       " ('รัก', 251),\n",
       " ('เปิด', 251),\n",
       " ('ส่วน', 250),\n",
       " ('ยังไง', 249),\n",
       " ('ประมาณ', 249),\n",
       " ('เล่น', 245),\n",
       " ('เดียว', 242),\n",
       " ('แนะนํา', 241),\n",
       " ('st', 240),\n",
       " ('เสีย', 237),\n",
       " ('ู่', 237),\n",
       " ('หลัง', 235),\n",
       " ('ตอบ', 235),\n",
       " ('al', 235),\n",
       " ('คุณ', 233),\n",
       " ('นี่', 231),\n",
       " ('หมด', 230),\n",
       " ('นิ', 229),\n",
       " ('น้อง', 229),\n",
       " ('เปลี่ยน', 229),\n",
       " ('เริ่ม', 228),\n",
       " ('เลือก', 228),\n",
       " ('เกิด', 228),\n",
       " ('ี้', 227),\n",
       " ('ย์', 226),\n",
       " ('บอกว่า', 226),\n",
       " ('พ่อ', 226),\n",
       " ('en', 226),\n",
       " ('ื่อ', 224),\n",
       " ('เว', 223),\n",
       " ('พูด', 222),\n",
       " ('สิ', 219),\n",
       " ('กิน', 219),\n",
       " ('▁M', 219),\n",
       " ('ลูก', 217),\n",
       " ('อ่าน', 216),\n",
       " ('เบ', 215),\n",
       " ('ปกติ', 213),\n",
       " ('จริง', 213),\n",
       " ('ั่น', 212),\n",
       " ('เป', 211),\n",
       " ('ใส่', 210),\n",
       " ('นึง', 210),\n",
       " ('25', 210),\n",
       " ('ั้น', 210),\n",
       " ('สุด', 210),\n",
       " ('อัน', 210),\n",
       " ('มั้ย', 207),\n",
       " ('แรก', 207),\n",
       " ('ตลอด', 206),\n",
       " ('สม', 205),\n",
       " ('ตัวเอง', 204),\n",
       " ('ตรง', 203),\n",
       " ('จริงๆ', 201),\n",
       " ('....', 201),\n",
       " ('มือ', 200),\n",
       " ('ไม่รู้', 199),\n",
       " ('อย', 199),\n",
       " ('นาน', 199),\n",
       " ('ิ่ง', 198),\n",
       " ('ทําไม', 196),\n",
       " ('นํา', 196),\n",
       " ('มากๆ', 196),\n",
       " ('เครื่อง', 196),\n",
       " ('อะ', 195),\n",
       " ('ผิด', 193),\n",
       " ('ราย', 193),\n",
       " ('ขาย', 192),\n",
       " ('นัก', 190)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Pra-Apai-Manee dataset with Unigram tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('มา', 3002),\n",
       " ('จะ', 2506),\n",
       " ('ไป', 2407),\n",
       " ('ให้', 2391),\n",
       " ('ว่า', 2214),\n",
       " ('ไม่', 2085),\n",
       " ('ที่', 1632),\n",
       " ('เป็น', 1587),\n",
       " ('▁จะ', 1580),\n",
       " ('พระ', 1561),\n",
       " ('ใจ', 1458),\n",
       " ('ก็', 1368),\n",
       " ('อยู่', 1339),\n",
       " ('▁พระ', 1251),\n",
       " ('ได้', 1247),\n",
       " ('เข้า', 1190),\n",
       " ('รา', 1128),\n",
       " ('ตาม', 1096),\n",
       " ('้า', 1086),\n",
       " ('▁แล้ว', 1085),\n",
       " ('ใน', 1069),\n",
       " ('ยา', 1063),\n",
       " ('ลา', 1052),\n",
       " ('ํา', 1039),\n",
       " ('เจ้า', 1024),\n",
       " ('เหมือน', 1009),\n",
       " ('กัน', 976),\n",
       " ('หา', 973),\n",
       " ('ประ', 944),\n",
       " ('อน', 933),\n",
       " ('ทรง', 914),\n",
       " ('ัง', 914),\n",
       " ('เห็น', 910),\n",
       " ('▁ให้', 898),\n",
       " ('าน', 879),\n",
       " ('นาง', 875),\n",
       " ('ัก', 868),\n",
       " ('ัด', 837),\n",
       " ('ดู', 834),\n",
       " ('องค์', 827),\n",
       " ('มี', 809),\n",
       " ('่า', 801),\n",
       " ('▁จึง', 799),\n",
       " ('▁นาง', 792),\n",
       " ('วน', 782),\n",
       " ('พา', 777),\n",
       " ('แล้ว', 771),\n",
       " ('นี้', 770),\n",
       " ('่อ', 765),\n",
       " ('ด้วย', 760),\n",
       " ('ลูก', 744),\n",
       " ('น้อง', 741),\n",
       " ('ทํา', 738),\n",
       " ('รัก', 736),\n",
       " ('หน', 731),\n",
       " ('พี่', 719),\n",
       " ('การ', 714),\n",
       " ('คิด', 712),\n",
       " ('พล', 712),\n",
       " ('กระ', 710),\n",
       " ('รู้', 695),\n",
       " ('กับ', 683),\n",
       " ('แต่', 677),\n",
       " ('หน้า', 670),\n",
       " ('ระ', 666),\n",
       " ('ออก', 665),\n",
       " ('▁ไม่', 662),\n",
       " ('ัน', 660),\n",
       " ('ับ', 656),\n",
       " ('เขา', 652),\n",
       " ('่ง', 637),\n",
       " ('พร', 622),\n",
       " ('นั่ง', 620),\n",
       " ('ข้า', 620),\n",
       " ('คน', 617),\n",
       " ('รับ', 616),\n",
       " ('วง', 614),\n",
       " ('ดี', 610),\n",
       " ('ถึง', 601),\n",
       " ('ริ', 597),\n",
       " ('▁แต่', 592),\n",
       " ('หล', 585),\n",
       " ('▁เห็น', 581),\n",
       " ('ลง', 578),\n",
       " ('กร', 576),\n",
       " ('อด', 576),\n",
       " ('กล', 574),\n",
       " ('สม', 573),\n",
       " ('ความ', 573),\n",
       " ('ตร', 568),\n",
       " ('ทั้ง', 566),\n",
       " ('ขึ้น', 566),\n",
       " ('เสีย', 564),\n",
       " ('ือ', 557),\n",
       " ('กลับ', 555),\n",
       " ('▁ฝ่าย', 553),\n",
       " ('ปร', 544),\n",
       " ('มิ', 540),\n",
       " ('ขอ', 540),\n",
       " ('้น', 539),\n",
       " ('หรือ', 534),\n",
       " ('ยัง', 531),\n",
       " ('▁ทั้ง', 528),\n",
       " ('เส', 526),\n",
       " ('ฟัง', 523),\n",
       " ('แม่', 522),\n",
       " ('ไว้', 521),\n",
       " ('▁พอ', 513),\n",
       " ('วัน', 512),\n",
       " ('ดัง', 508),\n",
       " ('รบ', 504),\n",
       " ('น้ํา', 502),\n",
       " ('ิน', 502),\n",
       " ('อา', 497),\n",
       " ('ี่', 493),\n",
       " ('ช่วย', 493),\n",
       " ('นา', 491),\n",
       " ('นั้น', 490),\n",
       " ('อง', 489),\n",
       " ('ทัพ', 477),\n",
       " ('▁มา', 474),\n",
       " ('ต้อง', 474),\n",
       " ('น้อย', 472),\n",
       " ('เรา', 468),\n",
       " ('จน', 467),\n",
       " ('ปล', 465),\n",
       " ('เอา', 462),\n",
       " ('สุด', 456),\n",
       " ('ละ', 453),\n",
       " ('เสียง', 452),\n",
       " ('สุ', 451),\n",
       " ('ข้าง', 448),\n",
       " ('ตี', 446),\n",
       " ('ึก', 444),\n",
       " ('หลัง', 440),\n",
       " ('พิ', 439),\n",
       " ('▁ถึง', 436),\n",
       " ('ตัว', 436),\n",
       " ('วัง', 433),\n",
       " ('นาย', 433),\n",
       " ('ทูล', 432),\n",
       " ('วิ', 431),\n",
       " ('่น', 429),\n",
       " ('ยง', 429),\n",
       " ('อก', 426),\n",
       " ('อย่า', 423),\n",
       " ('ทุก', 420),\n",
       " ('นึก', 417),\n",
       " ('ลี', 412),\n",
       " ('เฝ้า', 406),\n",
       " ('ตา', 403),\n",
       " ('ตาย', 398),\n",
       " ('ฝรั่ง', 398),\n",
       " ('▁เป็น', 394),\n",
       " ('แล', 392),\n",
       " ('อั', 391),\n",
       " ('ใคร', 391),\n",
       " ('ราช', 390),\n",
       " ('ย์', 390),\n",
       " ('เรือ', 383),\n",
       " ('้ม', 380),\n",
       " ('ของ', 372),\n",
       " ('สอง', 369),\n",
       " ('กษัตริย์', 367),\n",
       " ('แสน', 367),\n",
       " ('ี้', 367),\n",
       " ('ึง', 363),\n",
       " ('หาย', 359),\n",
       " ('ั่ง', 359),\n",
       " ('จิต', 358),\n",
       " ('หมาย', 357),\n",
       " ('สั่ง', 355),\n",
       " ('จึง', 355),\n",
       " ('▁ด้วย', 354),\n",
       " ('แจ้ง', 352),\n",
       " ('นิ', 346),\n",
       " ('เห', 346),\n",
       " ('ุด', 345),\n",
       " ('สาม', 343),\n",
       " ('▁อย่า', 342),\n",
       " ('เมือง', 341),\n",
       " ('ั่น', 340),\n",
       " ('สาว', 340),\n",
       " ('เหลือ', 339),\n",
       " ('กลาง', 338),\n",
       " ('ยิ่ง', 336),\n",
       " ('ตรัส', 335),\n",
       " ('มัน', 333),\n",
       " ('ลังกา', 333),\n",
       " ('เลย', 332),\n",
       " ('เล่า', 331),\n",
       " ('ทาง', 329),\n",
       " ('ศรี', 328),\n",
       " ('ไหน', 328),\n",
       " ('้อง', 327),\n",
       " ('เคย', 327),\n",
       " ('หนี', 326),\n",
       " ('ื้อ', 323),\n",
       " ('น่า', 322),\n",
       " ('ที', 322),\n",
       " ('ยก', 321),\n",
       " ('ค่อย', 320),\n",
       " ('ชาย', 318),\n",
       " ('พราหมณ์', 313),\n",
       " ('ถาม', 311),\n",
       " ('ไร', 306),\n",
       " ('สิ้น', 305),\n",
       " ('กาย', 305),\n",
       " ('ฉัน', 305),\n",
       " ('ลม', 304),\n",
       " ('บอก', 303),\n",
       " ('ชาว', 301),\n",
       " ('พ่อ', 301),\n",
       " ('คอย', 299),\n",
       " ('สงสาร', 298),\n",
       " ('สาร', 297),\n",
       " ('อบ', 296),\n",
       " ('ชม', 295),\n",
       " ('จริง', 295),\n",
       " ('▁อัน', 291),\n",
       " ('คํา', 291),\n",
       " ('ตั้ง', 291),\n",
       " ('ทหาร', 291),\n",
       " ('เที่ยว', 290),\n",
       " ('ผู้', 289),\n",
       " ('วาย', 288),\n",
       " ('ทอง', 287),\n",
       " ('ผา', 286),\n",
       " ('ศึก', 286),\n",
       " ('สิ', 283),\n",
       " ('จะได้', 282),\n",
       " ('ใหญ่', 281),\n",
       " ('ถือ', 281),\n",
       " ('สา', 280),\n",
       " ('▁บ้าง', 279),\n",
       " ('ั้น', 279),\n",
       " ('พบ', 279),\n",
       " ('พระอภัย', 278),\n",
       " ('▁ต่าง', 277),\n",
       " ('ม้า', 277),\n",
       " ('จับ', 276),\n",
       " ('จง', 273),\n",
       " ('พลาง', 273),\n",
       " ('ล่อ', 271),\n",
       " ('พวก', 270),\n",
       " ('▁จน', 268),\n",
       " ('ลาย', 268),\n",
       " ('ไพร่', 267),\n",
       " ('ราย', 267),\n",
       " ('รอง', 264),\n",
       " ('ร้อย', 264),\n",
       " ('ล้วน', 263),\n",
       " ('▁พวก', 263),\n",
       " ('อาย', 262),\n",
       " ('พร้อม', 261),\n",
       " ('้ว', 261),\n",
       " ('ชิง', 260),\n",
       " ('พัน', 260),\n",
       " ('งาม', 259),\n",
       " ('อย่าง', 258),\n",
       " ('แกล้ง', 257),\n",
       " ('จํา', 254),\n",
       " ('เคียง', 252),\n",
       " ('เครื่อง', 251),\n",
       " ('กลัว', 251),\n",
       " ('ลํา', 250),\n",
       " ('นี่', 249),\n",
       " ('▁จง', 245),\n",
       " ('สินสมุทร', 244),\n",
       " ('สาย', 241),\n",
       " ('แก้ว', 240),\n",
       " ('พระองค์', 240),\n",
       " ('▁แม้น', 239),\n",
       " ('▁ครั้น', 238),\n",
       " ('เพราะ', 238),\n",
       " ('บน', 236),\n",
       " ('ร้อง', 235),\n",
       " ('คง', 234),\n",
       " ('ต่าง', 232),\n",
       " ('ผล', 231),\n",
       " ('ห้อง', 229),\n",
       " ('สู้', 227),\n",
       " ('จาก', 222),\n",
       " ('ท่าน', 220),\n",
       " ('ขัด', 220),\n",
       " ('โห', 219),\n",
       " ('เดิน', 217),\n",
       " ('หลับ', 217),\n",
       " ('เรียก', 216),\n",
       " ('ป่า', 216),\n",
       " ('นัก', 215),\n",
       " ('เหล่า', 215),\n",
       " ('กิน', 214),\n",
       " ('กุมาร', 214),\n",
       " ('ลิ', 212),\n",
       " ('คู่', 211),\n",
       " ('โศก', 210),\n",
       " ('หลง', 209),\n",
       " ('ร่ํา', 209),\n",
       " ('ู่', 209)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Analyze the most common tokens in a dataset\n",
    "def analyze_top_tokens(data, tokenizer, top_n=300, min_len=1):\n",
    "    # Tokenize the data\n",
    "    all_tokens = []\n",
    "    for line in data:\n",
    "        tokens = tokenizer.encode(line.strip(), out_type=str)\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count token frequencies\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Filter tokens with length > min_len\n",
    "    filtered_tokens = {token: count for token, count in token_counts.items() if len(token) > min_len}\n",
    "    \n",
    "    # Sort tokens by frequency\n",
    "    sorted_tokens = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create a DataFrame for top tokens\n",
    "    top_tokens_df = pd.DataFrame(sorted_tokens[:top_n], columns=[\"Token\", \"Frequency\"])\n",
    "    \n",
    "    # Return both DataFrame and list of pairs\n",
    "    return top_tokens_df, sorted_tokens[:top_n]\n",
    "\n",
    "\n",
    "# Tokenize and analyze tokens for each dataset and tokenizer\n",
    "print(\"Analyzing Pantip dataset with Unigram tokenizer...\")\n",
    "pantip_unigram_results_df, sorted_tokens = analyze_top_tokens(pantip_train_text, sp_pantip)\n",
    "display(sorted_tokens)\n",
    "\n",
    "print(\"\\nAnalyzing Pra-Apai-Manee dataset with Unigram tokenizer...\")\n",
    "pam_unigram_results_df, sorted_tokens = analyze_top_tokens(pam_train_text, sp_pam)\n",
    "display(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz0GdZ-5YYM9"
   },
   "source": [
    "### To answer\n",
    "What are some notable differences you see between the two vocabs?\n",
    "\n",
    "Write your answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QxxYr0QLbDoU"
   },
   "outputs": [],
   "source": [
    "# - Token from Pantip dataset with Unigram tokenizer Seems to be spoken thai language level with ครับ ค่ะ นะคะ \n",
    "# - Token from Pra-Apai-Manee dataset with Unigram tokenizer Seems to be written in old thai language level with เจ้า พระ ข้า"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipjO87HPYl4N"
   },
   "source": [
    "## Using tokenizer across domains\n",
    "\n",
    "One problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n",
    "\n",
    "Next you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (พระอภัยมณี) and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4_6JG_l5BXh"
   },
   "source": [
    "### Q3 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole พระอภัยมณี dataset with a tokenizer trained on Pantip compared to the one trained on พระอภัยมณี."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3tCh1RaZrTAM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '๏', '▁แต่', 'ป', 'า', 'ง', 'หลัง', 'ยัง', 'มี', 'กรุง']\n",
      "['▁', '๏', '▁แต่', 'ป', 'า', 'ง', 'หลัง', 'ยัง', 'มี', 'ก']\n",
      "141.50978497925553\n"
     ]
    }
   ],
   "source": [
    "tokens_pam_on_pam_trained = [a for line in pra_apai_manee_data for a in sp_pam.encode(line, out_type=str)]\n",
    "tokens_pam_on_pantip_trained = [a for line in pra_apai_manee_data for a in sp_pantip.encode(line, out_type=str)]\n",
    "\n",
    "print(tokens_pam_on_pam_trained[:10])\n",
    "print(tokens_pam_on_pantip_trained[:10])\n",
    "print(100*len(tokens_pam_on_pantip_trained)/len(tokens_pam_on_pam_trained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duaCJRO96SX1"
   },
   "source": [
    "### Q4 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on พระอภัยมณี compared to the one trained on Pantip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "axk9gOIgrTYd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'ใคร', 'รู้จัก', 'คน', 'นี้', 'บ้าง', '▁คือ', 'เรา', 'ค', 'ุ']\n",
      "['▁', 'ใคร', 'รู้', 'จัก', 'คน', 'นี้', 'บ้าง', '▁', 'ค', 'ือ']\n",
      "115.5704503918366\n"
     ]
    }
   ],
   "source": [
    "tokens_pantip_on_pantip_trained = [a for line in pantip_text for a in sp_pantip.encode(line, out_type=str)]\n",
    "tokens_pantip_on_pam_trained = [a for line in pantip_text for a in sp_pam.encode(line, out_type=str)]\n",
    "\n",
    "print(tokens_pantip_on_pantip_trained[:10])\n",
    "print(tokens_pantip_on_pam_trained[:10])\n",
    "print(100*len(tokens_pantip_on_pam_trained)/len(tokens_pantip_on_pantip_trained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZYKuamv7-wI"
   },
   "source": [
    "### To answer\n",
    "Why do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gh9a6d7Q8ivJ"
   },
   "outputs": [],
   "source": [
    "# I belive that In Pra apai manee dataset have a more diverse vocabulary than Pantip dataset. \n",
    "# Therefore spm trained on pam should have more generalization of the language than spm trained on pantip.\n",
    "# and result in a lower percentage of tokens from pantip dataset that can be tokenized by spm trained on pam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7j_Cc0p9-5S"
   },
   "source": [
    "## The effect on language models\n",
    "\n",
    "Next, we will see the effect of using \"cross-domain\" tokenizers on Language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiWztANvohhn"
   },
   "source": [
    "### Setup\n",
    "We are going to reuse the code from the last assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7pVtSbmVpwOo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (2.5.0.post0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (2.5.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (1.6.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (2.5.0.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
      "Requirement already satisfied: setuptools in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (73.0.1)\n",
      "Requirement already satisfied: filelock in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.3)\n",
      "Requirement already satisfied: idna>=2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JMt5GzLrW4x3"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0OIs_VS_oo1M"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, seq_len = 128):\n",
    "\n",
    "    token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids))\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hk6vEPiMq34n"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size=vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        emb = self.dropout(self.embedding(src))\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        with torch.no_grad():\n",
    "          prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oKhuOygixndB"
   },
   "outputs": [],
   "source": [
    "vocab_size = sp_pam.get_piece_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_batch_size = 64\n",
    "test_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOtOE7mr-heY"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8-x9HiPDcpE"
   },
   "source": [
    "<a name=\"no1\"></a>\n",
    "#### 1. Training on Pantip data with Pantip tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oUv_A4MTx0Ob"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (44) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 44/44 [00:01<00:00, 24.94it/s, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 44/44 [00:01<00:00, 23.64it/s, v_num=5]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1e-Y1_GYy65g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 9/9 [00:00<00:00, 53.43it/s]  \n",
      "Perplexity on Pantip train set is:\t77.62148913999245\n",
      "Perplexity on Pra apai manee train set is:\t111.55689831390926\n",
      "Perplexity on Pantip test set is:\t106.26474865149454\n",
      "Perplexity on Pra apai manee test set is:\t113.8366556777151\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s3AmE4nDjmL"
   },
   "source": [
    "<a name=\"no2\"></a>\n",
    "#### 2. Training on Pantip data with Pra apai manee tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vfRdW3m1Dmj_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 51/51 [00:01<00:00, 25.55it/s, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 51/51 [00:02<00:00, 24.41it/s, v_num=6]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xwLN1IarD3g9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 7/7 [00:00<00:00, 41.26it/s]  \n",
      "Perplexity on Pantip train set is:\t36.38565572511128\n",
      "Perplexity on Pra apai manee train set is:\t442.9921778117153\n",
      "Perplexity on Pantip test set is:\t46.447643820664446\n",
      "Perplexity on Pra apai manee test set is:\t419.1088872194669\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB8zqptTWcA6"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n",
    "2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>\n",
    "Hint:\n",
    "1. think about \"general\" vocabs and domain-specific vocabs.\n",
    "2. what do you think happens to the model when the token ids become longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pantip tokenizer uses more common subwords, \n",
    "allowing the model to generalize better across datasets by breaking rare or domain-specific words into familiar components. \n",
    "\n",
    "\n",
    "\n",
    "In contrast, the Pra Apai Manee tokenizer focuses on rare, domain-specific tokens \n",
    "that do not appear in the Pantip dataset, leading to poor generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VPMm7pLdSl"
   },
   "source": [
    "\n",
    "<a name=\"no3\"></a>\n",
    "#### 3. Training on Pra apai manee data with Pantip tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "oR5fp-YCLnnU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 66/66 [00:02<00:00, 24.23it/s, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 66/66 [00:02<00:00, 23.29it/s, v_num=7]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "f_LhF7w7Lxwo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 9/9 [00:00<00:00, 41.94it/s]  \n",
      "Perplexity on Pantip train set is:\t3895.934105398706\n",
      "Perplexity on Pra apai manee train set is:\t41.26073564164805\n",
      "Perplexity on Pantip test set is:\t3151.679503293764\n",
      "Perplexity on Pra apai manee test set is:\t44.29268968018155\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apk9crJjMLoW"
   },
   "source": [
    "<a name=\"no4\"></a>\n",
    "#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_G7GMBIKLzGK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 48/48 [00:01<00:00, 25.01it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 48/48 [00:02<00:00, 23.69it/s, v_num=8]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9H753o_JMRFw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 7/7 [00:00<00:00, 41.98it/s]  \n",
      "Perplexity on Pantip train set is:\t595.8866183048801\n",
      "Perplexity on Pra apai manee train set is:\t76.8869146157321\n",
      "Perplexity on Pantip test set is:\t584.2165288791805\n",
      "Perplexity on Pra apai manee test set is:\t85.709975512397\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Lmmj4dZ-1"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n",
    "2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The Pra Apai Manee tokenizer likely contains a broader range of tokens, capturing both general and domain-specific vocabulary. Since it is trained on a rich and varied dataset (Pra Apai Manee), it can better handle diverse compositions and word usage, allowing it to generalize more effectively across different domains.\n",
    "\n",
    "On the other hand, the Pantip tokenizer is specialized in everyday language and informal writing, which limits its ability to represent the complex and less-common vocabulary found in the Pra Apai Manee dataset. As a result, it struggles to generalize well when encountering text from different domains.\n",
    "\n",
    "This explains why the Pra Apai Manee tokenizer leads to better generalization, despite both models overfitting on the Pra Apai Manee data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
