{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "15QfB7RAuXAc"
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gucid6KNuXAe"
   },
   "source": [
    "In this Exercise, we will be using Pytorch Lightning to implement our neural LM. Your job will be just to write the forward method of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL_M2zf4myYa"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MRRrn78ZjL54"
   },
   "outputs": [],
   "source": [
    "# # #download corpus\n",
    "# !wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
    "# !unzip BEST2010.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SGmYebp38OUl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (2.5.0.post0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (2.5.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (1.6.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning) (2.5.0.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
      "Requirement already satisfied: setuptools in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (73.0.1)\n",
      "Requirement already satisfied: filelock in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.3)\n",
      "Requirement already satisfied: idna>=2.0 in /home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR4HK5jQm17K"
   },
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oPE1RqKOrWJ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in BEST2010 news training dataset :\t21678\n",
      "Total word counts in BEST2010 news training dataset :\t1042797\n"
     ]
    }
   ],
   "source": [
    "total_word_count = 0\n",
    "best2010 = []\n",
    "with open('BEST2010/news.txt','r',encoding='utf-8') as f:\n",
    "  for i,line in enumerate(f):\n",
    "    line=line.strip()[:-1] #remove the trailing |\n",
    "    total_word_count += len(line.split(\"|\"))\n",
    "    best2010.append(line)\n",
    "\n",
    "train = best2010[:int(len(best2010)*0.7)]\n",
    "test = best2010[int(len(best2010)*0.7):]\n",
    "#Training data\n",
    "train_word_count =0\n",
    "for line in train:\n",
    "    for word in line.split('|'):\n",
    "        train_word_count+=1\n",
    "print ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\n",
    "print ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQBjqe5arHGX"
   },
   "source": [
    "Here we are going to use a library from huggingface called `tokenizers`. This will help us create a vocabulary and handle the encoding and decoding, i.e., convert text to its corresponding ID (which will be learned by the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "elwE0gh2rE3C"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "#Basically, we just use the new tokenizer as our vocab building tool.\n",
    "#In practice, you will have to use a compatible tokenizer like newmm to tokenize the corpus first then do this step\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=\"|\") #now the tokenizer will split \"|\" for us\n",
    "trainer = WordLevelTrainer(min_frequency=3,  #we can set a frequency threshold for taking a word into our vocab. for this example, words with freq < 3 will be excluded from the vocab.\n",
    "                           special_tokens=[\"[UNK]\", \"<s>\", \"</s>\"]) #these are our special tokens: for unknown, begin-of-sentence, and end-of-sentence, respectively.\n",
    "tokenizer.train_from_iterator(train, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TrKtjv4PJpg2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9062"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab()) #same as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WqM_jrZwrJpB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎหมาย', 'กับ', 'การ', 'เบียดบัง', 'คน', 'จน', '[UNK]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").tokens #tokens we get after tokenizing this sentence. unknown words will be tokenized as [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1r1pJ1B_sp9j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[242, 28, 5, 8883, 22, 190, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").ids #this is what we will feed to the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Fkx6CSoXWXmG"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3XHJsP8_898x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-r_kyrrrDHZq"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, seq_len = 128):\n",
    "    #  data is currently a list of sentences\n",
    "    #  [sent1,\n",
    "    #   sent2,\n",
    "    #   ...,\n",
    "    #  ]\n",
    "\n",
    "    data = [d+'|</s>' for d in data] #append an </s> token to each sentence\n",
    "    encodings = tokenizer.encode_batch(data) #encode (turn token into token_id) data\n",
    "    token_ids = [enc.ids for enc in encodings] #get the token ids for each sentence\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids)) #turn a list of token_ids into one long token_ids\n",
    "    ## now data looks like this [sent1_ids </s> sent2_ids </s> ...]\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    #remove some left over tokens so that we can form batches of seq_len (128 in this case). Optionally, we can use padding tokens instead.\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len) #reshape data so it becomes a 2-D matrix of shape (len(encoded)//128, 128), i.e. each row contains data of len==128\n",
    "    ## now data looks like this\n",
    "    ## [ [1,2,3, ... , 128] (this is just an example, not actual input_ids)\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ## ]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YmW-K0XBZ4Dq"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 128\n",
    "train_dataset = TextDataset(train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True) #DataLoader will take care of the random sampling and batching of data\n",
    "\n",
    "test_dataset = TextDataset(test)\n",
    "test_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElhZcB94MUtC"
   },
   "source": [
    "## Model : Implement the forward function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nKNJAolug-1I"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #this will turn the token ids into vectors\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) #turn the vectors back into token ids\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        emb = self.dropout(self.embedding(src))\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src) # run the sequence through the model (the forward method)\n",
    "        prediction = prediction.reshape(-1, vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]  #[batch_size (64) , seq_len-1 (127)] except last words\n",
    "        target = batch[:, 1:] #[batch_size (64) , seq_len-1 (127)] except first words\n",
    "        with torch.no_grad(): #disable gradient calculation for faster inference\n",
    "          prediction = self(src) #[batch_size (64), seq_len-1 (127) , vocab size (9000)]\n",
    "        prediction = prediction.reshape(-1, vocab_size) #[batch_size*(seq_len-1) (64*127=8128) , vocab]\n",
    "        target = target.reshape(-1) #[batch_size (64), seq_len-1 (127)] -> [batch_size*(seq_len-1) (8128)]\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jBnYCh-miOEr"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HHWXaPsvigPq"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_yNEZ4jwXumR"
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger\n",
    "csv_logger = CSVLogger(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZwqhWicMdH0"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kr0zdeMAjD1U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=csv_logger,\n",
    "    deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "A9qcwNA0mN6J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 1.8 M  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 4.6 M  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.1 M    Total params\n",
      "48.504    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 130/130 [00:08<00:00, 15.09it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 130/130 [00:08<00:00, 14.70it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader) # takes about 8 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUfWF_V6Me9H"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WXVj9ewNqweZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jaf/anaconda3/envs/nlp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 39/39 [00:01<00:00, 25.40it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss            4.109495639801025\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4pVjEyYDtnc-"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uuIPToGQs-ZG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity : 60.91598622150782\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity : {np.exp(test_result[0]['test_loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pAZwiRqsnOPe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(9062, 200)\n",
       "  (lstm): LSTM(200, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=9062, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() #disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VFtebDAmVh_T"
   },
   "outputs": [],
   "source": [
    "unk_token_id = tokenizer.encode(\"[UNK]\").ids\n",
    "eos_token_id = tokenizer.encode(\"</s>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hj-V4OsDqpBO"
   },
   "outputs": [],
   "source": [
    "def generate_seq(context, max_new_token = 10):\n",
    "  encoded = tokenizer.encode(context).ids\n",
    "  with torch.no_grad():\n",
    "      for i in range(max_new_token):\n",
    "          src = torch.LongTensor([encoded]).to(model.device)\n",
    "          prediction = model(src)\n",
    "          probs = torch.softmax(prediction[:, -1] / 1, dim=-1)\n",
    "          prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          while prediction == unk_token_id:\n",
    "              prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          if prediction == eos_token_id:\n",
    "              break\n",
    "\n",
    "          encoded.append(prediction)\n",
    "\n",
    "  return tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "u20r9w8zvJi4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'วัน จันทร์ ที่   26   มิถุนายน   และ ได้ พบปะ   แจก ประชาสัมพันธ์ ให้ ผู้ ประกอบ การ ดูแล   ดัง นั้น   หลัง จาก ขอ ให้ ตรวจสอบ ผล ได้   ซึ่ง หาก เจ้าหน้าที่ ได้ รวบรวม หลักฐาน ให้ เสีย ชีวิต ใน ข้อ หา \"   นายปรีชา เลาหพงศ์ชนะ   กล่าว   และ โดย  '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"<s>|วัน|จันทร์\"\n",
    "generate_seq(context, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fr536NVvGX3"
   },
   "source": [
    "## Questions: Answer the following in MyCourseville\n",
    "\n",
    "1. What is the perplexity of the neural LM you trained?\n",
    "2. Paste your favorite sentence generated with the LM."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1sCWXCiBDs6UhYtxXTfWfIb30nC7kHTgc",
     "timestamp": 1612276273852
    },
    {
     "file_id": "1FIsDx7KTE5tiF-Xag22pl4VLQZc6G8Yw",
     "timestamp": 1612057948373
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
